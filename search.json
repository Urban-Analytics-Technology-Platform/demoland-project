[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Land Use Demonstrator",
    "section": "",
    "text": "Overview\nThis project develops a modelling system which is able to quantify several competing aspects of land use in a given urban environment, both as it currently exists (the ‘baseline’), and also under specific scenarios which change the distribution of such land use.\nIt comprises a sequence of models, designed to predict the impact of land use changes following large-scale planning decisions on a subset of indicators reflecting the quality of life. At the same time, it also aims to use neural networks to determine the optimal land use composition given target indicator levels.\nThe project is a partnership between the Geospatial Commission and The Alan Turing Institute, working with Newcastle City Council to develop a modelling system leveraging data science and AI to support decision-making in land use policy. In particular, the project provides tools to aid decision-makers in strategic planning by:\n\nevaluating the impacts of large-scale land use changes on policy priorities (house prices, air quality, job accessibility, and greenspace accessibility); and\nusing machine learning and AI to suggest interventions which can achieve desired policy outcomes."
  },
  {
    "objectID": "book/intro.html#summary",
    "href": "book/intro.html#summary",
    "title": "1  Introduction",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n\n\n\n\n\n\nObjectives\nDevelop modelling system to quantify features of land use in urban environment\n\n\nOutput\nPrediction of the quality of life indicators following modelled scenarios of development\nOur case study is Tyne and Wear county, comprising the following local authorities: Gateshead, Newcastle upon Tyne, North Tyneside, South Tyneside, and Sunderland.\n\n\nHow\nCreating key indicators for assessing a baseline scenario and future scenarios\n\n\nDuration\nSeptember 2022 – present"
  },
  {
    "objectID": "book/intro.html#project-aims",
    "href": "book/intro.html#project-aims",
    "title": "1  Introduction",
    "section": "1.2 Project aims",
    "text": "1.2 Project aims\nThe project aims to provide insight into the impact of land use policies in cities across the UK, piloting on the case of Tyne and Wear. It:\n\nDerives indicators of quality of life.\nDevelops machine learning models which can predict the impact of land use changes on such indicators.\nInversely, develops a neural network able to predict the required land use change to reach target levels of quality of life indicators.\n\nAll these technological components are presented in an interactive tool allowing quick and easy exploration of impacts aimed at policymakers."
  },
  {
    "objectID": "book/intro.html#quality-of-life-indicators",
    "href": "book/intro.html#quality-of-life-indicators",
    "title": "1  Introduction",
    "section": "1.3 Quality of life indicators",
    "text": "1.3 Quality of life indicators\nThe project defines four indicators related to the quality of life, which are intended to capture selected dimensions of the environment, society and economy:\n\nAir pollution\nHouse prices\nJob accessibility\nGreenspace accessibility\n\nAir pollution and house prices are directly derived from observed values. For these two indicators, we developed learning-based, predictive models which assess the impact of of land use changes. These models are based on land use variables derived from the Urban Grammar project.\nOn the other hand, both accessibility metrics (job and greenspace) were generated for this project. These were calculated for four modes of transport (walking, bicycle, vehicles, and public transit), between a relevant set of origins and destinations at the UK 2011 Census Output Area level.\nMore details about the indicators, as well as the explanatory variables used in the model, can be found in the Methodology chapter."
  },
  {
    "objectID": "book/intro.html#development-scenarios",
    "href": "book/intro.html#development-scenarios",
    "title": "1  Introduction",
    "section": "1.4 Development scenarios",
    "text": "1.4 Development scenarios\nThe project is done in collaboration with the Geospatial Commission and Newcastle City Council (NCC). It defines development scenarios of the Tyne and Wear county, for which it reports predicted changes of selected indicators, allowing assessment of a proposed land use change based on machine learning.\nThe indicators are first modelled for a baseline, reflecting the current land use in the area under study. Once the baseline is created, the project defines a number of scenarios of future development (e.g. densification of the city centre, or land release in the green belt area) and assesses the effect of those scenarios on the quality of life indicators.\nSee the chapter on scenarios for more details."
  },
  {
    "objectID": "book/intro.html#interactive-web-app",
    "href": "book/intro.html#interactive-web-app",
    "title": "1  Introduction",
    "section": "1.5 Interactive web app",
    "text": "1.5 Interactive web app\nAs part of the project, an interactive web app has also been created. The web app allows users to project both the proposed land use, as well as the predicted values of the quality-of-life indicators, onto a map of Newcastle. Policymakers and professionals will thus be able to explore the outputs of this project by visualising and comparing different scenarios."
  },
  {
    "objectID": "book/data_sources.html#project-wide",
    "href": "book/data_sources.html#project-wide",
    "title": "2  Data sources",
    "section": "2.1 Project-wide",
    "text": "2.1 Project-wide\n\nOrdnance Survey (OS) | 2011 Census Output Areas | link"
  },
  {
    "objectID": "book/data_sources.html#accessibility-indicators",
    "href": "book/data_sources.html#accessibility-indicators",
    "title": "2  Data sources",
    "section": "2.2 Accessibility indicators",
    "text": "2.2 Accessibility indicators\n\nOffice for National Statistics (ONS) | Population Weighted Centroids (PWC) of Output Areas | link\nONS | Workplace zones | link\nOS | Open Greenspace | link\nOpenStreetMap (OSM) | OpenStreetMap network data | link\nDepartment for Transport | Bus Open Data Service timetable data | link"
  },
  {
    "objectID": "book/data_sources.html#air-quality",
    "href": "book/data_sources.html#air-quality",
    "title": "2  Data sources",
    "section": "2.3 Air quality",
    "text": "2.3 Air quality\n\nDepartment for Environment, Food and Rural Affairs (DEFRA) | UK AIR | link"
  },
  {
    "objectID": "book/data_sources.html#house-prices",
    "href": "book/data_sources.html#house-prices",
    "title": "2  Data sources",
    "section": "2.4 House prices",
    "text": "2.4 House prices\n\nChi et al. | A new attribute-linked residential property price dataset for England and Wales 2011-2019 | link"
  },
  {
    "objectID": "book/data_sources.html#explanatory-variables-for-modelling",
    "href": "book/data_sources.html#explanatory-variables-for-modelling",
    "title": "2  Data sources",
    "section": "2.5 Explanatory variables for modelling",
    "text": "2.5 Explanatory variables for modelling\n\nONS | Mid-2020 population estimates (SAPE23DT10d edition) | link\nONS | Workplace population | link\nCopernicus | CORINE Land Cover classification (2018) | link\nThe Alan Turing Institute | Spatial Signatures of Great Britain | link"
  },
  {
    "objectID": "book/method.html#indicator-selection",
    "href": "book/method.html#indicator-selection",
    "title": "3  Methodology",
    "section": "3.1 Indicator selection",
    "text": "3.1 Indicator selection\nWe resolved to use four indicators of interest, which were shortlisted after an initial review of the available data and software, as well as the interests of the stakeholders. These are:\n\nAir quality\nHouse prices\nJob accessibility\nGreenspace accessibility\n\nFor each of these indicators, we have detailed below how they are generated. This step represents the basic scenario.\nWe have two different types of indicators: while air quality and house prices are derived using models which are fit to actual data, accessibilities are directly calculated using geospatial data, so are not directly modelled/predicted using explanatory variables.\n\n3.1.1 Accessibility calculation\nFor accessibility computations, we calculate the cumulative potential accessibility to green spaces and jobs (collectively ‘opportunities’). We consider a set of origins and opportunity destination points, and the time it takes to move between them on the road network for different transport modes.\nIn order to calculate the time it takes from each origin to reach each destination, we need to have a Time Travel Matrix (TTM) between origins and destinations for different means of transport. To build this, we can generate a graph/network starting from two data sources: one for the road network, and one for public transportation schedules. Once a TTM is built between a set of origins and a set of destinations for a given mode of transport, we can run accessibility analyses on it.\n\nOrigins: For this project, we take the coordinates of the Population Weighted Centroids (PWC) of Output Areas (OA) as the origins.\nDestinations: For each destination, we need to provide the coordinates as well as a datum for the opportunity (or ‘supply’, in our case this is the land use). For example, this could be the number of jobs available. (Other opportunities, though not considered here, could be the number of schools, health centres, or any other countable feature.)\n\nJobs: The destinations used for job accessibility are the PWCs of the working population for each Workplace Zone (WPZ) in the 2011 Census (see original data and definition).\nTo obtain an opportunity measure (job counts), we make the approximation that the number of workers is equal to the number of available jobs. This is in order to preserve the spatial location of the jobs, which is needed in the calculation of the time travel matrix. In fact, the number of jobs in the Census is given at the OA level, but the PWC for this aggregated level can be off-centered in relation to where people work.\nFinally, we define job accessibility as the number of jobs which are accessible by public transport and walking within 15 minutes.\nGreenspace: Here, we use the open data set available from Ordnance Survey (OS). In particular, as a first approximation we consider the layer “Access points” from this datum. This gives the coordinates of the access points to green spaces in all of Great Britain, which we use as destinations.\nAs an opportunity measure, we count the total areas of the greenspace (layer “Sites”) to which these points give access to. The greenspace accessibility is then defined as the sum of the areas of greenspace sites reachable within 15 minutes.\n\n\nA step-by-step description of the accessibility calculations is as follows:\nIndicator definition\n\nObtain greenspace sites from OS\n\nFilter out irrelevant categories (allotments, golf courses, bowling greens)\nRetain entrances on the edge of the sites\nAssociate\n\n(An idea for future improvement would be to obtain “missing” areas from Corine or OSM data.)\nGet the area of each site\nFilter entrances within time threshold (15 min) of each origin (OA)\nConsider unique values for parks (can be reached in )\nGenerate metric for each OA as the sum of reachable site size\n\nBuild time travel matrix (TTM)\nWe can build a TTM from two data sources: the road network, and a timetable for public transport. We can obtain the first by donwloading OpenStreetMap (OSM) data for the area of interest. Timetables for England public transport in GTFS format are available from UK2GTFS. We use GTFS data because it is more compatible with the ttm calculation package.\n\nGet time table for public transport &gt; GTFS data\nGet roads network &gt; OSM data\nGenerate network graph &gt; r5 engine (Conveyal) &gt; r5py package\nGenerate TTM for 4 different modes &gt; ‘transit’, ‘bike’, ‘car’, ‘walking’\n(An idea for future improvement would be to adjust the time threshold depending on the time of day / day of the week, as these would influence e.g. traffic conditions.)\n\nRun accessibility analysis\n\nGet land use data (opportunities detailed above in Destinations)\nJobs &gt; run tracc package on the TTM for each transport mode:\n\nCompute impedance function based on a 15 minute cost (cumulative)\nSetting up the accessibility object, i.e. joining the destination data to the travel time data\nMeasuring potential accessibility to jobs as the cumulative sum of opportunities at destination per origin\n\nGreenspace &gt; convert the tracc functions to work only on reachable sites’ area, on the TTM per transport mode:\n\nCompute impedance function based on a 15 minute cost (cumulative)\nJoin the destination data to the travel time data\nPer OA (origin), select all entrances which can be reached within the threshold time, and deduplicate entrances which correspond to the same park\nCalculate the metric as the sum of parks’ areas\n\n\n\n\n3.1.2 Air quality index\nWe develop an air quality index as a composite of PM2.5, PM10, NO2 and SO2 values, derived from the UK AIR project by DEFRA. Data are available as a 1 km grid. The composite index follows the methodology of the European Environmental Agency (EEA), reflecting the relative health risk associated with the exposure to particle intensities. Quoting from the EEA:\n\nThe bands are based on the relative risks associated with short-term exposure to PM2.5, O3 and NO2, as defined by the World Health Organization in their Health Risks of Air Pollution in Europe (HRAPIE) project report.\n\n\nThe relative risk of exposure to PM2.5 is taken as the basis for driving the index, specifically, the increase in the risk of mortality per 10 µg/m3 increase in the daily mean concentration of PM2.5.\n\n\nAssuming linearity across the relative risk functions for O3 and NO2, we calculate the concentrations of these pollutants that pose an equivalent relative risk to a 10 µg/m3 increase in the daily mean of PM2.5.\n\n\nFor PM10 concentrations, a constant ratio between PM10 and PM2.5 of 1:2 is assumed, in line with the World Health Organization´s air quality guidelines for Europe.\n\n\nFor SO2, the bands reflect the limit values set under the EU Air Quality Directive.\n\nThe relationship between PM2.5 : PM10 : NO2 : O3: SO2 is then equal to 1 : 2 : 4 : 5 : 10. The combined index can then be computed as \\[Q_{\\text{air}} = \\frac{\\text{PM2.5}}{1} + \\frac{\\text{PM10}}{2} + \\frac{\\mathrm{NO_2}}{4} + \\frac{\\mathrm{O_3}}{5} + \\frac{\\mathrm{SO_2}}{10}\\]\nExcept for the O3, UK AIR reports all of these values as concentrations in µg/m3. O3 is reported in terms of the number of days above a threshold of 120 µg/m3, and thus cannot be used in this formula, but even EEA omits data when unavailable, so we can create the index based on the four other measurements.\nIt should also be borne in mind that UK AIR data are not direct measurements, but rather a model.\nThe data from the 1 km grid are spatially interpolated to Output Area geometries. The index itself is computed on the grid.\n\n\n3.1.3 House price\nAn optimal way of working with house prices in the modelling exercise like ours is to use price per square metre (sqm). However, these data are not generally available at the Output Area or similar level. Luckily, we can retrieve such data from the paper “A new attribute-linked residential property price dataset for England and Wales 2011-2019”, by Chi et al.. This dataset contains individual house prices, total floor area, and a resulting price per sqm obtained from a combination of Land Registry Price Paid Data and Domestic Energy Performance Certificates.\nWe use the data from the years 2018–2019, and compute the mean price per sqm for each output area. Given the data are not fully up-to-date, the results of our modelling are presented as a percent increase or decrease compared to the baseline value rather than in absolute terms."
  },
  {
    "objectID": "book/method.html#explanatory-variables",
    "href": "book/method.html#explanatory-variables",
    "title": "3  Methodology",
    "section": "3.2 Explanatory variables",
    "text": "3.2 Explanatory variables\nWe curated a set of explanatory variables, which describe the urban environment and can be changed in order to model the different scenarios. These variables are chosen to have predictive power for the four indicators, and can be categorised into several groups according to their source.\n\n3.2.1 Group 1: ONS population estimates\nThe first group consists of only one variable, namely, population estimates. We use the population estimates by the Office for National Statistics (ONS) at the OA level for mid-2020. The other data used in the project generally reflect the period between 2018 and 2020, so this was chosen in order to describe the compatible point in time. The dataset (titled ‘SAPE23DT10d’) is retrieved from the ONS website. Since it is already reported at the Output Area level, it does not need to be processed further.\n\n\n3.2.2 Group 2: Workplace population by industry\nThe workplace population is obtained from Census 2011 data, and is reported on Workplace Zone geometries. We use preprocessed data from the Urban Grammar project which aggregated industries into the following groups:\n\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO, P, Q. Public administration, education and health\nR, S, T, U. Other\n\nThe data are then interpolated from Workplace Zones to Output Areas, giving us another 8 variables.\n\n\n3.2.3 Group 3: CORINE Land Cover classification\nWe use CORINE Land Cover classification data for 2018, which are provided as polygons of contiguous areas belonging to the same class. We use the data extracted for Great Britain within the Urban Grammar project, and interpolate the data to Output Areas, which gives us the proportion of each OA covered by each class. We further filter out fully or nearly invariant classes and use only:\n\nDiscontinuous urban fabric\nContinuous urban fabric\nNon-irrigated arable land\nIndustrial or commercial units\nGreen urban areas\nPastures\nSport and leisure facilities\n\nleading to 7 more variables.\n\n\n3.2.4 Group 4: Urban morphometrics\nUrban morphometrics offers a way of describing the physical built environment (buildings, streets) in a set of quantitative measurements which capture different aspects of morphological elements. We directly use the set measured within the Urban Grammar project, which are presented as individual characters (prior contextualisation), and interpolate their values from the original geometry (enclosed tessellation cells) to Output Areas. This way, we obtain a contextual version using the project-specific aggregation to OA. This gives us 59 morphometric variables.\n\n\n3.2.5 Variable pruning and final variable set\nIn total, from these four groups, we have 75 explanatory variables. Given that some of these are collected for different purposes than modelling, and with different geographical extents in mind, it may happen that some of the variables are correlated within our limited area of interest (Tyne and Wear). This may negatively affect the performance of the predictive models, and it is better to minimise the number of such pairs. We, therefore, measure both the Pearson and Spearman correlation indices between all pairs of explanatory variables. For each pair of variables where both correlation indices have an absolute value larger than 0.8, we retain only one, ideally the one that is more interpretable.\nThis procedure resulted in 16 of the morphometric variables being removed, leaving us with 59 explanatory variables in total:\n\n\n\n\n\n\n\n\nGroup\n#\nVariable name\n\n\n\n\nPopulation\n1\npopulation estimate\n\n\nWorkplace\n2\nA, B, D, E. Agriculture, energy and water\n\n\n\n3\nC. Manufacturing\n\n\n\n4\nF. Construction\n\n\n\n5\nG, I. Distribution, hotels and restaurants\n\n\n\n6\nH, J. Transport and communication\n\n\n\n7\nK, L, M, N. Financial, real estate, professional and administrative activities\n\n\n\n8\nO, P, Q. Public administration, education and health\n\n\n\n9\nR, S, T, U. Other\n\n\nCorine\n10\nLand cover [Discontinuous urban fabric]\n\n\n\n11\nLand cover [Continuous urban fabric]\n\n\n\n12\nLand cover [Non-irrigated arable land]\n\n\n\n13\nLand cover [Industrial or commercial units]\n\n\n\n14\nLand cover [Green urban areas]\n\n\n\n15\nLand cover [Pastures]\n\n\n\n16\nLand cover [Sport and leisure facilities]\n\n\nMorphometric\n17\narea of building\n\n\n\n18\ncourtyard area of building\n\n\n\n19\ncircular compactness of building\n\n\n\n20\ncorners of building\n\n\n\n21\nsquareness of building\n\n\n\n22\nequivalent rectangular index of building\n\n\n\n23\ncentroid - corner mean distance of building\n\n\n\n24\ncentroid - corner distance deviation of building\n\n\n\n25\norientation of building\n\n\n\n26\narea of ETC\n\n\n\n27\ncircular compactness of ETC\n\n\n\n28\nequivalent rectangular index of ETC\n\n\n\n29\ncovered area ratio of ETC\n\n\n\n30\ncell alignment of building\n\n\n\n31\nalignment of neighbouring buildings\n\n\n\n32\nmean distance between neighbouring buildings\n\n\n\n33\nperimeter-weighted neighbours of ETC\n\n\n\n34\nmean inter-building distance\n\n\n\n35\nwidth of street profile\n\n\n\n36\nwidth deviation of street profile\n\n\n\n37\nopenness of street profile\n\n\n\n38\nlength of street segment\n\n\n\n39\nlinearity of street segment\n\n\n\n40\nmean segment length within 3 steps\n\n\n\n41\nnode degree of junction\n\n\n\n42\nlocal proportion of 3-way intersections of street network\n\n\n\n43\nlocal proportion of 4-way intersections of street network\n\n\n\n44\nlocal proportion of cul-de-sacs of street network\n\n\n\n45\nlocal closeness of street network\n\n\n\n46\nlocal cul-de-sac length of street network\n\n\n\n47\nsquare clustering of street network\n\n\n\n48\nlocal degree weighted node density of street network\n\n\n\n49\nstreet alignment of building\n\n\n\n50\narea covered by edge-attached ETCs\n\n\n\n51\nbuildings per meter of street segment\n\n\n\n52\nreached ETCs by neighbouring segments\n\n\n\n53\nreached ETCs by tessellation contiguity\n\n\n\n54\narea of enclosure\n\n\n\n55\ncircular compactness of enclosure\n\n\n\n56\nequivalent rectangular index of enclosure\n\n\n\n57\norientation of enclosure\n\n\n\n58\nperimeter-weighted neighbours of enclosure\n\n\n\n59\narea-weighted ETCs of enclosure"
  },
  {
    "objectID": "book/method.html#modelling-regression-analysis",
    "href": "book/method.html#modelling-regression-analysis",
    "title": "3  Methodology",
    "section": "3.3 Modelling / regression analysis",
    "text": "3.3 Modelling / regression analysis\nThe accessibility indicators result from a deterministic model based on a fixed travel-time matrix. Hence, any new scenario being modelled can re-use it by changing only the raw values at the destinations. On the other hand, we need to develop predictive models to be able to see the changes in air quality and house prices.\nBoth of these models use the same classifier, a histogram-based gradient boosting regression tree as implemented in the scikit-learn Python package, and the set of 59 explanatory variables listed above. To ensure we are able to properly capture the spatial nature of the indicators, we add another 59 features which are a spatial lag of the original set. The spatial lag is measured as a mean value of the variable within a set neighbourhood, the definition of which is empirically determined. We test the neighbourhoods based on contiguity (from order of contiguity 1 to 5 inclusive of lower-order neighbors), Euclidean distance (500 m, 1000 m, 2000 m), and their unions. Each option is then included as part of the grid search aimed at the selection of the best model parameters and the best definition of the spatial lag in relation to the model performance metrics (the coefficient of determination R2, mean squared error, and mean absolute error). Other model parameters which we also assess are the learning rate, the maximum number of iterations, and the maximum number of bins. For more details, see the implementation in the Air Quality notebook and the House Price notebook.\nFurthermore, we have experimented with the geographical extent of the training data. While the original models were based only on the data from the study area (Tyne and Wear), we have tested models trained on other geographical subsets:\n\nthe whole of England\nthe whole of England, but excluding Greater London, which is known to be an outlier that may not help in predictive quality within Tyne and Wear\nTyne and Wear plus all OAs belonging to “Urbanity” classes from the rest of England\n\nThe aim of this is to add additional robustness to the model by training on a wider set of data.\n\n3.3.1 Resulting model specifications\nThe resulting models, selected based on their performance, are based on the following specifications:\nAir Quality\n\nSpatial extent: Tyne and Wear plus all OAs belonging to “Urbanity” classes from the rest of England\nSpatial lag: A union of 5 orders of Queen contiguity and 2000 m distance band\nModel parameters: learning_rate=0.2, max_bins=64, max_iter=1000\n\nThe model performance (R2) is ~0.8.\nHouse Price\n\nSpatial extent: England excluding Greater London\nSpatial lag: 5 orders of Queen contiguity\nModel parameters: learning_rate=0.1, max_bins=128, max_iter=1000\n\nThe model performance (R2) is ~0.5."
  },
  {
    "objectID": "book/method.html#building-scenarios",
    "href": "book/method.html#building-scenarios",
    "title": "3  Methodology",
    "section": "3.4 Building scenarios",
    "text": "3.4 Building scenarios\nScenarios are constructed by changing four macro variables, which together drive a sampling mechanism which derives the 59 variables used in the modelling. The four variables are:\n\nLevel of urbanity, which is a proxy for a signature type of spatial signatures as defined in the Urban Grammar project;\nUse of buildings, referring to the ratio of residential and commercial or industrial use;\nJob types, which determines whether jobs in each Output Area are more white-collar or blue-collar; and\nGreenspace, reflecting the amount of formal greenspace (i.e. parks).\n\nScenarios are modelled by specifying the macro variables on Output Areas where the change takes place. Any of the four variables can be specified in combination with any other. The algorithm then samples the data from either the baseline (capturing the existing state) or from the known distribution of values per signature type based on the country-wide data.\nThe first step in the sampling procedure is the selection of a signature type. If it is not changed, subsequent steps modify the baseline. Otherwise, we sample the values for a set signature type reflecting its common characteristics as observed across Great Britain. For example, if we specify the ‘local urbanity’ signature type for an Output Area which previously had ‘dense urban neighbourhoods’ assigned, we look at how ‘local urbanity’ usually looks like and sample all the required values from a narrow normal distribution centred on the median of the nationwide distribution. Further macro variables are adjusting these values. The use macro variable will change the variables of population and workplace population, job types will change the distribution of jobs between the job type categories, and greenspace will allocate new parks onto an Output Area (adjusting other values such as population accordingly).\nDevelopment of each scenario is then comprised of a few simple steps:\n\nSelect Output Areas that are supposed to change\nAssessing the target level of urbanity for each Output Area\nAssessing other macro variables for each Output Area\nFrom these, sample the values to be used as input for the model\nRun the models to assess the effect of tested changes"
  },
  {
    "objectID": "book/scenarios.html#scenario-1-low-density-residential-development",
    "href": "book/scenarios.html#scenario-1-low-density-residential-development",
    "title": "4  Scenarios",
    "section": "4.1 Scenario 1: Low-density residential development",
    "text": "4.1 Scenario 1: Low-density residential development\nThis scenario models the situation where land in the green belt is released for development. The area is taken over by a large developer used to build residential areas around the country. The new neighbourhood is a combination of low-rise detached and semi-detached housing with only minimal additional land use. The primarily residential neighbourhood does not generate a significant amount of jobs, inducing higher traffic to industrial zones and Newcastle city centre. The development is located west of the city around Callerton.\nThe new development is modelled as a combination of open sprawl and disconnected suburbia signature types, combined with an estimation of the allocation of new population and a small number of jobs in retail and education.\nThe model results in approximately:\n\n63 000 new residents\n9 250 new jobs (14% of new residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-2-mid-density-mixed-neighbourhood",
    "href": "book/scenarios.html#scenario-2-mid-density-mixed-neighbourhood",
    "title": "4  Scenarios",
    "section": "4.2 Scenario 2: Mid-density mixed neighbourhood",
    "text": "4.2 Scenario 2: Mid-density mixed neighbourhood\nThis scenario models the development in the green belt under the idea of a 15-minute neighbourhood that is dense, therefore taking up less space and mixed in terms of use. Such a neighbourhood contains not only residential housing but also a few places for new retail, commercial, and other uses. As such, it should be more self-sufficient than the low-density Scenario 1, inducing less traffic from the neighbourhood to other areas in the city. The development is assumed to be in the same area west of Callerton as in Scenario 1. The form is composed more of row houses and multi-story tenement buildings forming the centre of the new neighbourhood.\nIt is modelled as a combination of accessible suburbia, connected residential neighbourhoods and dense residential neighbourhoods signature types, with an approximation of new population and job allocation. Land cover is changed accordingly to discontinuous urban fabric and continuous urban fabric, but on a smaller area than in Scenario 1, leaving space dedicated to large urban parks.\nThe model results in approximately:\n\n54 000 new residents\n10 500 new jobs (19% of new residents)\n2 300 000 sq.m. of new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-3-densification-of-inner-city",
    "href": "book/scenarios.html#scenario-3-densification-of-inner-city",
    "title": "4  Scenarios",
    "section": "4.3 Scenario 3: Densification of inner city",
    "text": "4.3 Scenario 3: Densification of inner city\nThis densification scenario models a high-density development in the already developed areas, following the gradual infill and rebuilding existing buildings into higher ones with more mixed-use. It is a long-term strategy aimed at preserving green spaces (especially the green belt) and creating 15-minute neighbourhoods in the existing city by adding new layers of functionality and new inhabitants to places that are already built. The scenario affects most of the city, with higher densification levels around local centres and main streets and lower levels in suburban residential areas.\nIt is modelled as a change of signature types based on their hierarchy to higher order ones and related estimations of new population and job allocation. Land cover changes from discontinuous urban fabric to continuous urban fabric.\nThe model results in approximately:\n\n25 000 new residents\n24 000 new jobs (96% of new residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-4-brownfields-to-dense-neighbourhoods",
    "href": "book/scenarios.html#scenario-4-brownfields-to-dense-neighbourhoods",
    "title": "4  Scenarios",
    "section": "4.4 Scenario 4: Brownfields to dense neighbourhoods",
    "text": "4.4 Scenario 4: Brownfields to dense neighbourhoods\nExisting brownfield land is redeveloped into high-density neighbourhoods with mixed-use, providing housing, services, and commercial units in an attempt to densify the inner city without affecting existing areas. Compared to Scenario 3, this strategy is less invasive but has a lower scale. However, both scenarios can be potentially combined, as shown in Scenario 6.\nIt is modelled as a change of signature types on brownfield land to dense urban neighbourhoods and local urbanity, plus relevant changes in land cover, population and job allocation.\nThe model results in approximately:\n\n55 700 new residents\n−1 000 new jobs (0% of new residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-5-brownfields-into-parks",
    "href": "book/scenarios.html#scenario-5-brownfields-into-parks",
    "title": "4  Scenarios",
    "section": "4.5 Scenario 5: Brownfields into parks",
    "text": "4.5 Scenario 5: Brownfields into parks\nContrary to Scenario 4, this scenario assumes that all the brownfield land is turned into urban parks with no development. While it does not help to solve the issue of the capacity of a city, it may be viewed favourably by the local population and can balance potential densification as outlined in Scenario 3. Both Scenarios 3 and 5 can be combined, as shown in Scenario 7.\nIt will be modelled as a change of signature types on brownfield land to park/warehouse land plus relevant land cover changes and removal of any population and job allocation.\nThe model results in approximately:\n\n0 new residents\n−20 000 new jobs\n6 230 000 sq.m. of new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-6-urbanisation-to-the-edge",
    "href": "book/scenarios.html#scenario-6-urbanisation-to-the-edge",
    "title": "4  Scenarios",
    "section": "4.6 Scenario 6: Urbanisation to the edge",
    "text": "4.6 Scenario 6: Urbanisation to the edge\nThis scenario models the city following the densification strategies outlined in both Scenarios 3, where we target higher density to already-dense central areas, and Scenario 4, we model the development of dense neighbourhoods in the brownfield and industrial areas around the River Tyne. As such, the scenario combines changes from Scenarios 3 and 4.\nThe model results in approximately:\n\n66 000 new residents\n30 000 new jobs (45% of new residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-7-urbanisation-with-greenery",
    "href": "book/scenarios.html#scenario-7-urbanisation-with-greenery",
    "title": "4  Scenarios",
    "section": "4.7 Scenario 7: Urbanisation with greenery",
    "text": "4.7 Scenario 7: Urbanisation with greenery\nThis scenario directs changes to two locations in two different directions. First, it assumes the densification of an already-dense city centre as outlined in Scenario 3, adding further population, jobs, and services to the area. Second, it combines this densification with the creation of new large parks around the River Tyne, where current brownfields and industrial areas are. As such, the scenario is, in principle, a combination of changes from Scenarios 3 and 5.\nThe model results in approximately:\n\n1 000 new residents\n30 000 new jobs\n6 230 000 sq.m. of new parks"
  },
  {
    "objectID": "book/scenarios.html#results",
    "href": "book/scenarios.html#results",
    "title": "4  Scenarios",
    "section": "4.8 Results",
    "text": "4.8 Results\nYou can see the results of these scenarios in the interactive web application."
  },
  {
    "objectID": "book/developer_notes.html#api-specification",
    "href": "book/developer_notes.html#api-specification",
    "title": "5  Developer notes",
    "section": "5.1 API specification",
    "text": "5.1 API specification\nThe calculation of indicators for custom scenarios in the web app relies on an API. Right now, the web app uses an API which is hosted as an Azure Function.\nIn particular, the function must accept a POST body that looks like this. Only output areas which are changed relative to the baseline need to be included in the inner JSON object:\n{\n  \"scenario_json\": {\n    output_area_name: {\n      \"signature_type\": int | null,\n      \"job\": float | null,\n      \"use\": float | null,\n      \"greenspace\": float | null\n    },\n    ...\n  }\n}\nUpon success, this must return a JSON response with the following format (all output areas must be included):\n{\n  output_area_name: {\n    \"air_quality\": float,\n    \"house_price\": float,\n    \"job_accessibility\": float,\n    \"greenspace_accessibility\": float,\n    \"signature_type\": int\n  },\n  ...\n}"
  },
  {
    "objectID": "book/developer_notes.html#azure-functions",
    "href": "book/developer_notes.html#azure-functions",
    "title": "5  Developer notes",
    "section": "5.2 Azure Functions",
    "text": "5.2 Azure Functions\nThe code required to deploy this can be found on the azure-functions branch of the engine repository, specifically the function_app.py file.\nTo deploy a new version of the Azure function, make sure you are a member of the “Land Use Indicator” Azure subscription (the subscription ID is 86307bb0-6d16-4077-b1f7-939370d5289a). If you are not, get Dani to add you.\nThen, follow these instructions:\n\nLog in to Azure. This will open a browser window where you can log in with your Microsoft account.\n az login\nSet your active subscription to the one above:\naz account set -s 86307bb0-6d16-4077-b1f7-939370d5289a\nInstall the Azure Functions Core Tools. The following Homebrew command works on macOS; for other operating systems, refer to the Azure docs\nbrew tap azure/functions\nbrew install azure-functions-core-tools@4\nClone the demoland-engine repository (if you don’t already have it), and cd to the top level of the demoland-engine repository.\nTo test the function locally, run:\nfunc start\nYou should then be able to access the API at http://localhost:7071/api/scenario. For example, you can test it using the following command, which asks it to calculate the baseline indicators (an empty scenario means no changes relative to the baseline):\ncurl http://localhost:7071/api/scenario -X POST -d \"{\\\"scenario_json\\\": {}}\" -v\nTo deploy it to Azure, run the following, which will take a few minutes:\nfunc azure functionapp publish demolandapi\nTest it with:\ncurl https://demolandapi.azurewebsites.net/api/scenario -X POST -d \"{\\\"scenario_json\\\": {}}\" -v"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#variables-definition-and-data-import",
    "href": "code/01_baseline/01_accessibility.html#variables-definition-and-data-import",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.1 0. Variables definition and data import",
    "text": "A.1 0. Variables definition and data import\n\n# definitions\nimport sys\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport datetime as dt\nimport tracc\nfrom r5py import TransportNetwork, TravelTimeMatrixComputer, TransitMode, LegMode\nfrom datetime import datetime, date, timedelta\nimport matplotlib.pyplot as plt\nfrom itertools import product  # needed for generating all combinations of O-D pairs\n\nsys.argv.append([\"--max-memory\", \"8G\"])\n\n\ndata_folder = \"/Users/azanchetta/OneDrive - The Alan Turing Institute/demoland_data\"\n\n\n# regional level files: (require previous editing)\noas_centroids_file = (\n    f\"{data_folder}/processed/OA_centroids_TyneWear.gpkg\"  # used for population origin\n)\noas_file = f\"{data_folder}/processed/authorities/OA_TyneWear.gpkg\"  # needed for visualisation purposes\nregion_lads_file = f\"{data_folder}/processed/authorities/LADs_tynewear.shp\"  # needed in order to filter greenspace data within the regional boundaries\nworkingplacezones_centroids_file = f\"{data_folder}/processed/authorities/WPZ_centroids_tynewear.gpkg\"  # needed for destinations centroids coordinates\n# greenspace_sites_file = f\"{data_folder}/processed/accessibility/greenspace-sites_tynewear.gpkg\" # needed for calcualting opportunities at greenspaces (area)\n# greenspace_entrances_file = f\"{data_folder}/processed/accessibility/accessTOgs_tynewear.gpkg\" # needed for destinations centroids coordinates\ngreenspace_file = (\n    f\"{data_folder}/processed/accessibility/greenspace_tynewear_edited.gpkg\"\n)\njobs_file = f\"{data_folder}/processed/accessibility/wpz_tynewear_occupation_edited.csv\"\n\n# national level files\n# greenspace_file = f\"{data_folder}/raw/accessibility/OS Open Greenspace (GPKG) GB/data/opgrsp_gb.gpkg\"\nosm_data_file = f\"{data_folder}/raw/accessibility/tyne-and-wear-latest.osm.pbf\"\ngtfs_data_file = f\"{data_folder}/raw/accessibility/itm_north_east_gtfs.zip\"\n\n\n# import\n\n# origins (IE output areas, OAs)\noas_centroids = gpd.read_file(oas_centroids_file, layer=\"OA_centroids_TyneWear\")\noas_centroids[\"id\"] = oas_centroids[\n    \"OA11CD\"\n]  # Origin dataset must contain an 'id' column for r5py\noas_centroids.head()\n\n# destination data\n# green space sites' entrances\ngs_entrances = gpd.read_file(greenspace_file, layer=\"access_points\")\n\ngs_entrances.head()  # Destination dataset already contains an 'id' column\n# WPZ centroids\nwpz_centroids = gpd.read_file(\n    workingplacezones_centroids_file, layer=\"WPZ_centroids_tynewear\"\n)\nwpz_centroids.head()\nwpz_centroids[\"id\"] = wpz_centroids[\n    \"wz11cd\"\n]  # Destination dataset must contain an 'id' column for r5py\n\ngs_sites = gpd.read_file(greenspace_file, layer=\"sites\")\n\n# network data\n# uploaded in the sequent operation\n\n# opportunities / land use data\njobs_per_wpz_df = pd.read_csv(\n    jobs_file\n)  # working place zones, population (as a proxy for n of jobs)\n# note: opportunities column is called \"pop\"\n\n\ngs_entrances.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nA.1.1 CRS conversion\n\n# Converting the original files' crs to GWS84, which is compatible with GTFS and OSM data\noas_centroids_wgs84 = oas_centroids.to_crs(\"epsg:4326\")\ngs_entrances = gs_entrances.to_crs(\"epsg:4326\")\n# gs_sites = gs_sites.to_crs(\"epsg:4326\") # let's leave the layer in epsg:27700, as we need the prj for calculating the areas\nwpz_centroids = wpz_centroids.to_crs(\"epsg:4326\")\n\n\n\nA.1.2 Origins and destinations\n\noas_centroids.head()\n\n\n\n\n\n\n\n\nOBJECTID\nOA11CD\nGlobalID\ngeometry\nid\n\n\n\n\n0\n126926\nE00041377\nc03c9813-26f3-41f9-85e5-d4cdf3742ca0\nPOINT (425583.000 562952.000)\nE00041377\n\n\n1\n126927\nE00041435\n16e6607e-0b59-4f6f-8ec6-06a7396a70a5\nPOINT (427216.699 555732.531)\nE00041435\n\n\n2\n126928\nE00041745\n4b5fa995-b251-4ee7-9a97-aef0a2598fe3\nPOINT (427897.004 559557.605)\nE00041745\n\n\n3\n126929\nE00041432\n6e660884-3917-4e46-a693-bad0821318cb\nPOINT (427856.367 555759.595)\nE00041432\n\n\n4\n126930\nE00041742\n0bfb7f06-a910-4fa2-8db1-e79d319ba232\nPOINT (427932.556 559770.754)\nE00041742\n\n\n\n\n\n\n\n\nwpz_centroids.head()\n\n\n\n\n\n\n\n\nOBJECTID\nwz11cd\nGlobalID\ngeometry\nid\n\n\n\n\n0\n2\nE33000251\n{AF2BD35C-B624-4E2D-9C78-F26DF4FCABCE}\nPOINT (-1.41992 54.91839)\nE33000251\n\n\n1\n3\nE33000799\n{8CB93749-3349-462C-93C7-B6E321CC765C}\nPOINT (-1.61606 54.97382)\nE33000799\n\n\n2\n4\nE33000257\n{03204BF6-50A6-4AD1-855F-C7BBE6D8137B}\nPOINT (-1.53272 54.90010)\nE33000257\n\n\n3\n5\nE33000079\n{53333BDF-9792-4370-94AB-BE7853FA2ACA}\nPOINT (-1.62268 55.01104)\nE33000079\n\n\n4\n8\nE33000174\n{35114C58-FAA7-4E83-9724-ACED166052D5}\nPOINT (-1.50942 55.02269)\nE33000174\n\n\n\n\n\n\n\n\ngs_entrances.head()\n\n\n\n\n\n\n\n\nid\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n4\nid1BED7A99-E143-48C3-90CE-B7227E820454\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55988 55.03359)\n\n\n\n\n\n\n\n\n# origins:\n#   OAs\n# destinations:\n#   gs: entrances + OAs centroids\n#   jobs: wpz centroids + OAs centroids\n# total destination: OAs centroids + wpz centroids + gs entrances\n\norigins = oas_centroids_wgs84\n\n# destinations common fields: 'id', 'geometry'\n# simply concatenate the dataframes...\n# need to keep the info on greenspace site's name to link with the entrances later on\n\ndestinations = pd.concat(\n    [\n        oas_centroids_wgs84[[\"id\", \"geometry\"]],\n        wpz_centroids[[\"id\", \"geometry\"]],\n        gs_entrances[[\"id\", \"geometry\", \"refToGreenspaceSite\"]],\n    ]\n).reset_index(drop=True)\n\n\n\nA.1.3 Opportunities\n\n# jobs: n of employees per WPZ\n# greenspace: area of site\n\n\n# add column with opportunity ... one for all?"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#travel-time-matrix-computation",
    "href": "code/01_baseline/01_accessibility.html#travel-time-matrix-computation",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.2 1. Travel time matrix computation",
    "text": "A.2 1. Travel time matrix computation\n\nA.2.1 Generate the transport network\nCompute the network starting from OSM and GTFS data\n\n# load in transport network\ntransport_network = TransportNetwork(osm_data_file, [gtfs_data_file])\n\n\n\nA.2.2 Create an empty matrix that contains all origins and destinations to be used later on\nThis table will be filled up once we calculate the ttm\n\n# # # only for testing purposes:\n# k = 1000\n# # selecting first n rows of dataframe for origins and destinations\n# # origins = oas_centroids.loc[:k, :]\n# # destinations = wpz_centroids.loc[:n, :]\n# # selecting random rows, so to make sure we have both wpz AND gs_entrances in the selection of destinations\n# origins = origins.sample(n=k)\n# destinations = destinations.sample(n=k)\n\n\n# generate dataframe with all from_id and all to_id pairs\n# (empty for now, to be filled up later on)\nprod = product(origins[\"id\"].unique(), destinations[\"id\"].unique())\nempty_ttm = pd.DataFrame(prod)\nempty_ttm.columns = [\"from_id\", \"to_id\"]\nempty_ttm.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\n\n\n\n\n0\nE00041377\nE00041377\n\n\n1\nE00041377\nE00041435\n\n\n2\nE00041377\nE00041745\n\n\n3\nE00041377\nE00041432\n\n\n4\nE00041377\nE00041742\n\n\n\n\n\n\n\n\n\nA.2.3 Travel time matrix\nThe following piece of code is split in 2: - first part is definition of variables that will be inputted as parameters in the ttm computation - second part is the loop to generate ttm for several transport modes\n\n# defining variables\ndate_time = \"2023,01,19,9,30\"  # CHOOSE BEST DATE/TIME\n# max_time = dt.timedelta(seconds=900) # SET TO 15 MIN\nwalking_speed = 4.8\ncycling_speed = 16\n# dataframe to match legmode and transitmode objects (to be inputted in the ttm computer):\nmodes_lut = pd.DataFrame(\n    [\n        [\"transit\", TransitMode.TRANSIT, LegMode.WALK],\n        [\"car\", \"\", LegMode.CAR],\n        [\"bicycle\", \"\", LegMode.BICYCLE],\n        [\"walk\", \"\", LegMode.WALK],\n    ],\n    columns=(\"Mode\", \"Transit_mode\", \"Leg_mode\"),\n)\n\n\n# function to generate custom list of transit+transport mode for the parameter transport_modes in TravelTimeMatrixComputer\ndef list_making(s, z):\n    return [s] + [z]\n\n\nttm_complete = empty_ttm\n\n# loop to compute a ttm for all the modes and generate one single ttm table in output\nfor row in modes_lut.itertuples():\n    start_time = dt.datetime.now()\n    mode = row.Mode\n    transit_mode = row.Transit_mode\n    leg_mode = row.Leg_mode\n    transport_mode = list_making(\n        transit_mode, leg_mode\n    )  # creating list of objects for transport_modes parameter\n\n    print(\n        \"The current mode is:\",\n        mode,\n        \", transit is:\",\n        transit_mode,\n        \", transport var is:\",\n        transport_mode,\n    )\n    ttm_computer = TravelTimeMatrixComputer(\n        transport_network,\n        origins=origins,\n        destinations=destinations,\n        departure=dt.datetime.strptime(date_time, \"%Y,%m,%d,%H,%M\"),\n        # max_time = max_time,\n        speed_walking=walking_speed,\n        speed_cycling=cycling_speed,\n        transport_modes=transport_mode,\n    )\n\n    ttm = ttm_computer.compute_travel_times()\n    ttm = ttm.rename(\n        columns={\"travel_time\": f\"time_{mode}\"}\n    )  # renaming 'travel_time' column (automatically generated) to 'time_{mode of transport}'\n    ttm.isna().sum()  # checking for empty values, to see if the ttm actually calculated something\n    #  merging the empty table generated before (with all possible origins and destinations) with the ttm, per each mode adding a travel time column\n    ttm_complete = ttm_complete.merge(\n        ttm, how=\"outer\", left_on=[\"from_id\", \"to_id\"], right_on=[\"from_id\", \"to_id\"]\n    )\n\n    print(\"finished calculating ttm for mode\", mode)\n    end_time = datetime.now()\n    print(\"Duration for\", mode, \": {}\".format(end_time - start_time))\n\nThe current mode is: transit , transit is: TransitMode.TRANSIT , transport var is: [&lt;TransitMode.TRANSIT: &lt;java object 'com.conveyal.r5.api.util.TransitModes'&gt;&gt;, &lt;LegMode.WALK: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\n\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/r5py/r5/regional_task.py:224: RuntimeWarning: Departure time 2023-01-19 09:30:00 is outside of the time range covered by currently loaded GTFS data sets.\n  warnings.warn(\n\n\nfinished calculating ttm for mode transit\nDuration for transit : 0:07:34.098400\nThe current mode is: car , transit is:  , transport var is: ['', &lt;LegMode.CAR: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\nfinished calculating ttm for mode car\nDuration for car : 0:21:01.904903\nThe current mode is: bicycle , transit is:  , transport var is: ['', &lt;LegMode.BICYCLE: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\nfinished calculating ttm for mode bicycle\nDuration for bicycle : 0:16:26.882727\nThe current mode is: walk , transit is:  , transport var is: ['', &lt;LegMode.WALK: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\nfinished calculating ttm for mode walk\nDuration for walk : 0:03:23.352848\n\n\n\nttm_complete.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\n\n\n\n\n\n\n\n\n# # saving ttm in output\n# ttm_complete.to_parquet(f\"{data_folder}/processed/accessibility/ttm_complete.parquet\")"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#accessibility-calculation",
    "href": "code/01_baseline/01_accessibility.html#accessibility-calculation",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.3 2. Accessibility calculation",
    "text": "A.3 2. Accessibility calculation\nUsing jamaps/tracc package"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#accessibility-to-jobs",
    "href": "code/01_baseline/01_accessibility.html#accessibility-to-jobs",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.4 Accessibility to jobs",
    "text": "A.4 Accessibility to jobs\n\nttm_jobs = ttm_complete.copy(\n    deep=True\n)  # saving a copy of the matrix (the following operations will add columns to it, but we want to keep the original one also)\n\n# generate tracc cost object\nttm_jobs_tracc = tracc.costs(ttm_jobs)\n\nmodes_list = [\"transit\", \"car\", \"bicycle\", \"walk\"]\n\n# empty dataframe to be filled up in the next for loop\nacc_pot_jobs = origins[[\"id\"]]\n\nfor m in modes_list:\n    # generate variable names to be used in the tracc function below\n    cost_name = \"time_\" + m\n    travel_costs_ids = [\"from_id\", \"to_id\"]\n    supplyID = \"wpz11cd\"\n    impedence_param = 15  # value for impedence function, to be changed as needed\n    impedence_param_string = str(impedence_param)\n    cost_output = \"cum_\" + impedence_param_string + \"_\" + m\n    acc_column_name = \"pot_cum_acc_\" + impedence_param_string + \"_\" + m\n    opportunity = \"pop\"\n    # Computing impedance function based on a 15 minute travel time threshold.\n    ttm_jobs_tracc.impedence_calc(\n        cost_column=cost_name,\n        impedence_func=\"cumulative\",\n        impedence_func_params=impedence_param,  # to calculate n of jobs in n min threshold\n        output_col_name=cost_output,\n        prune_output=False,\n    )\n\n    # Setting up the accessibility object. This includes joining the destination data to the travel time data\n    acc_jobs = tracc.accessibility(\n        travelcosts_df=ttm_jobs_tracc.data,\n        supply_df=jobs_per_wpz_df,\n        travelcosts_ids=travel_costs_ids,\n        supply_ids=supplyID,\n    )\n    acc_jobs.data.head()\n\n    # Measuring potential accessibility to jobs, using a 15 minute cumulative impedance function\n    # acc_pot_jobs = acc_jobs.potential(\n    #         opportunity = \"pop\",\n    #         impedence = cost_output,\n    #         output_col_name= \"pot_acc_\" + cost_output\n    #         )\n    # the above function generate overwrite the column at every loop\n    # so we reproduce the same function (from tracc documentation) per each mode:\n    acc_jobs.data[acc_column_name] = (\n        acc_jobs.data[opportunity] * acc_jobs.data[cost_output]\n    )\n    group_sum_bymode_acc = acc_jobs.data.groupby(acc_jobs.data[travel_costs_ids[0]])[\n        [acc_column_name]\n    ].sum()\n    acc_pot_jobs = acc_pot_jobs.merge(\n        group_sum_bymode_acc, how=\"outer\", left_on=\"id\", right_on=\"from_id\"\n    )\n\n\nacc_jobs.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\ncum_15_transit\ncum_15_car\ncum_15_bicycle\ncum_15_walk\nwpz11cd\npop\npot_cum_acc_15_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\n1\n1\n1\n1\nNaN\nNaN\nNaN\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\n0\n1\n0\n0\nNaN\nNaN\nNaN\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\n0\n1\n0\n0\nNaN\nNaN\nNaN\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\n0\n0\n0\n0\nNaN\nNaN\nNaN\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\n0\n1\n0\n0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nacc_pot_jobs.head()\n\n\n\n\n\n\n\n\nid\npot_cum_acc_15_transit\npot_cum_acc_15_car\npot_cum_acc_15_bicycle\npot_cum_acc_15_walk\n\n\n\n\n0\nE00041377\n32139.0\n318618.0\n107910.0\n11817.0\n\n\n1\nE00041435\n4839.0\n164613.0\n8649.0\n3814.0\n\n\n2\nE00041745\n865.0\n208472.0\n9597.0\n865.0\n\n\n3\nE00041432\n2086.0\n67634.0\n11214.0\n2086.0\n\n\n4\nE00041742\n865.0\n170808.0\n10267.0\n615.0\n\n\n\n\n\n\n\n\n# saving output to external file"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#accessibility-to-greenspace",
    "href": "code/01_baseline/01_accessibility.html#accessibility-to-greenspace",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.5 Accessibility to greenspace",
    "text": "A.5 Accessibility to greenspace\n\n# edit greenspace layers\n# change the 'id' column name, as it's the same in both layers and generates issues later on\ngs_entrances.columns  # ['id', 'accessType', 'refToGreenspaceSite', 'geometry']\ngs_entrances.rename(columns={\"id\": \"id_entrance\"}, inplace=True)\ngs_sites.columns  # ['id', 'function', 'geometry']\ngs_sites.rename(columns={\"id\": \"id_site\"}, inplace=True)\n\n# calculates sites' area:\ngs_sites[\"area_m2\"] = gs_sites[\"geometry\"].area\n\n\ngs_entrances.head()\ngs_sites.head()\ngs_sites.explore(column=\"area_m2\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngs_entrances.head()\n\n\n\n\n\n\n\n\nid_entrance\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n4\nid1BED7A99-E143-48C3-90CE-B7227E820454\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55988 55.03359)\n\n\n\n\n\n\n\n\ngs_sites.head()\n\n\n\n\n\n\n\n\nid_site\nfunction\ngeometry\narea_m2\n\n\n\n\n0\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nPlay Space\nMULTIPOLYGON (((440767.260 552692.600, 440777....\n1560.70565\n\n\n1\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440761.280 552942.510, 440753....\n1966.87245\n\n\n2\nidE56DE6D8-C9BD-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440968.500 552987.220, 440983....\n8135.95125\n\n\n3\nidE56DE6D8-8F64-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((439560.480 560021.050, 439578....\n2868.07275\n\n\n4\nidE56DE6D8-8F65-13A9-E053-AAEFA00A0D0E\nCemetery\nMULTIPOLYGON (((439858.700 560473.170, 439817....\n153540.65735\n\n\n\n\n\n\n\n\n# associate park area to entrances\ngs_entrances_with_parkarea = pd.merge(\n    gs_entrances[[\"id_entrance\", \"refToGreenspaceSite\"]],\n    gs_sites[[\"id_site\", \"function\", \"area_m2\"]],\n    left_on=\"refToGreenspaceSite\",\n    right_on=\"id_site\",\n    how=\"right\",\n)\n\n\ngs_entrances_with_parkarea.head()\n\n\n\n\n\n\n\n\nid_entrance\nrefToGreenspaceSite\nid_site\nfunction\narea_m2\n\n\n\n\n0\nidCAC0A6B3-0FDB-446D-8E36-700AF2CC1256\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nPlay Space\n1560.70565\n\n\n1\nidCE043231-4C15-4265-A370-2D70261224C7\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n2\nid379B3089-2FF5-4BD3-B695-9B7DA915FB02\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n3\nid7AE0057A-2F40-43F3-970E-A517BBC99804\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n4\nidE5DAAC5C-29B5-49A0-BB46-62C78F46BA6C\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n\n\n\n\n\n\nttm_complete.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\n\n\n\n\n\n\n\n\nttm_greenspace = (\n    ttm_complete.copy()\n)  # saving a copy of the matrix (the following operations will add columns to it, but we want to keep the original one also)\n\n\nttm_gs_with_area = pd.merge(\n    ttm_greenspace,\n    gs_entrances_with_parkarea[[\"id_entrance\", \"refToGreenspaceSite\", \"area_m2\"]],\n    left_on=\"to_id\",\n    right_on=\"id_entrance\",\n    how=\"left\",\n)\n# generate tracc cost object\nttm_gs_tracc = tracc.costs(ttm_gs_with_area)\n\nmodes_list = [\"transit\", \"car\", \"bicycle\", \"walk\"]\n\n# empty dataframes to be filled up in the next for loop\nacc_pot_gs = origins[[\"id\"]]\ngs_acc = []\n\nfor m in modes_list:\n    # generate variable names to be used in the tracc function below\n    cost_name = \"time_\" + m\n    travel_costs_ids = [\"from_id\", \"to_id\"]\n    impedence_param = 15  # value for impedence function, to be changed as needed\n    impedence_param_string = str(impedence_param)\n    # name of the column\n    cost_output = (\n        \"cum_\" + impedence_param_string + \"_\" + m\n    )  # naming depends on impedence function threshold\n    area_column_name = \"area_\" + impedence_param_string + \"_\" + m\n    acc_column_name = (\n        \"pot_cum_acc_\" + impedence_param_string + \"_\" + m\n    )  # naming depends on impedence function threshold\n    opportunity = \"pop\"\n    # Computing impedence function based on a 15 minute travel time threshold.\n    ttm_gs_tracc.impedence_calc(\n        cost_column=cost_name,\n        impedence_func=\"cumulative\",\n        impedence_func_params=impedence_param,  # to calculate opportunities in X min threshold\n        output_col_name=cost_output,\n        prune_output=False,\n    )\n    ttm_gs_df = ttm_gs_tracc.data\n    print(ttm_gs_df.columns)\n    # Setting up the accessibility object. This includes joining the destination data to the travel time data\n    # this needed to be done differently for greenspace, as opportunity is sites's area cumulative sum\n    # A. Filtering only rows with time travel within the threshold\n    print(\"cost output is\", cost_output)\n    print(\"area column name is\", area_column_name)\n    # tracc_15min = ttm_gs_tracc.data[ttm_gs_tracc.data.loc[:,cost_output]==1] # this doesn't work because of the different lenghts of the columns generated per mode\n    ttm_gs_tracc.data[area_column_name] = (\n        ttm_gs_tracc.data[\"area_m2\"] * ttm_gs_tracc.data[cost_output]\n    )\n    ttm_gs_df = ttm_gs_tracc.data\n\n    # B. Filter entrances (only one per park)\n    oneaccess_perpark = ttm_gs_df.sort_values(cost_name).drop_duplicates(\n        [\"from_id\", \"refToGreenspaceSite\"]\n    )\n    oneaccess_perpark.head()\n    # C. Assign metric as sum[parks' area]\n    # generate df with one row per OA centroid ('from_id') and sum of sites' areas - per each mode\n    gs_metric_per_mode = oneaccess_perpark.groupby([\"from_id\"])[\n        area_column_name\n    ].sum()  # .reset_index()\n    gs_acc.append(gs_metric_per_mode)\ngs_acc = pd.concat(gs_acc, axis=1)\n\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit'],\n      dtype='object')\ncost output is cum_15_transit\narea column name is area_15_transit\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit', 'area_15_transit', 'cum_15_car'],\n      dtype='object')\ncost output is cum_15_car\narea column name is area_15_car\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit', 'area_15_transit', 'cum_15_car', 'area_15_car',\n       'cum_15_bicycle'],\n      dtype='object')\ncost output is cum_15_bicycle\narea column name is area_15_bicycle\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit', 'area_15_transit', 'cum_15_car', 'area_15_car',\n       'cum_15_bicycle', 'area_15_bicycle', 'cum_15_walk'],\n      dtype='object')\ncost output is cum_15_walk\narea column name is area_15_walk\n\n\n\nttm_gs_tracc.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\nid_entrance\nrefToGreenspaceSite\narea_m2\ncum_15_transit\narea_15_transit\ncum_15_car\narea_15_car\ncum_15_bicycle\narea_15_bicycle\ncum_15_walk\narea_15_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\nNaN\nNaN\nNaN\n1\nNaN\n1\nNaN\n1\nNaN\n1\nNaN\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\nNaN\nNaN\nNaN\n0\nNaN\n1\nNaN\n0\nNaN\n0\nNaN\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\nNaN\nNaN\nNaN\n0\nNaN\n1\nNaN\n0\nNaN\n0\nNaN\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\nNaN\nNaN\nNaN\n0\nNaN\n0\nNaN\n0\nNaN\n0\nNaN\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\nNaN\nNaN\nNaN\n0\nNaN\n1\nNaN\n0\nNaN\n0\nNaN\n\n\n\n\n\n\n\nExporting results in output\n\nacc_pot_jobs.to_csv(\n    f\"{data_folder}/processed/accessibility/acc_jobs_allmodes_15min_tynewear.csv\"\n)\ngs_acc.to_csv(\n    f\"{data_folder}/processed/accessibility/acc_greenspace_allmodes_15min_tynewear.csv\"\n)\n\nPlotting results\n\noas_boundaries = gpd.read_file(oas_file, layer=\"OA_TyneWear\")\noas_boundaries_wgs84 = oas_boundaries.to_crs(\"epsg:4326\")\n\n\noas_boundaries_jobs = oas_boundaries_wgs84.merge(\n    acc_pot_jobs, left_on=\"geo_code\", right_on=\"id\", how=\"right\"\n)\n\n\noas_boundaries_jobs.plot(\n    \"pot_cum_acc_15_transit\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_jobs.explore(\n    column=\"pot_cum_acc_15_car\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_jobs.explore(\n    column=\"pot_cum_acc_15_transit\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_metric = oas_boundaries_wgs84.merge(\n    gs_acc, left_on=\"geo_code\", right_on=\"from_id\", how=\"right\"\n)\n\n\noas_boundaries_metric.explore(\n    column=\"area_15_transit\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_metric.explore(\n    column=\"area_15_car\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#variables-definition-and-data-import",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#variables-definition-and-data-import",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.1 Variables definition and data import",
    "text": "B.1 Variables definition and data import\n\nimport sys\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport datetime as dt\nimport tracc\nfrom r5py import TransportNetwork, TravelTimeMatrixComputer, TransitMode, LegMode\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\n\nsys.argv.append([\"--max-memory\", \"8G\"])\n\n\ndata_folder = \"/Users/azanchetta/OneDrive - The Alan Turing Institute/demoland_data\"\n\n\n# Ordnance Survey (OS) Greenspace data\n# (using Tyne and Wear data for now, generated previously in QGis)\n# greenspace_file = f\"{data_folder}/raw/accessibility/OS Open Greenspace (GPKG) GB/data/opgrsp_gb.gpkg\"\ngreenspace_sites_file = (\n    f\"{data_folder}/processed/accessibility/greenspace-sites_tynewear.gpkg\"\n)\naccesspoints_file = f\"{data_folder}/processed/accessibility/accessTOgs_tynewear.gpkg\"\n\n# OSM landuse data (Tyne and Wear data)\nosm_landuse_file = f\"{data_folder}/raw/OSM_tynewear/tyne-and-wear-latest-free.shp/gis_osm_landuse_a_free_1.shp\"\n\n# if needed for mapping purposes (?)\nregion_lads_file = f\"{data_folder}/processed/authorities/LADs_tynewear.shp\"  # needed in order to filter greenspace data within the regional boundaries\n\n\nData import\n\n\ngreenspace_sites = gpd.read_file(\n    greenspace_sites_file, layer=\"grenspace-sites_tynewear\"\n)\n\ngreenspace_sites.head()\n\naccesspoints = gpd.read_file(accesspoints_file, layer=\"pointsaccessTOgs_tynewear\")\naccesspoints.head()\n\n# for mapping:\nregion_lads = gpd.read_file(region_lads_file)\nregion_lads.head()\n\n\n\n\n\n\n\n\nOBJECTID\nLAD20CD\nLAD20NM\nLAD20NMW\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Are\nShape__Len\nlabel\ngeometry\n\n\n\n\n0\n265\nE08000021\nNewcastle upon Tyne\nNone\n422287\n569662\n-1.65297\n55.02101\n1.134619e+08\n65202.925674\nNewcastle upon Tyne\\nE08000021\nPOLYGON ((422592.399 576160.095, 422618.297 57...\n\n\n1\n266\nE08000022\nNorth Tyneside\nNone\n431471\n570602\n-1.50923\n55.02896\n8.231373e+07\n65337.781081\nNorth Tyneside\\nE08000022\nMULTIPOLYGON (((435203.599 575441.701, 435209....\n\n\n2\n267\nE08000023\nSouth Tyneside\nNone\n435514\n564057\n-1.44679\n54.96988\n6.442842e+07\n51370.230506\nSouth Tyneside\\nE08000023\nPOLYGON ((438030.200 568413.300, 438021.350 56...\n\n\n3\n268\nE08000024\nSunderland\nNone\n436470\n551524\n-1.43344\n54.85719\n1.374412e+08\n99737.411804\nSunderland\\nE08000024\nMULTIPOLYGON (((441259.800 557854.000, 441252....\n\n\n4\n281\nE08000037\nGateshead\nNone\n420168\n559658\n-1.68680\n54.93120\n1.423691e+08\n90476.826397\nGateshead\\nE08000037\nPOLYGON ((415042.801 565083.296, 415104.202 56...\n\n\n\n\n\n\n\n\n# selecting from greenspace layer only the relevant layers\ngreenspace_sites = greenspace_sites[[\"id\", \"function\", \"geometry\"]]\ngreenspace_sites.head()\n\n\n\n\n\n\n\n\nid\nfunction\ngeometry\n\n\n\n\n0\nidE56DE6D8-CB5D-13A9-E053-AAEFA00A0D0E\nAllotments Or Community Growing Spaces\nMULTIPOLYGON (((440933.560 552196.810, 440929....\n\n\n1\nidE56DE6D8-CB1F-13A9-E053-AAEFA00A0D0E\nAllotments Or Community Growing Spaces\nMULTIPOLYGON (((441101.270 552520.550, 441095....\n\n\n2\nidE56DE6BE-8E16-13A9-E053-AAEFA00A0D0E\nPlaying Field\nMULTIPOLYGON (((440984.530 552520.890, 440985....\n\n\n3\nidE56DE6C8-8E48-13A9-E053-AAEFA00A0D0E\nPlay Space\nMULTIPOLYGON (((440985.360 552520.750, 441000....\n\n\n4\nidE56DE6C6-4CFA-13A9-E053-AAEFA00A0D0E\nOther Sports Facility\nMULTIPOLYGON (((440986.650 552565.050, 440989....\n\n\n\n\n\n\n\n\ngreenspace_sites.explore(column=\"function\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# viewing 'function' categories to work on later\ngreenspace_sites[\"function\"].unique()\n\narray(['Allotments Or Community Growing Spaces', 'Playing Field',\n       'Play Space', 'Other Sports Facility', 'Public Park Or Garden',\n       'Religious Grounds', 'Bowling Green', 'Cemetery', 'Tennis Court',\n       'Golf Course'], dtype=object)\n\n\n\nWorking on OS data before understanding how to integrate OSM data\nIE points 1, 2, 5, 6 from Workflow above"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#filter-categories-from-os-data",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#filter-categories-from-os-data",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.2 1. filter categories from OS data",
    "text": "B.2 1. filter categories from OS data\nwe consider only open and not paid-for spaces\n\ngreenspace_sites_select = greenspace_sites.query(\n    \"function!='Allotments Or Community Growing Spaces' & function!='Golf Course' & function!='Bowling Green'\"\n)\n\n\ngreenspace_sites_select.explore(column=\"function\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#clean-os-data",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#clean-os-data",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.3 2. Clean OS data",
    "text": "B.3 2. Clean OS data\nA. dissolve the overlapping polygons\n\npublicpark = greenspace_sites_select.query(\"function=='Public Park Or Garden'\")\nplayingfield = greenspace_sites_select.query(\"function=='Playing Field'\")\nothersport = greenspace_sites_select.query(\"function=='Other Sports Facility'\")\ntherest = greenspace_sites_select.query(\n    \"function!='Playing Field' & function!='Public Park Or Garden' & function!='Other Sports Facility'\"\n)\n\n\n# find 'therest' not included in the upper categories\n# we use sjoin to performe a spatial filter of 'therest' polygons contained in upper categories\njoin11 = gpd.sjoin(therest, othersport, op=\"within\", how=\"inner\")\njoin12 = gpd.sjoin(therest, playingfield, op=\"within\", how=\"inner\")\njoin13 = gpd.sjoin(therest, publicpark, op=\"within\", how=\"inner\")\n# join13.columns = join13.columns.str.replace('index_', 'join3')\njoin13.head()\n# generate list of the IDs of 'therest' contained in upper categories, in order to eliminate the corresponding polygons from the layer\nlist_for_diff11 = join11[\"id_left\"].drop_duplicates().to_list()\n# diff1 =therest.query('id'.isin('list_for_diff1')) # doesn't work\n# diff1 = therest[therest['id'].isin('list_for_diff1')] # doesn't work\ndiff11 = therest[\n    ~therest.id.isin(list_for_diff11)\n]  # 1st difference layer # note the negation character ~ to take the polygons NOT included\n\nlist_for_diff12 = join12[\"id_left\"].drop_duplicates().to_list()\ndiff12 = diff11[~diff11.id.isin(list_for_diff12)]  # 2nd difference layer\n\nlist_for_diff13 = join13[\"id_left\"].drop_duplicates().to_list()\ndiff13 = diff12[\n    ~diff12.id.isin(list_for_diff13)\n]  # 3rd difference layer, this is for 'therest' categories\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# we repeat the same operation for subsequent categories:\n# find 'othersport' not included in the upper categories\njoin21 = gpd.sjoin(othersport, playingfield, op=\"within\", how=\"inner\")\njoin22 = gpd.sjoin(othersport, publicpark, op=\"within\", how=\"inner\")\n\nlist_for_diff21 = join21[\"id_left\"].drop_duplicates().to_list()\ndiff21 = othersport[~othersport.id.isin(list_for_diff21)]\n\nlist_for_diff22 = join22[\"id_left\"].drop_duplicates().to_list()\ndiff22 = diff21[~diff21.id.isin(list_for_diff22)]  # 'othersport' difference\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# find 'playing fields' not included in the upper categories (and viceversa?)\njoin31 = gpd.sjoin(playingfield, publicpark, op=\"within\", how=\"inner\")\njoin32 = gpd.sjoin(\n    publicpark, playingfield, op=\"within\", how=\"inner\"\n)  ## check it is not empty ... it is empty, we do not use this join\n\nlist_for_diff31 = join31[\"id_left\"].drop_duplicates().to_list()\ndiff31 = playingfield[\n    ~playingfield.id.isin(list_for_diff31)\n]  # 'playingfield' difference\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# put together all the differences layers: (and should bring out to the desired output)\ntogether1 = pd.concat([diff13, diff22]).pipe(\n    gpd.GeoDataFrame\n)  # 'therest' + 'othersport' differences\ntogether1.head()\ntogether2 = pd.concat([together1, diff31]).pipe(\n    gpd.GeoDataFrame\n)  # last gdf + 'playingfield' difference\ntogether_again = pd.concat([together2, publicpark]).pipe(\n    gpd.GeoDataFrame\n)  # last gdf + all the public parks)\n\ntogether_again.head()\n\n\n\n\n\n\n\n\nid\nfunction\ngeometry\n\n\n\n\n5\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nPlay Space\nMULTIPOLYGON (((440767.260 552692.600, 440777....\n\n\n7\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440761.280 552942.510, 440753....\n\n\n8\nidE56DE6D8-C9BD-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440968.500 552987.220, 440983....\n\n\n16\nidE56DE6D8-8F64-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((439560.480 560021.050, 439578....\n\n\n17\nidE56DE6D8-8F65-13A9-E053-AAEFA00A0D0E\nCemetery\nMULTIPOLYGON (((439858.700 560473.170, 439817....\n\n\n\n\n\n\n\n\ntogether_again = together_again.set_crs(\"epsg:27700\")\n\n\n# re-setting the crs as we lost in the last operation\ntogether_again.explore(\n    column=\"function\"\n)  # any reason why it doesn't show the whole gdf\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nB. Filter the entrances (only the ones on the edges)\nWe can, as a first measure, filter out from the entrances the ones to greenspaces that are not in the data set anymore.\nThe entrances that are left should only be located along the edges of the relevant polygons.\n\nlist_gs_id = together_again[\"id\"].to_list()\n\n\naccesspoints.head()\n\n\n\n\n\n\n\n\nid\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid548D0EAC-E6BE-4DFA-B90C-DB631A75309B\nPedestrian\nidE56DE841-2BC6-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55981 55.03343)\n\n\n4\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n\n\n\n\n\n\naccesspoints_edge = accesspoints[accesspoints.refToGreenspaceSite.isin(list_gs_id)]\n\n\naccesspoints_edge.explore()\n\n\n# checking crs\ntogether_again.crs  # epsg:27700\naccesspoints_edge.crs  # epsg:4326 (wgs84)\naccesspoints_edge_OSGB36 = accesspoints_edge.to_crs(\"epsg:27700\")\n\n\n# saving the two edited into geopackage. first the sites\ncomplete_gpkg_filename = (\n    f\"{data_folder}/processed/accessibility/greenspace_tynewear_edited.gpkg\"\n)\ntogether_again.to_file(complete_gpkg_filename, layer=\"sites\")\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  pd.Int64Index,\n\n\n\n# adding the entrances to the same gpkg\naccesspoints_edge_OSGB36.to_file(complete_gpkg_filename, layer=\"access_points\")\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  pd.Int64Index,"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#get-areas-size",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#get-areas-size",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.4 5 . get areas size",
    "text": "B.4 5 . get areas size\n\ntogether_again[\"area_m2\"] = together_again[\"geometry\"].area\n\n\ntogether_again.explore(column=\"area_m2\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10)"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#run-accessibility-analysis",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#run-accessibility-analysis",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.5 6. run accessibility analysis",
    "text": "B.5 6. run accessibility analysis\n\n# define variables path\noas_centroids_file = (\n    f\"{data_folder}/processed/OA_centroids_TyneWear.gpkg\"  # used for population origin\n)\noas_file = f\"{data_folder}/processed/authorities/OA_TyneWear.gpkg\"  # needed for visualisation purposes\nosm_data_file = f\"{data_folder}/raw/accessibility/tyne-and-wear-latest.osm.pbf\"\ngtfs_data_file = f\"{data_folder}/raw/accessibility/itm_north_east_gtfs.zip\"\n\n\n# importing needed data\n# origins (IE output areas, OAs)\norigin_centroids = gpd.read_file(oas_centroids_file, layer=\"OA_centroids_TyneWear\")\norigin_centroids[\"id\"] = origin_centroids[\n    \"OA11CD\"\n]  # Origin dataset must contain an 'id' column\norigin_centroids.head()\noas_boundaries = gpd.read_file(oas_file, layer=\"OA_TyneWear\")\n\n\noas_boundaries.crs\n\n&lt;Projected CRS: EPSG:27700&gt;\nName: OSGB 1936 / British National Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: UK - Britain and UKCS 49°46'N to 61°01'N, 7°33'W to 3°33'E\n- bounds: (-9.2, 49.75, 2.88, 61.14)\nCoordinate Operation:\n- name: British National Grid\n- method: Transverse Mercator\nDatum: OSGB 1936\n- Ellipsoid: Airy 1830\n- Prime Meridian: Greenwich\n\n\n\n# checking crs are compatible with OSM and GTFS data\norigin_centroids.crs\naccesspoints_edge.crs  # wgs84\norigin_centroids_wgs84 = origin_centroids.to_crs(\"epsg:4326\")\norigin_centroids_wgs84.crs\noas_boundaries_wgs84 = oas_boundaries.to_crs(\"epsg:4326\")\n\n\n# generate transport network\ntransport_network = TransportNetwork(osm_data_file, [gtfs_data_file])\n\n\n# generate ttm (by foot only for now)\nttm_walking_OAtoGS = TravelTimeMatrixComputer(\n    transport_network,\n    origins=origin_centroids_wgs84,\n    destinations=accesspoints_edge,\n    max_time=dt.timedelta(seconds=900),  # restricting travel to 15min\n    speed_walking=4.8,\n    transport_modes=[LegMode.WALK],\n)\nttm_walking_OAtoGS = ttm_walking_OAtoGS.compute_travel_times()\nttm_walking_OAtoGS.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\n\n\n\n\n0\nE00041377\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nNaN\n\n\n1\nE00041377\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nNaN\n\n\n2\nE00041377\nid0E14522B-427F-47C1-B043-BC3847ABE673\nNaN\n\n\n3\nE00041377\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nNaN\n\n\n4\nE00041377\nid1BED7A99-E143-48C3-90CE-B7227E820454\nNaN\n\n\n\n\n\n\n\n\n# accessibility calculation\ndf_tracc = tracc.costs(ttm_walking_OAtoGS)\ndf_tracc.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\n\n\n\n\n0\nE00041377\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nNaN\n\n\n1\nE00041377\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nNaN\n\n\n2\nE00041377\nid0E14522B-427F-47C1-B043-BC3847ABE673\nNaN\n\n\n3\nE00041377\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nNaN\n\n\n4\nE00041377\nid1BED7A99-E143-48C3-90CE-B7227E820454\nNaN\n\n\n\n\n\n\n\n\nmax_time = ttm_walking_OAtoGS.groupby(\"from_id\")[\"travel_time\"].max()\nmax_time.max()\n\n24.0\n\n\n\n# Computing impedance function based on a 15 minute travel time threshold.\ndf_tracc.impedence_calc(\n    cost_column=\"travel_time\",\n    impedence_func=\"cumulative\",\n    impedence_func_params=15,  # minutes cap\n    output_col_name=\"cum_15\",\n    prune_output=False,\n)\ndf_tracc.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\n\n\n\n\n0\nE00041377\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nNaN\n0\n\n\n1\nE00041377\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nNaN\n0\n\n\n2\nE00041377\nid0E14522B-427F-47C1-B043-BC3847ABE673\nNaN\n0\n\n\n3\nE00041377\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nNaN\n0\n\n\n4\nE00041377\nid1BED7A99-E143-48C3-90CE-B7227E820454\nNaN\n0\n\n\n\n\n\n\n\n\ndf_tracc_15min = df_tracc.data[df_tracc.data.loc[:, \"cum_15\"] == 1]\ndf_tracc_15min.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\n\n\n\n\n1360\nE00041377\nidA22A0B91-5C86-435E-9903-1504CF541EBE\n10.0\n1\n\n\n1361\nE00041377\nid46A79A0F-CC21-453B-8D1E-718602AC52B2\n11.0\n1\n\n\n1362\nE00041377\nid5CCFD37A-C45C-4812-B1A0-90EDA6A28407\n10.0\n1\n\n\n1363\nE00041377\nidF12A35EF-34D2-4233-8DA7-0C804753070F\n11.0\n1\n\n\n1364\nE00041377\nid09D9EBD9-CDAC-443F-9381-E95FF2C69732\n12.0\n1\n\n\n\n\n\n\n\nA. filter entrances (one per park)\n\naccesspoints.head()\n\n\n\n\n\n\n\n\nid\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid548D0EAC-E6BE-4DFA-B90C-DB631A75309B\nPedestrian\nidE56DE841-2BC6-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55981 55.03343)\n\n\n4\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n\n\n\n\n\n\n# associate park ID to entrances\naccesspoints_withPark = pd.merge(\n    df_tracc_15min,\n    accesspoints[[\"id\", \"refToGreenspaceSite\"]],\n    left_on=\"to_id\",\n    right_on=\"id\",\n    how=\"right\",\n)\n\n\naccesspoints_withPark.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\nid\nrefToGreenspaceSite\n\n\n\n\n0\nE00043145\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n2.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n1\nE00043142\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n7.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n2\nE00043143\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n4.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n3\nE00043140\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n10.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n4\nE00043141\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n3.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n\n\n\n\n\n\nlen(accesspoints_withPark.from_id.unique())  # 3726\nlen(accesspoints_withPark.refToGreenspaceSite.unique())  # 1171\n# not relevant, but just to get an idea of the dimensions\n\n1853\n\n\n\noneaccess_perpark = accesspoints_withPark.sort_values(\"travel_time\").drop_duplicates(\n    [\"from_id\", \"refToGreenspaceSite\"]\n)\noneaccess_perpark.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\nid\nrefToGreenspaceSite\n\n\n\n\n50823\nE00170026\nid02365D7F-C250-4444-89F1-DBE551510281\n0.0\n1.0\nid02365D7F-C250-4444-89F1-DBE551510281\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n\n\n36894\nE00042112\nid2232DADE-7F34-44A9-A68A-A251D6F4401D\n0.0\n1.0\nid2232DADE-7F34-44A9-A68A-A251D6F4401D\nidE56DE840-A14F-13A9-E053-AAEFA00A0D0E\n\n\n66716\nE00041433\nidA0076802-9488-4034-A4FF-70873196025F\n0.0\n1.0\nidA0076802-9488-4034-A4FF-70873196025F\nidE56DE6C6-B4B3-13A9-E053-AAEFA00A0D0E\n\n\n83464\nE00043860\nidD07325AC-C4A1-4684-AF75-2FB2BB03EE98\n0.0\n1.0\nidD07325AC-C4A1-4684-AF75-2FB2BB03EE98\nidE56DE849-1E1A-13A9-E053-AAEFA00A0D0E\n\n\n90749\nE00043604\nidEFA07B19-6B76-419D-91B9-47D61F7C6709\n0.0\n1.0\nidEFA07B19-6B76-419D-91B9-47D61F7C6709\nidE56DE6D7-1FB7-13A9-E053-AAEFA00A0D0E\n\n\n\n\n\n\n\n\n# oneaccess_perpark = accesspoints_withPark.groupby('from_id')['refToGreenspaceSite'].agg(['unique'])\n# oneaccess_perpark.head()\n\nB. assign metric as sum[park size]\n\noneaccess_perpark.describe()\noneaccess_perpark.columns\n\nIndex(['from_id', 'to_id', 'travel_time', 'cum_15', 'id',\n       'refToGreenspaceSite'],\n      dtype='object')\n\n\n\n# generate df with area per site\noneaccess_perpark_witharea = pd.merge(\n    oneaccess_perpark[[\"from_id\", \"refToGreenspaceSite\"]],\n    together_again[[\"id\", \"area_m2\"]],\n    left_on=\"refToGreenspaceSite\",\n    right_on=\"id\",\n)\noneaccess_perpark_witharea.head()\n\n\n\n\n\n\n\n\nfrom_id\nrefToGreenspaceSite\nid\narea_m2\n\n\n\n\n0\nE00170026\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n1\nE00041915\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n2\nE00041910\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n3\nE00041941\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n4\nE00170027\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n\n\n\n\n\n\n# generate df with one row pr OA and sum of sites' areas\nOAs_metric = (\n    oneaccess_perpark_witharea.groupby([\"from_id\"])[\"area_m2\"].sum().reset_index()\n)\n\n\nOAs_metric.head()\n\n\n\n\n\n\n\n\nfrom_id\narea_m2\n\n\n\n\n0\nE00041363\n526165.89095\n\n\n1\nE00041364\n521458.94910\n\n\n2\nE00041366\n311400.41615\n\n\n3\nE00041367\n505729.33200\n\n\n4\nE00041368\n325226.25195\n\n\n\n\n\n\n\n\nOAs_metric.to_csv(\"../output/acc_walking_gs15_OAtoGS_tynewear.csv\")\n\n\n# plotting results\noas_boundaries_metric = oas_boundaries_wgs84.merge(\n    OAs_metric, left_on=\"geo_code\", right_on=\"from_id\", how=\"right\"\n)\n\n\noas_boundaries_metric.explore(\n    column=\"area_m2\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "code/01_baseline/02_air_quality.html#exploration-of-air-quality-data-from-newcastle-urban-observatory",
    "href": "code/01_baseline/02_air_quality.html#exploration-of-air-quality-data-from-newcastle-urban-observatory",
    "title": "Appendix C — Air quality",
    "section": "C.1 Exploration of air quality data from Newcastle urban observatory",
    "text": "C.1 Exploration of air quality data from Newcastle urban observatory\nDownloaded from https://archive.dev.urbanobservatory.ac.uk/agg/900/ aggregated for 15 minutes blocks. We can also get coarser aggregation (hour, 6 hours, day) or even raw minute data.\n\nimport pandas as pd\nimport geopandas as gpd\nimport xarray as xr\nimport contextily as ctx\n\nSpecify paths to existing files, some of which need to be downloaded manually (see Notes).\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\n\nlsoa_list_file = (\n    f\"{data_folder}/processed/tynewear_lsoas_list.csv\"  # list of LSOAs within AoI\n)\naoi_file = f\"{data_folder}/processed/authorities/\"\n\n# national level files\nLSOA_boundaries_file = f\"{data_folder}/raw/LSOA_(Dec_2011)_Boundaries_Super_Generalised_Clipped_(BSC)_EW_V3.geojson\"  # administrative boundaries from gov.uk OS\n\nLoad the data for CO for December to see the structure\n\nco_dec22 = pd.read_csv(f\"{data_folder}/raw/air_quality/2022-12-CO-900.0.csv.zip\")\n\nGenerate actual geometry\n\nco_dec22 = gpd.GeoDataFrame(\n    co_dec22, geometry=gpd.GeoSeries.from_wkt(co_dec22[\"Location (WKT)\"], crs=4326)\n).to_crs(27700)\n\n\nco_dec22.head()\n\n\n\n\n\n\n\n\nSensor Name\nVariable\nUnits\nTimestamp\nMean Value\nMin Value\nMax Value\nMedian Value\nStandard Dev\nCount Records\nLocation (WKT)\nGround Height Above Sea Level\nSensor Height Above Ground\nBroker Name\nThird Party\nSensor Centroid Longitude\nSensor Centroid Latitude\nRaw ID\ngeometry\n\n\n\n\n0\nPER_AIRMON_MESH1972150\nCO\nugm -3\n2022-12-01 00:00:00\n417.865689\n406.879185\n491.198130\n410.974850\n20.274449\n15.0\nPOINT (-1.617121 54.974193)\n47.290001\n2.0\naq_mesh_api\nFalse\n-1.617121\n54.974193\n79208\nPOINT (424607.499 564463.931)\n\n\n1\nPER_AIRMON_MESH1907150\nCO\nugm -3\n2022-12-01 00:00:00\n630.707678\n614.071515\n678.104495\n627.741670\n17.546116\n15.0\nPOINT (-1.603448 54.965679)\n31.170000\n2.0\naq_mesh_api\nFalse\n-1.603448\n54.965679\n79150\nPOINT (425488.062 563521.339)\n\n\n2\nPER_AIRMON_MESH1968150\nCO\nugm -3\n2022-12-01 00:00:00\n1.257286\n1.196525\n1.766735\n1.217135\n0.138612\n15.0\nPOINT (-1.614848 54.97149)\n42.040001\n2.0\naq_mesh_api\nFalse\n-1.614848\n54.971490\n79204\nPOINT (424754.639 564163.933)\n\n\n3\nPER_AIRMON_MESH1969150\nCO\nugm -3\n2022-12-01 00:00:00\n343.280313\n337.130365\n364.159235\n342.825595\n6.512988\n15.0\nPOINT (-1.593537 55.039309)\n64.440002\n2.0\naq_mesh_api\nFalse\n-1.593537\n55.039309\n79205\nPOINT (426075.104 571718.592)\n\n\n4\nPER_AIRMON_MONITOR1157100\nCO\nugm -3\n2022-12-01 00:00:00\n0.179765\n0.179765\n0.179765\n0.179765\n0.000000\n1.0\nPOINT (-1.623043 54.970093)\n45.270000\n2.0\naq_mesh_api\nFalse\n-1.623043\n54.970093\n79565\nPOINT (424230.858 564005.621)\n\n\n\n\n\n\n\nWe have classic summary stats for the CO values, each per 15 minute slot. We can get those for every month back to roughly 2016.\nThe same can be done for the other metrics:\n\nCO (this one)\nNO\nNO2\nNOx\nO3\nPM 4\nPM 1\nPM 10\nPM 2.5\nParticle count\n\nGiven we are dealing with rather static scenarios, the target indicator in the modelling process needs to be an aggregation of some sort per a longer period of time (a year?).\nFurther, we may not want to model each metric individually, so there may be a scope for a creation of an index of air quality combining all these into a single metric that will be then predicted. Or we can predict them all…\n\naoi = gpd.read_file(aoi_file)\n\n\nm = co_dec22.loc[~co_dec22[\"Location (WKT)\"].duplicated()].explore(marker_type=\"marker\")\naoi.boundary.explore(m=m, color=\"black\")\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe main issue with the dataset is its coverage. As shown above, it is very limited and even if we wanted to interpolate the data we wouldn’t be able to do so south of the city. UO has other sensors there but not those for air quality."
  },
  {
    "objectID": "code/01_baseline/02_air_quality.html#eea-based-index",
    "href": "code/01_baseline/02_air_quality.html#eea-based-index",
    "title": "Appendix C — Air quality",
    "section": "C.2 EEA-based index",
    "text": "C.2 EEA-based index\nEEA specifies the relationship between concentration of different pollutants to get an index of hazard for human health (here). We can use their multipliers to create a single index based on the UK AIR data. From EEA:\n\nThe bands are based on the relative risks associated to short-term exposure to PM2.5, O3 and NO2, as defined by the World Health Organization in its report on the Health Risks of Air Pollution in Europe project (HRAPIE project report).\nThe relative risk of exposure to PM2.5 is taken as basis for driving the index, specifically the increase in the risk of mortality per 10 µg/m3 increase in the daily mean concentration of PM2.5.\nAssuming linearity across the relative risks functions for O3 and NO2, we calculate the concentrations of these pollutants that pose an equivalent relative risk to a 10 µg/m3 increase in the daily mean of PM2.5.\nFor PM10 concentrations, a constant ratio between PM10 and PM2.5 of 1:2 is assumed, in line with the World Health Organization´s air quality guidelines for Europe.\nFor SO2, the bands reflect the limit values set under the EU Air Quality Directive.\n\nThe relationship between \\(PM_{2.5}\\) : \\(PM_{10}\\) : \\(NO_{2}\\) : \\(O_{3}\\) : \\(SO_{2}\\) is then equal to 1 : 2 : 4 : 5 : 10. The combined index can then be computed as \\[Q_{air} = \\frac{PM_{2.5}}{1} + \\frac{PM_{10}}{2} + \\frac{NO_{2}}{4} + \\frac{O_{3}}{5} + \\frac{SO_{2}}{10}\\]\nExcept for the \\(O_{3}\\), UK AIR reports all as a concentration in \\(\\mu g \\cdot m^{-3}\\). \\(O_{3}\\) is reported as a number of days above a threshold of 120 \\(\\mu g \\cdot m^{-3}\\) and cannot be used in this formula but even EEA omits data when unavailable so we shall be able to create the index based on 4 remaning measurements.\nIt shall also be condsidered that UK AIR is not representing a direct measurements but a model.\n\nC.2.1 Data\nData is available from DEFRA (https://uk-air.defra.gov.uk/data/pcm-data) as CSVs in a UK grid.\n\nThe data begins on row 6 of each file and contains 4 columns: - a unique code (ukgridcode) for each 1x1km cell in the map - the x coordinates for the centroid of each grid cell - the y coordinates for the centroid of each grid cell - the values for the metric itself.\nThe coordinate system is OSGB and the coordinates represent the centre of each 1x1km cell.\nThere are some grid squares in each data set that do not have values associated with them and are labelled as ‘MISSING’\n\nWe can use pandas to read grids to Xarray for efficient array computation of the index.\n\npm10_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm102021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npm25_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm252021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nno2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapno22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nso2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapso22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npollutants_2021 = xr.merge([pm10_21, pm25_21, no2_21, so2_21])\npollutants_2021\n\nWe can compute the index based on the formula above.\n\naqi = (\n    pollutants_2021.pm252021g\n    + pollutants_2021.pm102021g / 2\n    + pollutants_2021.no22021 / 4\n    + pollutants_2021.so22021 / 10\n)\npollutants_2021 = pollutants_2021.assign(aqi=aqi)\n\nWe can filter only the area of interest before converting to a GeoDataFrame.\n\nbds = aoi.total_bounds\npollutants_2021_tyne = pollutants_2021.sel(\n    x=slice(bds[0], bds[2]), y=slice(bds[1], bds[3])\n)\n\nWe convert the array to a GeoDataFrame with polygons representing grid area. This will be needed for areal interpolation to LSOA/MSOA.\n\npollutants_2021_tyne_df = pollutants_2021_tyne.to_dataframe().reset_index()\npollutants_2021_tyne_df = gpd.GeoDataFrame(\n    pollutants_2021_tyne_df,\n    geometry=gpd.points_from_xy(\n        pollutants_2021_tyne_df.x, pollutants_2021_tyne_df.y, crs=27700\n    ).buffer(500, cap_style=3),\n)\n\nWe can check how the derived index looks.\n\npollutants_2021_tyne_df.explore(\"aqi\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\npollutants_2021_tyne_df.to_file(\n    f\"{data_folder}/processed/air_quality/air_quality_grid_2021.gpkg\"\n)"
  },
  {
    "objectID": "code/01_baseline/03_house_prices.html#process-house-prices-and-store-them-on-lsoa-level",
    "href": "code/01_baseline/03_house_prices.html#process-house-prices-and-store-them-on-lsoa-level",
    "title": "Appendix D — House price",
    "section": "D.1 Process house prices and store them on LSOA level",
    "text": "D.1 Process house prices and store them on LSOA level\nThis notebook processes a dataset containing a time-series of house price per LSOA and saves it as a GeoPackage for future use.\n\nimport datetime\n\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nSpecify paths to existing files, some of which need to be downloaded manually (see Notes).\n\nhttps://www.ons.gov.uk/peoplepopulationandcommunity/housing/datasets/medianpricepaidbylowerlayersuperoutputareahpssadataset46\nhttps://geoportal.statistics.gov.uk/datasets/2fb4e13605cb4745992390165307697e_0/explore?location=52.755877%2C-2.489798%2C7.62\n\n\nlsoa_list_file = (\n    f\"{data_folder}/processed/tynewear_lsoas_list.csv\"  # list of LSOAs within AoI\n)\n\n# national level files\nhouse_prices_lsoa_xls_file = f\"{data_folder}/raw/house_prices/hpssadataset46medianpricepaidforresidentialpropertiesbylsoa.xlsx\"  # excel spreadsheet downloaded from gov website (see Notes) with median house prices, quarterly data from Dec 1995\nLSOA_boundaries_file = f\"{data_folder}/raw/LSOA_(Dec_2011)_Boundaries_Super_Generalised_Clipped_(BSC)_EW_V3.geojson\"  # administrative boundaries from gov.uk OS\n\nRead house prices for whole country. The actual table starts with row 5 in sheet “Data”. ONS uses : to indicate missing value so we shall filter it on import.\n\nhouse_prices = pd.read_excel(\n    house_prices_lsoa_xls_file, \"Data\", header=5, na_values=\":\"\n)\n\nRead LSOA boundaries to link to data.\n\nlsoa = gpd.read_file(LSOA_boundaries_file)\n\nCheck structure before merge.\n\nlsoa.head()\n\n\n\n\n\n\n\n\nOBJECTID\nLSOA11CD\nLSOA11NM\nLSOA11NMW\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Area\nShape__Length\nGlobalID\ngeometry\n\n\n\n\n0\n1\nE01000001\nCity of London 001A\nCity of London 001A\n532129\n181625\n-0.097060\n51.51810\n157794.481079\n1685.391778\nb12173a3-5423-4672-a5eb-f152d2345f96\nPOLYGON ((-0.09474 51.52060, -0.09546 51.51544...\n\n\n1\n2\nE01000002\nCity of London 001B\nCity of London 001B\n532480\n181699\n-0.091970\n51.51868\n164882.427628\n1804.828196\n90274dc4-f785-4afb-95cd-7cc1fc9a2cad\nPOLYGON ((-0.08810 51.51941, -0.09546 51.51544...\n\n\n2\n3\nE01000003\nCity of London 001C\nCity of London 001C\n532245\n182036\n-0.095230\n51.52176\n42219.805717\n909.223277\n7e89d0ba-f186-45fb-961c-8f5ffcd03808\nPOLYGON ((-0.09453 51.52205, -0.09274 51.52139...\n\n\n3\n4\nE01000005\nCity of London 001E\nCity of London 001E\n533581\n181265\n-0.076280\n51.51452\n212682.404259\n2028.654904\na14c307a-874c-4862-828a-3b1486cc21ea\nPOLYGON ((-0.07589 51.51590, -0.07394 51.51445...\n\n\n4\n5\nE01000006\nBarking and Dagenham 016A\nBarking and Dagenham 016A\n544994\n184276\n0.089318\n51.53876\n130551.387161\n1716.896118\n65121a2d-3d2b-4935-9712-690f2993cfd2\nPOLYGON ((0.09328 51.53787, 0.09363 51.53767, ...\n\n\n\n\n\n\n\n\nhouse_prices.head()\n\n\n\n\n\n\n\n\nLocal authority code\nLocal authority name\nLSOA code\nLSOA name\nYear ending Dec 1995\nYear ending Mar 1996\nYear ending Jun 1996\nYear ending Sep 1996\nYear ending Dec 1996\nYear ending Mar 1997\n...\nYear ending Mar 2020\nYear ending Jun 2020\nYear ending Sep 2020\nYear ending Dec 2020\nYear ending Mar 2021\nYear ending Jun 2021\nYear ending Sep 2021\nYear ending Dec 2021\nYear ending Mar 2022\nYear ending Jun 2022\n\n\n\n\n0\nE06000001\nHartlepool\nE01011949\nHartlepool 009A\n34750.0\n34500.0\n30500.0\n30000.0\n29950.0\n29000.0\n...\n89000.0\n84000.0\n88000.0\n88000.0\n81500.0\n80500.0\n85500.0\n100750.0\n100000.0\n121000.0\n\n\n1\nE06000001\nHartlepool\nE01011950\nHartlepool 008A\n25000.0\n25000.0\n25300.0\n25625.0\n25000.0\n24800.0\n...\n27000.0\n26000.0\n28500.0\n30000.0\n33000.0\n47000.0\n50079.0\n50159.0\n50159.0\n49000.0\n\n\n2\nE06000001\nHartlepool\nE01011951\nHartlepool 007A\n27000.0\n27000.0\n27250.0\n28950.0\n28500.0\n28950.0\n...\n40000.0\n29425.0\n30000.0\n50000.0\n51500.0\n53000.0\n58573.5\n60000.0\n61499.5\n60000.0\n\n\n3\nE06000001\nHartlepool\nE01011952\nHartlepool 002A\n44500.0\n44500.0\n30000.0\n26675.0\n26000.0\n25500.0\n...\n70000.0\n66475.0\n85000.0\n85000.0\nNaN\n83500.0\n83000.0\n80000.0\n75500.0\n75000.0\n\n\n4\nE06000001\nHartlepool\nE01011953\nHartlepool 002B\n22000.0\n27000.0\n27000.0\n20600.0\n20000.0\n19500.0\n...\n58000.0\n60000.0\nNaN\nNaN\nNaN\nNaN\nNaN\n90000.0\nNaN\n95000.0\n\n\n\n\n5 rows × 111 columns\n\n\n\nMerge the data on LSOA codes.\n\nlsoa = lsoa.merge(house_prices, right_on=\"LSOA code\", left_on=\"LSOA11CD\", how=\"left\")\n\nRead the list of LSOA belonging to the area of interest.\n\ntyne_lsoa = pd.read_csv(lsoa_list_file, index_col=0)\n\nFilter the whole country.\n\ntyne_prices = lsoa[lsoa.LSOA11CD.isin(tyne_lsoa[\"LSOA code\"])]\n\n\ntyne_prices.head()\n\n\n\n\n\n\n\n\nOBJECTID\nLSOA11CD\nLSOA11NM\nLSOA11NMW\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Area\nShape__Length\n...\nYear ending Mar 2020\nYear ending Jun 2020\nYear ending Sep 2020\nYear ending Dec 2020\nYear ending Mar 2021\nYear ending Jun 2021\nYear ending Sep 2021\nYear ending Dec 2021\nYear ending Mar 2022\nYear ending Jun 2022\n\n\n\n\n7948\n7949\nE01008162\nGateshead 027A\nGateshead 027A\n426408\n562669\n-1.58915\n54.95797\n4.337674e+05\n2795.514484\n...\n124000.0\n107500.0\n97000.0\n97000.0\n100000.0\n137260.0\n137510.0\n107000.0\n108250.0\n99500.0\n\n\n7949\n7950\nE01008163\nGateshead 028A\nGateshead 028A\n425687\n562359\n-1.60044\n54.95522\n3.886396e+05\n2960.341729\n...\n113500.0\n105000.0\n124000.0\n138000.0\n138975.0\n138000.0\n121500.0\n107000.0\n107000.0\n115000.0\n\n\n7950\n7951\nE01008164\nGateshead 027B\nGateshead 027B\n426855\n562213\n-1.58222\n54.95385\n1.853580e+05\n2097.848575\n...\n76250.0\n87500.0\n66475.0\n62000.0\n60000.0\n56250.0\n65000.0\n84925.0\n125000.0\n140000.0\n\n\n7951\n7952\nE01008165\nGateshead 011A\nGateshead 011A\n426340\n562156\n-1.59026\n54.95336\n2.549571e+05\n2523.273552\n...\n79250.0\n81250.0\n83250.0\n105000.0\n120000.0\n123500.0\n147250.0\n119500.0\n112630.0\n125260.0\n\n\n7952\n7953\nE01008166\nGateshead 027C\nGateshead 027C\n426244\n563319\n-1.59166\n54.96382\n1.412473e+06\n4863.402280\n...\n145750.0\n149000.0\n145975.0\n136000.0\n139025.0\n138050.0\n139025.0\n150500.0\n173500.0\n177000.0\n\n\n\n\n5 rows × 123 columns\n\n\n\nSimplify columns names\n\ntyne_prices.columns = [c.replace(\"Year ending \", \"\") for c in tyne_prices.columns]\n\n\ntyne_prices.columns\n\nIndex(['OBJECTID', 'LSOA11CD', 'LSOA11NM', 'LSOA11NMW', 'BNG_E', 'BNG_N',\n       'LONG', 'LAT', 'Shape__Area', 'Shape__Length',\n       ...\n       'Mar 2020', 'Jun 2020', 'Sep 2020', 'Dec 2020', 'Mar 2021', 'Jun 2021',\n       'Sep 2021', 'Dec 2021', 'Mar 2022', 'Jun 2022'],\n      dtype='object', length=123)\n\n\nSave to GeoPackage\n\ntyne_prices.reset_index(drop=True).to_file(\n    f\"{data_folder}/house_prices_lsoa_jun22.gpkg\"\n)\n\nQuickly check the distribution. Housing prices are often better to model as log.\n\ntyne_prices[\"Jun 2022\"].plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThere is a long tail but not that critical. Let’s check logged.\n\nnp.log(tyne_prices[\"Jun 2022\"]).plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nNow we have some outliers on lower end. Will need to decide how to pass the data to the modelling process, some additional preprocessing may be needed.\n\ntyne_prices[\"Jun 2022\"].describe()\n\ncount       694.000000\nmean     160843.278818\nstd       72681.078875\nmin       26000.000000\n25%      114612.500000\n50%      139972.500000\n75%      186937.500000\nmax      705500.000000\nName: Jun 2022, dtype: float64\n\n\nThe problem with this dataset is that it is not normalised, so we have values per whole house without acknowledging its size or any other variable.\nWe can potentially get other measures - mean, lower quartile and tenth percentile price, plus counts of sales per type but at this level of aggregation, it probably won’t help a lot. The optimal would be a measure per sq.m. or access to other variables allowing us to model it but that does not seem to be available."
  },
  {
    "objectID": "code/01_baseline/03_house_prices.html#a-new-attribute-linked-residential-property-price-dataset-for-england-and-wales-2011-2019",
    "href": "code/01_baseline/03_house_prices.html#a-new-attribute-linked-residential-property-price-dataset-for-england-and-wales-2011-2019",
    "title": "Appendix D — House price",
    "section": "D.2 A new attribute-linked residential property price dataset for England and Wales 2011-2019",
    "text": "D.2 A new attribute-linked residential property price dataset for England and Wales 2011-2019\nWe can use https://reshare.ukdataservice.ac.uk/854942/ that derived a price per sqm from link between sales data and EPC.\n\n# It is 6.3GB...\nlinked_epc_path = \"https://reshare.ukdataservice.ac.uk/854942/1/tranall2011_19.csv\"\n\nFilter only Tyne and Wear for the last two years (2018-19).\n\nepc = pd.read_csv(linked_epc_path)\nepc[\"dateoftransfer\"] = pd.to_datetime(epc.dateoftransfer)\nlast2years = epc[epc.dateoftransfer &gt; datetime.datetime(2018, 1, 1)]\ntyne = last2years[last2years.county == \"TYNE AND WEAR\"]\ntyne\n\n/tmp/ipykernel_3533493/3103178836.py:1: DtypeWarning: Columns (40,60) have mixed types. Specify dtype option on import or set low_memory=False.\n  epc = pd.read_csv(\"https://reshare.ukdataservice.ac.uk/854942/1/tranall2011_19.csv\")\n\n\n\n\n\n\n\n\n\nid\ntransactionid\noa11\npostcode\nprice\ndateoftransfer\npropertytype\noldnew\nduration\npaon\n...\nMAIN_FUEL\nWIND_TURBINE_COUNT\nHEAT_LOSS_CORRIDOOR\nUNHEATED_CORRIDOR_LENGTH\nFLOOR_HEIGHT\nPHOTO_SUPPLY\nSOLAR_WATER_HEATING_FLAG\nMECHANICAL_VENTILATION\nLOCAL_AUTHORITY_LABEL\nCONSTITUENCY_LABEL\n\n\n\n\n938059\n12825134\n{8A78B2B0-1AE4-5CB0-E053-6B04A8C0F504}\nE00042737\nNE3 1QX\n162000.0\n2019-04-24\nF\nN\nL\n132\n...\nmains gas (not community)\n0\nno corridor\nNaN\n3.07\n0.0\nNaN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne North\n\n\n938062\n12825155\n{75050A85-C3A2-9A88-E053-6B04A8C02390}\nE00042719\nNE3 4RQ\n363200.0\n2018-08-02\nT\nN\nF\n2\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne Central\n\n\n938067\n12825169\n{75050A85-C385-9A88-E053-6B04A8C02390}\nE00042710\nNE3 1NN\n205000.0\n2018-07-18\nF\nN\nL\nFERNDENE COURT\n...\nmains gas (not community)\n0\nunheated corridor\n7.8\nNaN\nNaN\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne North\n\n\n938068\n12825170\n{68FEB20C-0D16-38DA-E053-6C04A8C051AE}\nE00042820\nNE4 6BA\n160000.0\n2018-03-15\nF\nN\nL\nBARRACK COURT\n...\nmains gas - this is for backwards compatibilit...\n0\nno corridor\nNaN\n2.28\n0.0\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne Central\n\n\n938082\n12825219\n{79A74E22-3FBD-1289-E053-6B04A8C01627}\nE00042388\nNE3 2HA\n176500.0\n2018-09-24\nF\nN\nL\n46\n...\nmains gas (not community)\n0\nno corridor\nNaN\nNaN\nNaN\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne North\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1543044\n14846632\n{8F1B26BE-496B-53DB-E053-6C04A8C03649}\nE00041827\nNE40 3RT\n200000.0\n2019-07-16\nT\nN\nL\n15\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nGateshead\nBlaydon\n\n\n1543047\n14846641\n{965B6D91-FF99-95E4-E053-6C04A8C07729}\nE00041731\nNE10 9SA\n135000.0\n2019-10-04\nS\nN\nL\n55\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nGateshead\nGateshead\n\n\n1543048\n14846652\n{8F1B26BE-47B9-53DB-E053-6C04A8C03649}\nE00041511\nNE9 7TQ\n179950.0\n2019-07-05\nD\nN\nF\n1\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nGateshead\nGateshead\n\n\n3053559\n3043457\n{93E6821E-E6ED-40FD-E053-6B04A8C0C1DF}\nE00104805\nNE17 7TB\n310000.0\n2019-09-09\nD\nN\nF\nSCHOOL HOUSE\n...\nbulk wood pellets\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nCounty Durham\nNorth West Durham\n\n\n3068117\n3098279\n{6DA0844A-764F-30F2-E053-6B04A8C05F3B}\nE00104812\nNE17 7QA\n81000.0\n2018-03-12\nT\nN\nF\n6\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nCounty Durham\nNorth West Durham\n\n\n\n\n20143 rows × 105 columns\n\n\n\nLink to geometry\n\noa = gpd.read_file(\n    \"https://borders.ukdataservice.ac.uk/ukborders/easy_download/prebuilt/shape/infuse_oa_lyr_2011_clipped.zip\"\n)\ntyne_agg = (\n    tyne[[\"oa11\", \"priceper\", \"numberrooms\", \"price\", \"tfarea\"]]\n    .groupby(\"oa11\")\n    .mean()\n    .reset_index()\n)\ntyne_oa = oa.merge(tyne_agg, left_on=\"geo_code\", right_on=\"oa11\", how=\"inner\")\ntyne_oa\n\nERROR 1: PROJ: proj_create_from_database: Open of /home/martin/mambaforge/envs/ulce/share/proj failed\n\n\n\n\n\n\n\n\n\ngeo_code\ngeometry\noa11\npriceper\nnumberrooms\nprice\ntfarea\n\n\n\n\n0\nE00042786\nPOLYGON ((428997.799 566018.331, 428998.491 56...\nE00042786\n1717.546956\n5.666667\n203158.333333\n124.333333\n\n\n1\nE00042707\nPOLYGON ((424221.655 568003.052, 424221.754 56...\nE00042707\n2933.816547\n4.066667\n341234.066667\n124.475333\n\n\n2\nE00042703\nPOLYGON ((419858.836 565454.433, 419858.374 56...\nE00042703\n1608.132502\n3.777778\n126666.666667\n84.666667\n\n\n3\nE00042782\nPOLYGON ((428932.199 566299.133, 428933.629 56...\nE00042782\n1864.265091\n5.000000\n179833.333333\n99.056667\n\n\n4\nE00042789\nPOLYGON ((428853.730 565689.295, 428860.602 56...\nE00042789\n1814.122597\n3.857143\n136528.571429\n75.697143\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3510\nE00041811\nPOLYGON ((428782.519 562282.321, 428783.470 56...\nE00041811\n1660.067873\n5.000000\n147500.000000\n86.000000\n\n\n3511\nE00041818\nPOLYGON ((416764.269 564158.045, 416764.216 56...\nE00041818\n2616.513428\n4.333333\n222333.333333\n81.800000\n\n\n3512\nE00041898\nPOLYGON ((423738.129 560286.484, 423738.500 56...\nE00041898\n1538.080092\n4.250000\n124487.500000\n81.070000\n\n\n3513\nE00041819\nPOLYGON ((415843.233 564846.473, 415868.526 56...\nE00041819\n2719.109422\n3.750000\n202417.000000\n75.750000\n\n\n3514\nE00041388\nPOLYGON ((426402.927 562379.016, 426417.019 56...\nE00041388\n1463.861134\n5.333333\n153000.000000\n103.333333\n\n\n\n\n3515 rows × 7 columns\n\n\n\n\ntyne_oa.to_file(\n    f\"{data_folder}/processed/house_prices/price_per_sqm.gpkg\", engine=\"pyogrio\"\n)\n\n\ntyne_oa.plot(\"priceper\", figsize=(12, 12), legend=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ntyne_oa.priceper.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\n\nnp.log(tyne_oa.priceper).plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nWe should be comfortably able to model log of price per sqm here."
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#air-quality",
    "href": "code/02_models/01_explanatory_variables.html#air-quality",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.1 Air Quality",
    "text": "E.1 Air Quality\nAir quality index is measured on a grid and needs to be interpolated.\nLoad the grid.\n\nair = gpd.read_file(f\"{data_folder}/processed/air_quality/air_quality_grid_2021.gpkg\")\n\nCheck the projections.\n\nair.crs == oa_aoi.crs\n\nTrue\n\n\nRun areal interpolation.\n\ninterp = tobler.area_weighted.area_interpolate(air, oa_aoi, intensive_variables=[\"aqi\"])\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: aqi, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n\n\nVisual check.\n\ninterp.plot(\"aqi\", figsize=(12, 12), legend=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nAssign the interpolated values to the OA dataframe.\n\noa_aoi[\"air_quality_index\"] = interp.aqi.values"
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#house-prices",
    "href": "code/02_models/01_explanatory_variables.html#house-prices",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.2 House prices",
    "text": "E.2 House prices\nHouse prices are reported on the OA level and can be merged via attribute.\nLoad data.\n\nhouse_price = gpd.read_file(f\"{data_folder}/processed/house_prices/price_per_sqm.gpkg\")\n\nMerge based on an OA code.\n\noa_aoi = oa_aoi.merge(house_price[[\"geo_code\", \"priceper\"]], on=\"geo_code\", how=\"left\")\n\nVisual check.\n\noa_aoi.plot(np.log(oa_aoi[\"priceper\"]), figsize=(12, 12), legend=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nQuestion: We need to figure out what to do with OAs that do not have any house price data. I would probably drop them before the training of ML. Interpolation may be tricky as we don’t know the exact reasons for their missingness (may be mostly parks…). We can also try to load older data there but given the dynamics of house pricing, that may cause more trouble than good.\n\nE.2.1 Job Accessibility\nJob accessibility is measured on the OA level and can be merged.\n\njobs = pd.read_csv(\n    f\"{data_folder}/outputs/results_accessibility/acc_jobs15_OAtoWPZ_tynewear.csv\",\n    index_col=0,\n)\njobs.head()\n\n\n\n\n\n\n\n\nfrom_id\nA_pop_cum_15\n\n\n\n\n0\nE00041363\n4318\n\n\n1\nE00041364\n7604\n\n\n2\nE00041366\n845\n\n\n3\nE00041367\n3413\n\n\n4\nE00041368\n1728\n\n\n\n\n\n\n\nMerge based on an attribute.\n\noa_aoi = oa_aoi.merge(jobs, left_on=\"geo_code\", right_on=\"from_id\", how=\"left\")\n\n\n\nE.2.2 Green space accessibility\nGreen space accessibility is measured on the OA level and can be merged.\n\ngs = pd.read_csv(\n    f\"{data_folder}/outputs/results_accessibility/acc_walking_gs15_OAtoGS_tynewear.csv\",\n    index_col=0,\n)\ngs.head()\n\n\n\n\n\n\n\n\nfrom_id\narea_m2\n\n\n\n\n0\nE00041363\n526165.89095\n\n\n1\nE00041364\n527070.65980\n\n\n2\nE00041366\n265337.36045\n\n\n3\nE00041367\n505729.33200\n\n\n4\nE00041368\n325226.25195\n\n\n\n\n\n\n\nMerge based on an attribute.\n\noa_aoi = oa_aoi.merge(gs, left_on=\"geo_code\", right_on=\"from_id\", how=\"left\")"
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#explanatory-variables",
    "href": "code/02_models/01_explanatory_variables.html#explanatory-variables",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.3 Explanatory variables",
    "text": "E.3 Explanatory variables\nWe also want to have all our explanatory variables linked to the OA geometries.\n\nE.3.1 Population estimates\nONS population estimates are reported on the OA level and can be merged.\nRead the file from ONS.\n\npop = pd.read_excel(\n    pooch.retrieve(\n        url=\"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/censusoutputareaestimatesinthenortheastregionofengland/mid2020sape23dt10d/sape23dt10dmid2020coaunformattedsyoaestimatesnortheast.xlsx\",\n        known_hash=None,\n    ),\n    sheet_name=\"Mid-2020 Persons\",\n    skiprows=4,\n)\n\nAttribute join.\n\noa_aoi = oa_aoi.merge(\n    pop[[\"OA11CD\", \"All Ages\"]], left_on=\"geo_code\", right_on=\"OA11CD\", how=\"left\"\n)\n\nVisual check.\n\noa_aoi.plot(\"All Ages\", figsize=(12, 12), legend=True, scheme=\"naturalbreaks\")\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nCheck of the distribution.\n\noa_aoi[\"All Ages\"].plot.hist(bins=500)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nWe will need to manage outliers prior modeling here.\n\n\nE.3.2 Workplace population\nWorkplace population is reported on Workplace Zones and needs to be interpolated to OA. We use the preprocessed data from the Urban Grammar project.\n\nwp = gpd.read_parquet(\n    f\"{data_folder}/raw/workplace_population/workplace_by_industry_gb.pq\"\n)\n\nWe use spatial query to filter only WPZs within the area of interest.\n\naoi_ix, wp_ix = wp.sindex.query_bulk(aoi.geometry, predicate=\"intersects\")\nwp_tyne = wp.iloc[np.unique(wp_ix)]\n\nVisual check.\n\nwp_tyne.plot(\n    wp_tyne.sum(axis=1, numeric_only=True),\n    figsize=(12, 12),\n    legend=True,\n    scheme=\"naturalbreaks\",\n)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nWe interpolate the data from WPZ to OA.\n\nwp_interpolated = tobler.area_weighted.area_interpolate(\n    wp_tyne,\n    oa_aoi,\n    extensive_variables=[\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ],\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nAnd assign the result to OA dataframe.\n\noa_aoi[\n    [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ]\n] = wp_interpolated.drop(columns=\"geometry\").values\n\n\n\nE.3.3 Land cover (CORINE)\nCORINE is shipped as custom polygons. We use the data downloaded for the Urban Grammar project.\n\ncorine = gpd.read_parquet(f\"{data_folder}/raw/land_cover/corine_gb.pq\")\n\nWe filter out only those polygons intersecting the area of interest.\n\naoi_ix, corine_ix = corine.sindex.query_bulk(aoi.geometry, predicate=\"intersects\")\ncorine_tyne = corine.iloc[np.unique(corine_ix)]\n\nWe interpolate the values to OA as a proportion covered by each type.\n\ncorine_interpolated = tobler.area_weighted.area_interpolate(\n    corine_tyne, oa_aoi, categorical_variables=[\"Code_18\"]\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nVisual check.\n\ncorine_interpolated.plot(\"Code_18_112\")\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nAssign land cover class names.\n\ncorine_names = {\n    \"Code_18_124\": \"Land cover [Airports]\",\n    \"Code_18_211\": \"Land cover [Non-irrigated arable land]\",\n    \"Code_18_121\": \"Land cover [Industrial or commercial units]\",\n    \"Code_18_421\": \"Land cover [Salt marshes]\",\n    \"Code_18_522\": \"Land cover [Estuaries]\",\n    \"Code_18_142\": \"Land cover [Sport and leisure facilities]\",\n    \"Code_18_141\": \"Land cover [Green urban areas]\",\n    \"Code_18_112\": \"Land cover [Discontinuous urban fabric]\",\n    \"Code_18_231\": \"Land cover [Pastures]\",\n    \"Code_18_311\": \"Land cover [Broad-leaved forest]\",\n    \"Code_18_131\": \"Land cover [Mineral extraction sites]\",\n    \"Code_18_123\": \"Land cover [Port areas]\",\n    \"Code_18_122\": \"Land cover [Road and rail networks and associated land]\",\n    \"Code_18_512\": \"Land cover [Water bodies]\",\n    \"Code_18_243\": \"Land cover [Land principally occupied by agriculture, with significant areas of natural vegetation]\",\n    \"Code_18_313\": \"Land cover [Mixed forest]\",\n    \"Code_18_412\": \"Land cover [Peat bogs]\",\n    \"Code_18_321\": \"Land cover [Natural grasslands]\",\n    \"Code_18_322\": \"Land cover [Moors and heathland]\",\n    \"Code_18_324\": \"Land cover [Transitional woodland-shrub]\",\n    \"Code_18_111\": \"Land cover [Continuous urban fabric]\",\n    \"Code_18_423\": \"Land cover [Intertidal flats]\",\n    \"Code_18_523\": \"Land cover [Sea and ocean]\",\n    \"Code_18_312\": \"Land cover [Coniferous forest]\",\n    \"Code_18_133\": \"Land cover [Construction sites]\",\n    \"Code_18_333\": \"Land cover [Sparsely vegetated areas]\",\n    \"Code_18_332\": \"Land cover [Bare rocks]\",\n    \"Code_18_411\": \"Land cover [Inland marshes]\",\n    \"Code_18_132\": \"Land cover [Dump sites]\",\n    \"Code_18_222\": \"Land cover [Fruit trees and berry plantations]\",\n    \"Code_18_242\": \"Land cover [Complex cultivation patterns]\",\n    \"Code_18_331\": \"Land cover [Beaches, dunes, sands]\",\n    \"Code_18_511\": \"Land cover [Water courses]\",\n    \"Code_18_334\": \"Land cover [Burnt areas]\",\n    \"Code_18_244\": \"Land cover [Agro-forestry areas]\",\n    \"Code_18_521\": \"Land cover [Coastal lagoons]\",\n}\n\ncorine_interpolated.columns = corine_interpolated.columns.map(corine_names)\n\nAssign the subset of classes that may be interesting for our purposes to OA dataframe.\n\ninteresting = [\n    \"Land cover [Discontinuous urban fabric]\",\n    \"Land cover [Continuous urban fabric]\",\n    \"Land cover [Non-irrigated arable land]\",\n    \"Land cover [Industrial or commercial units]\",\n    \"Land cover [Green urban areas]\",\n    \"Land cover [Pastures]\",\n    \"Land cover [Sport and leisure facilities]\",\n]\noa_aoi[interesting] = corine_interpolated[interesting].values\n\n\n\nE.3.4 Urban morphometrics\nMorphometric data is coming directly from the Urban Grammar project.\nWe need to first identify the chunks covering the AOI.\n\nchunks = gpd.read_parquet(f\"{data_folder}/raw/urban_morpho/local_auth_chunks.pq\")\n\nVisual exploration.\n\nm = chunks[chunks.intersects(aoi.unary_union)].explore()\naoi.explore(m=m, color=\"red\")\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n  return lib.intersects(a, b, **kwargs)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe need to work with the chunk 26.\n\ncells = gpd.read_parquet(f\"{data_folder}/raw/urban_morpho/cells_26.pq\")\n\nDrop columns we don’t need.\n\nchars = cells.columns.drop(\n    [\n        \"hindex\",\n        \"tessellation\",\n        \"buildings\",\n        \"nodeID\",\n        \"edgeID_keys\",\n        \"edgeID_values\",\n        \"edgeID_primary\",\n    ]\n)\n\nInterpolate values to OA.\n\nmorhp_interpolated = tobler.area_weighted.area_interpolate(\n    cells, oa_aoi, intensive_variables=chars.tolist()\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sdbAre, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sdbPer, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sdbCoA, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCCo, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCor, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbSqu, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbERI, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbElo, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCCM, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCCD, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: stbOri, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sicCAR, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: stbCeA, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: mtbAli, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: mtbNDi, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: stbSAl, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: lieWCe, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n\n\nAssign to the OA dataframe.\n\noa_aoi[morhp_interpolated.columns.drop(\"geometry\")] = morhp_interpolated.drop(\n    columns=\"geometry\"\n).values\n\nClean OA dataframe and rename some columns.\n\noa_aoi = oa_aoi.drop(columns=\"OA11CD\").rename(\n    columns={\n        \"priceper\": \"house_price_index\",\n        \"All Ages\": \"population_estimate\",\n        \"A_pop_cum_15\": \"jobs_accessibility_index\",\n        \"area_m2\": \"greenspace_accessibility_index\",\n    }\n)\n\nSave to Parquet file.\n\noa_aoi.to_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\n\noa_aoi.drop(columns=\"geometry\").to_csv(\n    f\"{data_folder}/processed/interpolated/all_oa.csv\", index=False\n)"
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#maps",
    "href": "code/02_models/01_explanatory_variables.html#maps",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.4 Maps",
    "text": "E.4 Maps\nWe can plot all the values as map for visualisation purposes.\nIn case we are loading the notebook again and want only plotting, we can load the OA dataframe directly here skipping most of the cells above.\n\noa_aoi = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nImport plotting libraries.\n\nimport contextily\nimport palettable.matplotlib as palmpl\nimport matplotlib.pyplot as plt\nimport matplotlib.cm\nimport mapclassify\nimport husl\nimport seaborn as sns\n\n\nfrom utils import legendgram\n\n\nE.4.0.1 Air Quality\nTo get the right extent, we create a box to ensure contextily loads the proper tiles.\n\nfrom shapely.geometry import box\n\nbds = oa_aoi.total_bounds\nextent = gpd.GeoSeries(\n    [box((bds[0] - 7000), bds[1], bds[2] + 7000, bds[3])], crs=oa_aoi.crs\n).to_crs(3857)\n\nPlot the data.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    \"air_quality_index\", scheme=\"equalinterval\", k=20, ax=ax, alpha=0.9, cmap=\"magma_r\"\n)\nbins = mapclassify.EqualInterval(oa_aoi[\"air_quality_index\"].values, k=20).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"air_quality_index\"],\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/air_quality_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\n\nE.4.0.2 House price\nFilling NAs just for plotting purposes.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    oa_aoi[\"house_price_index\"].fillna(method=\"pad\"),\n    scheme=\"naturalbreaks\",\n    k=10,\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nbins = mapclassify.NaturalBreaks(oa_aoi[\"house_price_index\"].dropna().values, k=10).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"house_price_index\"].dropna(),\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        oa_aoi[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/house_price_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/house_price_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\n\nE.4.0.3 Jobs accessibility\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    oa_aoi[\"jobs_accessibility_index\"].fillna(method=\"pad\"),\n    scheme=\"naturalbreaks\",\n    k=10,\n    ax=ax,\n    alpha=0.9,\n    cmap=\"plasma\",\n)\nbins = mapclassify.NaturalBreaks(\n    oa_aoi[\"jobs_accessibility_index\"].dropna().values, k=10\n).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"jobs_accessibility_index\"].dropna(),\n    bins,\n    pal=palmpl.Plasma_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    # clip = (10,20), # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/jobs_accessibility_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/jobs_accessibility_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    oa_aoi[\"greenspace_accessibility_index\"].fillna(method=\"pad\"),\n    scheme=\"naturalbreaks\",\n    k=10,\n    ax=ax,\n    alpha=0.9,\n    cmap=\"YlGn\",\n)\nbins = mapclassify.NaturalBreaks(\n    oa_aoi[\"greenspace_accessibility_index\"].dropna().values, k=10\n).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"greenspace_accessibility_index\"].dropna(),\n    bins,\n    pal=matplotlib.cm.get_cmap(\"YlGn\"),\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    # clip = (10,20), # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/greenspace_accessibility_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/greenspace_accessibility_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\n\nE.4.0.4 Explanatory variables\nSome explanatory variables are better to be plotted as densities.\n\nto_density = [\n    \"A, B, D, E. Agriculture, energy and water\",\n    \"C. Manufacturing\",\n    \"F. Construction\",\n    \"G, I. Distribution, hotels and restaurants\",\n    \"H, J. Transport and communication\",\n    \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n    \"O,P,Q. Public administration, education and health\",\n    \"R, S, T, U. Other\",\n    \"population_estimate\",\n]\n\nLoop over explanatory variables, assign a random hue, generate a colormap and plot.\n\nfor char in oa_aoi.columns.drop(\n    [\n        \"geometry\",\n        \"geo_code\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n    ]\n):\n    if char in to_density:\n        values = oa_aoi[char] / oa_aoi.area\n    else:\n        values = oa_aoi[char]\n\n    gdf = oa_aoi.to_crs(3857)\n    hue = np.random.randint(0, 360, 1)[0]\n    k = 10  # number of classes\n    clip = None  # clip=(0, 100)  # in case of long tail\n    mc = mapclassify.NaturalBreaks(values, k=k).bins  # classification scheme\n    bins = 50  # resolution of histogram\n\n    # plotting\n    color = (hue, 75, 35)\n    c = husl.husl_to_hex(*color)\n    cmap = sns.light_palette(color, input=\"husl\", as_cmap=True, n_colors=k)\n\n    ax = gdf.plot(\n        values,\n        cmap=cmap,\n        figsize=(18, 12),\n        scheme=\"UserDefined\",\n        k=k,\n        classification_kwds=dict(bins=mc),\n    )\n    extent.plot(ax=ax, alpha=0)\n    ax.set_axis_off()\n\n    # add legend\n    hax = legendgram(\n        plt.gcf(),  # grab the figure, we need it\n        ax,  # the axis to add the legend\n        values,  # the attribute to map\n        mc,\n        cmap,  # the palette to use\n        legend_size=(0.35, 0.15),\n        loc=4,\n        #    tick_params={'color':c, 'labelcolor':c,'labelsize':10},\n        clip=clip,\n        bins=bins,\n    )  # when usign legendgram, dev version from mpl_cmap branch should be used\n    contextily.add_basemap(\n        ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n    )\n    contextily.add_basemap(\n        ax=ax,\n        source=contextily.providers.Stamen.TonerLines,\n        alpha=0.4,\n        attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n    )\n    plt.savefig(\n        f\"{data_folder}/outputs/figures/svg/ex_var_{char}.svg\",\n        dpi=144,\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n\nsignatures = pd.read_parquet(\"../blackbox/data/oa_key.parquet\")\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    color=oa_aoi.merge(\n        signatures, how=\"left\", left_on=\"geo_code\", right_index=True\n    ).primary_type.map(ugg.get_signature_colors()),\n    ax=ax,\n    alpha=0.9,\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\n# contextily.add_basemap(ax=ax, source=contextily.providers.Stamen.TonerLines, alpha=.4, attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\")\n# plt.savefig(f\"{data_folder}/outputs/figures/greenspace_accessibility_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/signatures.svg\", dpi=144, bbox_inches=\"tight\"\n)"
  },
  {
    "objectID": "code/02_models/01b_latent_sentinel.html",
    "href": "code/02_models/01b_latent_sentinel.html",
    "title": "Appendix F — Sentinel 2 latent representation",
    "section": "",
    "text": "Process the data from the https://doi.org/10.1016/j.compenvurbsys.2022.101802 paper to be used in modelling.\n\nimport geopandas as gpd\nimport tobler\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_69158/2891901632.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas as gpd\n\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\noa = gpd.read_parquet(\n    f\"{data_folder}/processed/interpolated/all_oa.parquet\",\n    columns=[\"geo_code\", \"geometry\"],\n)\n\n\npostcodes = gpd.read_parquet(\n    f\"{data_folder}/raw/sentinel_latent/latent_smoothed.parquet\"\n)\n\nCreate geometry of original samples.\n\npostcodes.geometry = postcodes.buffer(80, cap_style=3)\n\nJoin with the OA in the area of interest.\n\nxmin, ymin, xmax, ymax = oa.total_bounds\npostcodes_aoi = postcodes.cx[xmin:xmax, ymin:ymax]\n\n\npostcodes_aoi = postcodes_aoi.sjoin(oa, how=\"inner\")\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/geopandas/geodataframe.py:2061: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\nUse `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n\nLeft CRS: EPSG:27700\nRight CRS: EPSG:27700\n\n  return geopandas.sjoin(left_df=self, right_df=df, *args, **kwargs)\n\n\nCreate a mean latent vector per OA.\n\nlatent_oa = (\n    postcodes_aoi.drop(columns=[\"geo_code\", \"geometry\"]).groupby(\"index_right\").mean()\n)\n\n\nlatent_oa\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\nindex_right\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n-0.682062\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.451931\n-0.211763\n0.387425\n0.928424\n...\n-1.021918\n-1.021918\n0.440444\n1.743971\n0.320842\n1.474930\n1.151639\n-0.132747\n1.498061\n0.871063\n\n\n1\n-0.663786\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.349480\n-0.311341\n0.504530\n0.902771\n...\n-1.021918\n-1.021918\n0.577340\n1.883114\n0.293000\n1.414315\n1.145569\n-0.093327\n1.454991\n0.880657\n\n\n2\n-0.657851\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.414436\n-0.291362\n0.512188\n0.857220\n...\n-1.021918\n-1.021918\n0.562472\n1.878608\n0.301625\n1.483689\n1.187823\n-0.086438\n1.513144\n0.918716\n\n\n3\n-0.666984\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.427887\n-0.262477\n0.397210\n0.949204\n...\n-1.021918\n-1.021918\n0.464777\n1.763278\n0.334995\n1.434697\n1.128427\n-0.116929\n1.466723\n0.861476\n\n\n4\n-0.679580\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.461694\n-0.156182\n0.458418\n0.925219\n...\n-1.021918\n-1.021918\n0.490356\n1.797309\n0.374750\n1.505300\n1.117723\n-0.139840\n1.487528\n0.887797\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3790\n-0.646544\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.448320\n-0.247494\n0.457923\n0.925764\n...\n-1.021918\n-1.021918\n0.596297\n1.882795\n0.314640\n1.466138\n1.180296\n-0.069748\n1.481510\n0.899854\n\n\n3791\n-0.641375\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.533849\n-0.143489\n0.441434\n0.966423\n...\n-1.021918\n-1.021918\n0.556728\n1.898314\n0.299499\n1.554656\n1.178569\n-0.087546\n1.507623\n0.944229\n\n\n3792\n-0.642435\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.508919\n-0.193876\n0.475717\n0.867998\n...\n-1.021918\n-1.021918\n0.583808\n1.872691\n0.381800\n1.518266\n1.206397\n-0.212747\n1.641260\n0.999635\n\n\n3793\n-0.626392\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.570091\n-0.141022\n0.387859\n0.993209\n...\n-1.021918\n-1.021918\n0.558174\n1.881344\n0.309562\n1.597636\n1.203249\n-0.086112\n1.549585\n0.941391\n\n\n3794\n-0.630401\n-1.021918\n-1.021918\n-1.021909\n-1.021918\n-1.021918\n0.507333\n-0.214628\n0.472556\n0.940042\n...\n-1.021918\n-1.021918\n0.606770\n1.923926\n0.276992\n1.529824\n1.194195\n-0.077461\n1.518514\n0.937542\n\n\n\n\n3795 rows × 64 columns\n\n\n\n\nlatent_oa = latent_oa.set_geometry(oa.geometry)\n\n\nlatent_oa.to_parquet(f\"{data_folder}/processed/sentinel/latent_oa.parquet\")"
  },
  {
    "objectID": "code/02_models/02_collinearity_filter.html",
    "href": "code/02_models/02_collinearity_filter.html",
    "title": "Appendix G — Filter collinear explanatory variables",
    "section": "",
    "text": "Some explanatory variables, primarily those coming from morphometrics, may be collinear within our limited area of interest. Those shall be removed prior modelling.\nThis notebook identifies correlations between variables and drops those that are correlated and less interpetable.\n\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport numpy as np\n\nSpecify a path to the data folder.\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"geo_code\",\n        \"geometry\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n    ]\n)\n\nMeasure Pearson’s and Spearman’s Rank correllations.\n\npearson = exvars.corr().abs()\nspearman = exvars.corr(\"spearman\").abs()\n\nFill the upper triangle to keep each pair only once.\n\npearson *= np.tri(*pearson.shape)\n\nRemove self.\n\nnp.fill_diagonal(pearson.values, 0)\n\nUnstack to get pairs.\n\npearson_pairs = pearson.unstack()\npearson_pairs[pearson_pairs &gt; 0.8]\n\nsdbAre  sdbPer    0.804675\nsdbPer  ssbCor    0.831300\nssbCCo  ssbElo    0.881746\nstbOri  stcOri    0.861596\nsdcLAL  sdcAre    0.897443\n        mtbNDi    0.925586\n        mdcAre    0.894625\n        sddAre    0.805920\n        ltcAre    0.821140\nsdcAre  mtbNDi    0.905940\n        mdcAre    0.946279\n        sddAre    0.875737\n        mdsAre    0.806880\nmtbNDi  mdcAre    0.922637\n        sddAre    0.841932\n        ltcAre    0.844478\nmdcAre  sddAre    0.886339\n        sdsAre    0.848497\n        mdsAre    0.853500\n        ltcAre    0.922004\nltcWRE  lcnClo    0.852001\nltbIBD  ltcAre    0.867520\nsdsSPW  sdsSPO    0.840288\nsdsLen  mtdMDi    0.937603\n        sdsAre    0.859295\nlcdMes  linPDE    0.862986\nmtdMDi  sddAre    0.805111\n        sdsAre    0.849652\n        mdsAre    0.811618\nlddNDe  linWID    0.944004\nsddAre  sdsAre    0.925253\n        mdsAre    0.889015\n        ltcAre    0.813191\nsdsAre  mdsAre    0.955687\n        ldsAre    0.883652\n        ltcAre    0.829096\nmisCel  lisCel    0.869592\nmdsAre  ldsAre    0.956400\n        ltcAre    0.837981\nldeAre  ldePer    0.872630\nldePer  lseCWA    0.964556\ndtype: float64\n\n\nThe same with Spearman\n\nspearman *= np.tri(*spearman.shape)\nnp.fill_diagonal(spearman.values, 0)\nspearman_pairs = spearman.unstack()\nspearman_pairs[spearman_pairs &gt; 0.8]\n\nG, I. Distribution, hotels and restaurants  K, L, M, N. Financial, real estate, professional and administrative activities    0.819823\n                                            R, S, T, U. Other                                                                 0.811958\nsdbAre                                      sdbPer                                                                            0.961472\n                                            ssbCCM                                                                            0.889109\nsdbPer                                      ssbCCM                                                                            0.951864\nssbCCo                                      ssbElo                                                                            0.871556\nssbCor                                      ssbCCD                                                                            0.901580\nstbOri                                      stcOri                                                                            0.858329\nsdcLAL                                      sdcAre                                                                            0.960952\n                                            mtcWNe                                                                            0.918613\n                                            mdcAre                                                                            0.928993\n                                            ltcRea                                                                            0.832384\n                                            ltcAre                                                                            0.830742\nsdcAre                                      mtcWNe                                                                            0.873712\n                                            mdcAre                                                                            0.957952\n                                            ltcRea                                                                            0.822674\n                                            ltcAre                                                                            0.841574\nmtcWNe                                      mdcAre                                                                            0.845915\nmdcAre                                      ltbIBD                                                                            0.822638\n                                            ltcRea                                                                            0.828665\n                                            ltcAre                                                                            0.933400\nltcWRE                                      lcnClo                                                                            0.815142\nltbIBD                                      ltcAre                                                                            0.862608\nsdsSPW                                      sdsSPO                                                                            0.821366\nsdsLen                                      mtdMDi                                                                            0.925780\nlcdMes                                      linPDE                                                                            0.890219\nlinPDE                                      ldsCDL                                                                            0.810251\nmtdMDi                                      sddAre                                                                            0.823618\nlddNDe                                      linWID                                                                            0.893646\nsddAre                                      sdsAre                                                                            0.905564\n                                            mdsAre                                                                            0.859780\nsdsAre                                      mdsAre                                                                            0.946564\n                                            ldsAre                                                                            0.809841\nmisCel                                      lisCel                                                                            0.909656\nmdsAre                                      ldsAre                                                                            0.916757\nldeAre                                      ldePer                                                                            0.971434\n                                            lseCWA                                                                            0.899233\nldePer                                      lseCWA                                                                            0.965517\nlseERI                                      lseCWA                                                                            0.812116\ndtype: float64\n\n\nCombine highly correlated pairs from both. We want to eliminate those with both indices above .8.\n\nhigh_both = (\n    pearson_pairs[pearson_pairs &gt; 0.8]\n    .to_frame(\"pearson\")\n    .assign(spearman=spearman_pairs[spearman_pairs &gt; 0.8])\n    .dropna()\n)\nhigh_both\n\n\n\n\n\n\n\n\n\npearson\nspearman\n\n\n\n\nsdbAre\nsdbPer\n0.804675\n0.961472\n\n\nssbCCo\nssbElo\n0.881746\n0.871556\n\n\nstbOri\nstcOri\n0.861596\n0.858329\n\n\nsdcLAL\nsdcAre\n0.897443\n0.960952\n\n\nmdcAre\n0.894625\n0.928993\n\n\nltcAre\n0.821140\n0.830742\n\n\nsdcAre\nmdcAre\n0.946279\n0.957952\n\n\nmdcAre\nltcAre\n0.922004\n0.933400\n\n\nltcWRE\nlcnClo\n0.852001\n0.815142\n\n\nltbIBD\nltcAre\n0.867520\n0.862608\n\n\nsdsSPW\nsdsSPO\n0.840288\n0.821366\n\n\nsdsLen\nmtdMDi\n0.937603\n0.925780\n\n\nlcdMes\nlinPDE\n0.862986\n0.890219\n\n\nmtdMDi\nsddAre\n0.805111\n0.823618\n\n\nlddNDe\nlinWID\n0.944004\n0.893646\n\n\nsddAre\nsdsAre\n0.925253\n0.905564\n\n\nmdsAre\n0.889015\n0.859780\n\n\nsdsAre\nmdsAre\n0.955687\n0.946564\n\n\nldsAre\n0.883652\n0.809841\n\n\nmisCel\nlisCel\n0.869592\n0.909656\n\n\nmdsAre\nldsAre\n0.956400\n0.916757\n\n\nldeAre\nldePer\n0.872630\n0.971434\n\n\nldePer\nlseCWA\n0.964556\n0.965517\n\n\n\n\n\n\n\nDefine variables to be dropped.\n\nto_drop = [\n    \"sdbPer\",\n    \"ssbElo\",\n    \"stcOri\",\n    \"sdcLAL\",\n    \"mdcAre\",\n    \"ltcAre\",\n    \"ltcWRE\",\n    \"mtdMDi\",\n    \"lcdMes\",\n    \"lddNDe\",\n    \"sddAre\",\n    \"mdsAre\",\n    \"ldsAre\",\n    \"lisCel\",\n    \"ldePer\",\n    \"lseCWA\",\n]\n\nCheck the result.\n\npearson_check = exvars.drop(columns=to_drop).corr().abs()\nspearman_check = exvars.drop(columns=to_drop).corr(\"spearman\").abs()\n\npearson_check *= np.tri(*pearson_check.shape)\nnp.fill_diagonal(pearson_check.values, 0)\npearson_check_pairs = pearson_check.unstack()\n\nspearman_check *= np.tri(*spearman_check.shape)\nnp.fill_diagonal(spearman_check.values, 0)\nspearman_check_pairs = spearman_check.unstack()\n\nhigh_both_check = (\n    pearson_check_pairs[pearson_check_pairs &gt; 0.8]\n    .to_frame(\"pearson\")\n    .assign(spearman=spearman_check_pairs[spearman_check_pairs &gt; 0.8])\n    .dropna()\n)\n\n\nhigh_both_check\n\n\n\n\n\n\n\n\n\npearson\nspearman\n\n\n\n\nsdsSPW\nsdsSPO\n0.840288\n0.821366\n\n\n\n\n\n\n\nThe street profile width - street profile openness pair is kept as there is not necessarily a logical (only empirical) relation between the two.\nCheck remaining high-correlation pairs if we consider only single index.\n\npearson_check_pairs[pearson_check_pairs &gt; 0.8]\n\nsdcAre  mtbNDi    0.905940\nsdsSPW  sdsSPO    0.840288\nsdsLen  sdsAre    0.859295\ndtype: float64\n\n\n\nspearman_check_pairs[spearman_check_pairs &gt; 0.8]\n\nG, I. Distribution, hotels and restaurants  K, L, M, N. Financial, real estate, professional and administrative activities    0.819823\n                                            R, S, T, U. Other                                                                 0.811958\nsdbAre                                      ssbCCM                                                                            0.889109\nssbCor                                      ssbCCD                                                                            0.901580\nsdcAre                                      mtcWNe                                                                            0.873712\n                                            ltcRea                                                                            0.822674\nsdsSPW                                      sdsSPO                                                                            0.821366\nlinPDE                                      ldsCDL                                                                            0.810251\ndtype: float64\n\n\nWe assume that these can stay in the dataset.\nDrop the collinear variables from original data.\n\ndata = data.drop(columns=to_drop)\n\nSave to file.\n\ndata.to_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nGet a table of all.\nKey to names of morphometric characters:\n\nkey = {\n    \"sdbAre\": \"area of building\",\n    \"sdbPer\": \"perimeter of building\",\n    \"sdbCoA\": \"courtyard area of building\",\n    \"ssbCCo\": \"circular compactness of building\",\n    \"ssbCor\": \"corners of building\",\n    \"ssbSqu\": \"squareness of building\",\n    \"ssbERI\": \"equivalent rectangular index of building\",\n    \"ssbElo\": \"elongation of building\",\n    \"ssbCCM\": \"centroid - corner mean distance of building\",\n    \"ssbCCD\": \"centroid - corner distance deviation of building\",\n    \"stbOri\": \"orientation of building\",\n    \"sdcLAL\": \"longest axis length of ETC\",\n    \"sdcAre\": \"area of ETC\",\n    \"sscCCo\": \"circular compactness of ETC\",\n    \"sscERI\": \"equivalent rectangular index of ETC\",\n    \"stcOri\": \"orientation of ETC\",\n    \"sicCAR\": \"covered area ratio of ETC\",\n    \"stbCeA\": \"cell alignment of building\",\n    \"mtbAli\": \"alignment of neighbouring buildings\",\n    \"mtbNDi\": \"mean distance between neighbouring buildings\",\n    \"mtcWNe\": \"perimeter-weighted neighbours of ETC\",\n    \"mdcAre\": \"area covered by neighbouring cells\",\n    \"ltcWRE\": \"weighted reached enclosures of ETC\",\n    \"ltbIBD\": \"mean inter-building distance\",\n    \"sdsSPW\": \"width of street profile\",\n    \"sdsSWD\": \"width deviation of street profile\",\n    \"sdsSPO\": \"openness of street profile\",\n    \"sdsLen\": \"length of street segment\",\n    \"sssLin\": \"linearity of street segment\",\n    \"ldsMSL\": \"mean segment length within 3 steps\",\n    \"mtdDeg\": \"node degree of junction\",\n    \"lcdMes\": \"local meshedness of street network\",\n    \"linP3W\": \"local proportion of 3-way intersections of street network\",\n    \"linP4W\": \"local proportion of 4-way intersections of street network\",\n    \"linPDE\": \"local proportion of cul-de-sacs of street network\",\n    \"lcnClo\": \"local closeness of street network\",\n    \"ldsCDL\": \"local cul-de-sac length of street network\",\n    \"xcnSCl\": \"square clustering of street network\",\n    \"mtdMDi\": \"mean distance to neighbouring nodes of street network\",\n    \"lddNDe\": \"local node density of street network\",\n    \"linWID\": \"local degree weighted node density of street network\",\n    \"stbSAl\": \"street alignment of building\",\n    \"sddAre\": \"area covered by node-attached ETCs\",\n    \"sdsAre\": \"area covered by edge-attached ETCs\",\n    \"sisBpM\": \"buildings per meter of street segment\",\n    \"misCel\": \"reached ETCs by neighbouring segments\",\n    \"mdsAre\": \"reached area by neighbouring segments\",\n    \"lisCel\": \"reached ETCs by local street network\",\n    \"ldsAre\": \"reached area by local street network\",\n    \"ltcRea\": \"reached ETCs by tessellation contiguity\",\n    \"ltcAre\": \"reached area by tessellation contiguity\",\n    \"ldeAre\": \"area of enclosure\",\n    \"ldePer\": \"perimeter of enclosure\",\n    \"lseCCo\": \"circular compactness of enclosure\",\n    \"lseERI\": \"equivalent rectangular index of enclosure\",\n    \"lseCWA\": \"compactness-weighted axis of enclosure\",\n    \"lteOri\": \"orientation of enclosure\",\n    \"lteWNB\": \"perimeter-weighted neighbours of enclosure\",\n    \"lieWCe\": \"area-weighted ETCs of enclosure\",\n}\n\nList all characters with names:\n\n[key[c] if c in key else c for c in exvars.drop(columns=to_drop).columns]\n\n['population_estimate',\n 'A, B, D, E. Agriculture, energy and water',\n 'C. Manufacturing',\n 'F. Construction',\n 'G, I. Distribution, hotels and restaurants',\n 'H, J. Transport and communication',\n 'K, L, M, N. Financial, real estate, professional and administrative activities',\n 'O,P,Q. Public administration, education and health',\n 'R, S, T, U. Other',\n 'Land cover [Discontinuous urban fabric]',\n 'Land cover [Continuous urban fabric]',\n 'Land cover [Non-irrigated arable land]',\n 'Land cover [Industrial or commercial units]',\n 'Land cover [Green urban areas]',\n 'Land cover [Pastures]',\n 'Land cover [Sport and leisure facilities]',\n 'area of building',\n 'courtyard area of building',\n 'circular compactness of building',\n 'corners of building',\n 'squareness of building',\n 'equivalent rectangular index of building',\n 'centroid - corner mean distance of building',\n 'centroid - corner distance deviation of building',\n 'orientation of building',\n 'area of ETC',\n 'circular compactness of ETC',\n 'equivalent rectangular index of ETC',\n 'covered area ratio of ETC',\n 'cell alignment of building',\n 'alignment of neighbouring buildings',\n 'mean distance between neighbouring buildings',\n 'perimeter-weighted neighbours of ETC',\n 'mean inter-building distance',\n 'width of street profile',\n 'width deviation of street profile',\n 'openness of street profile',\n 'length of street segment',\n 'linearity of street segment',\n 'mean segment length within 3 steps',\n 'node degree of junction',\n 'local proportion of 3-way intersections of street network',\n 'local proportion of 4-way intersections of street network',\n 'local proportion of cul-de-sacs of street network',\n 'local closeness of street network',\n 'local cul-de-sac length of street network',\n 'square clustering of street network',\n 'local degree weighted node density of street network',\n 'street alignment of building',\n 'area covered by edge-attached ETCs',\n 'buildings per meter of street segment',\n 'reached ETCs by neighbouring segments',\n 'reached ETCs by tessellation contiguity',\n 'area of enclosure',\n 'circular compactness of enclosure',\n 'equivalent rectangular index of enclosure',\n 'orientation of enclosure',\n 'perimeter-weighted neighbours of enclosure',\n 'area-weighted ETCs of enclosure']"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#air-quality",
    "href": "code/02_models/03_england-wide-data.html#air-quality",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.1 Air Quality",
    "text": "H.1 Air Quality\nRead DEFRA data.\n\npm10_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm102021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npm25_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm252021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nno2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapno22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nso2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapso22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npollutants_2021 = xr.merge([pm10_21, pm25_21, no2_21, so2_21])\npollutants_2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (x: 657, y: 1170)\nCoordinates:\n  * x          (x) int64 -500 500 1500 2500 3500 ... 652500 653500 654500 655500\n  * y          (y) int64 5500 6500 7500 8500 ... 1216500 1217500 1218500 1219500\nData variables:\n    pm102021g  (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nan\n    pm252021g  (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nan\n    no22021    (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nan\n    so22021    (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nanxarray.DatasetDimensions:x: 657y: 1170Coordinates: (2)x(x)int64-500 500 1500 ... 654500 655500array([  -500,    500,   1500, ..., 653500, 654500, 655500])y(y)int645500 6500 7500 ... 1218500 1219500array([   5500,    6500,    7500, ..., 1217500, 1218500, 1219500])Data variables: (4)pm102021g(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])pm252021g(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])no22021(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])so22021(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])Indexes: (2)xPandasIndexPandasIndex(Int64Index([  -500,    500,   1500,   2500,   3500,   4500,   5500,   6500,\n              7500,   8500,\n            ...\n            646500, 647500, 648500, 649500, 650500, 651500, 652500, 653500,\n            654500, 655500],\n           dtype='int64', name='x', length=657))yPandasIndexPandasIndex(Int64Index([   5500,    6500,    7500,    8500,    9500,   10500,   11500,\n              12500,   13500,   14500,\n            ...\n            1210500, 1211500, 1212500, 1213500, 1214500, 1215500, 1216500,\n            1217500, 1218500, 1219500],\n           dtype='int64', name='y', length=1170))Attributes: (0)\n\n\nWe can compute the index based on the formula.\n\naqi = (\n    pollutants_2021.pm252021g\n    + pollutants_2021.pm102021g / 2\n    + pollutants_2021.no22021 / 4\n    + pollutants_2021.so22021 / 10\n)\npollutants_2021 = pollutants_2021.assign(aqi=aqi)\n\nWe convert the array to a GeoDataFrame with polygons representing grid area. This will be needed for areal interpolation to LSOA/MSOA.\n\npollutants_2021 = pollutants_2021.to_dataframe().reset_index()\npollutants_2021 = gpd.GeoDataFrame(\n    pollutants_2021,\n    geometry=gpd.points_from_xy(pollutants_2021.x, pollutants_2021.y, crs=27700).buffer(\n        500, cap_style=3\n    ),\n)\n\n\npollutants_2021.to_parquet(\n    f\"{data_folder}/processed/air_quality/air_quality_grid_2021_england.parquet\"\n)\n\n\npollutants_2021\n\n\n\n\n\n\n\n\nx\ny\npm102021g\npm252021g\nno22021\nso22021\naqi\ngeometry\n\n\n\n\n0\n-500\n5500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 6000.000, 0.000 5000.000, -100...\n\n\n1\n-500\n6500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 7000.000, 0.000 6000.000, -100...\n\n\n2\n-500\n7500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 8000.000, 0.000 7000.000, -100...\n\n\n3\n-500\n8500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 9000.000, 0.000 8000.000, -100...\n\n\n4\n-500\n9500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 10000.000, 0.000 9000.000, -10...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n768685\n655500\n1215500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1216000.000, 656000.000 1...\n\n\n768686\n655500\n1216500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1217000.000, 656000.000 1...\n\n\n768687\n655500\n1217500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1218000.000, 656000.000 1...\n\n\n768688\n655500\n1218500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1219000.000, 656000.000 1...\n\n\n768689\n655500\n1219500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1220000.000, 656000.000 1...\n\n\n\n\n768690 rows × 8 columns\n\n\n\nInterpolate to OAs.\n\n%%time\ninterp = tobler.area_weighted.area_interpolate(\n    pollutants_2021, oa, intensive_variables=[\"aqi\"]\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/pygeos/set_operations.py:129: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: aqi, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n\n\nCPU times: user 17.2 s, sys: 215 ms, total: 17.4 s\nWall time: 17.4 s\n\n\n\noa[\"air_quality\"] = interp.aqi"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#house-price",
    "href": "code/02_models/03_england-wide-data.html#house-price",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.2 House price",
    "text": "H.2 House price\nRead the same dataset as before.\n\n# It is 6.3GB...\nlinked_epc_path = \"https://reshare.ukdataservice.ac.uk/854942/1/tranall2011_19.csv\"\n\nepc = pd.read_csv(linked_epc_path)\nepc[\"dateoftransfer\"] = pd.to_datetime(epc.dateoftransfer)\nlast2years = epc[epc.dateoftransfer &gt; datetime.datetime(2018, 1, 1)]\n\nprice_per_oa = last2years[[\"oa11\", \"priceper\"]].groupby(\"oa11\").mean().reset_index()\nprice_per_oa.to_parquet(f\"{data_folder}/processed/house_prices/price_per_oa.parquet\")\n\n\nprice_per_oa = pd.read_parquet(\n    f\"{data_folder}/processed/house_prices/price_per_oa.parquet\"\n)\n\n\nprice_per_oa\n\n\n\n\n\n\n\n\noa11\npriceper\n\n\n\n\n0\nE00000003\n15722.758094\n\n\n1\nE00000005\n12587.314452\n\n\n2\nE00000007\n12640.262294\n\n\n3\nE00000010\n9535.653363\n\n\n4\nE00000012\n10341.463415\n\n\n...\n...\n...\n\n\n173553\nW00010260\n1632.955752\n\n\n173554\nW00010261\n1834.401969\n\n\n173555\nW00010262\n1511.677538\n\n\n173556\nW00010263\n1679.802956\n\n\n173557\nW00010264\n2046.069031\n\n\n\n\n173558 rows × 2 columns\n\n\n\n\noa\n\n\n\n\n\n\n\n\nOA11CD\nLAD11CD\nGlobalID\ngeometry\nair_quality\n\n\n\n\n0\nE00000001\nE09000001\nf8512cce-6727-42cf-9840-6866cdbb2deb\nPOLYGON ((532303.492 181814.110, 532213.378 18...\n27.501372\n\n\n1\nE00000003\nE09000001\n9eeeb3aa-ce92-4cea-bd70-3a0e680cddc1\nPOLYGON ((532180.131 181763.020, 532155.909 18...\n27.501372\n\n\n2\nE00000005\nE09000001\n012372dd-5e03-43c3-a915-b697d02f88e2\nPOLYGON ((532124.321 181682.675, 532127.299 18...\n27.501372\n\n\n3\nE00000007\nE09000001\nb61eb464-9c5b-4f0e-8f78-000ea938ee78\nPOLYGON ((532124.321 181682.675, 532201.292 18...\n27.545142\n\n\n4\nE00000010\nE09000001\nefc37450-7064-4f5e-bedd-433bf4de1167\nPOLYGON ((532071.302 182159.586, 532127.958 18...\n27.587650\n\n\n...\n...\n...\n...\n...\n...\n\n\n181403\nW00010261\nW06000011\nc1051d13-4fa6-48d8-8f32-5020938ce7d6\nPOLYGON ((262156.208 196600.223, 262074.703 19...\n13.828992\n\n\n181404\nW00010262\nW06000011\nea1fef0a-138d-4035-a401-48c459abbca0\nPOLYGON ((263241.217 197440.210, 263271.904 19...\n13.670047\n\n\n181405\nW00010263\nW06000011\n38819942-d3db-47e1-a85d-eec4d7a091e4\nPOLYGON ((262156.208 196600.223, 262205.269 19...\n14.125627\n\n\n181406\nW00010264\nW06000011\n61fbb0fb-10bf-4000-a669-7114865776ff\nPOLYGON ((268829.001 198038.000, 268708.179 19...\n14.219191\n\n\n181407\nW00010265\nW06000011\n999facfa-dce5-42bf-82fe-c1f04221661e\nPOLYGON ((266130.758 192630.558, 265987.755 19...\n14.927127\n\n\n\n\n181408 rows × 5 columns\n\n\n\nLink to our OA dataset.\n\noa = oa.merge(price_per_oa, left_on=\"OA11CD\", right_on=\"oa11\", how=\"left\")\noa = oa.drop(columns=\"oa11\").rename(columns={\"priceper\": \"house_price\"})\noa[[\"OA11CD\", \"geometry\", \"air_quality\", \"house_price\"]].to_parquet(\n    f\"{data_folder}/processed/oa_data_england.parquet\"\n)"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#explanatory-variables",
    "href": "code/02_models/03_england-wide-data.html#explanatory-variables",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.3 Explanatory variables",
    "text": "H.3 Explanatory variables\nWe also want to have all our explanatory variables linked to the OA geometries.\n\nH.3.1 Population estimates\nONS population estimates are reported on the OA level and can be merged.\nRead the file processed in the Urban Grammar project.\n\npop = pd.read_parquet(f\"{data_folder}/processed/population.parquet\")\n\nAttribute join.\n\noa = oa.merge(pop, left_on=\"OA11CD\", right_on=\"code\", how=\"left\")\n\n\noa = oa.drop(columns=[\"code\"])\n\n\n\nH.3.2 Workplace population\nWorkplace population is reported on Workplace Zones and needs to be interpolated to OA. We use the preprocessed data from the Urban Grammar project.\n\nwp = gpd.read_parquet(\n    f\"{data_folder}/raw/workplace_population/workplace_by_industry_gb.pq\"\n)\n\n\n%%time\nwp_interpolated = tobler.area_weighted.area_interpolate(\n    wp,\n    oa,\n    extensive_variables=[\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ],\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/pygeos/set_operations.py:129: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nCPU times: user 31.5 s, sys: 351 ms, total: 31.9 s\nWall time: 32 s\n\n\n\noa[\n    [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ]\n] = wp_interpolated.drop(columns=\"geometry\").values\n\n\noa = oa.drop(columns=[\"LAD11CD\", \"GlobalID\"])\n\n\noa.to_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")\n\n\n\nH.3.3 Land cover (CORINE)\nCORINE is shipped as custom polygons. We use the data downloaded for the Urban Grammar project.\n\ncorine = gpd.read_parquet(f\"{data_folder}/raw/land_cover/corine_gb.pq\")\n\n\n%%time\ncorine_interpolated = tobler.area_weighted.area_interpolate(\n    corine, oa, categorical_variables=[\"Code_18\"]\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/pygeos/set_operations.py:129: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nCPU times: user 22min 56s, sys: 5.49 s, total: 23min 1s\nWall time: 23min 4s\n\n\n\ncorine_names = {\n    \"Code_18_124\": \"Land cover [Airports]\",\n    \"Code_18_211\": \"Land cover [Non-irrigated arable land]\",\n    \"Code_18_121\": \"Land cover [Industrial or commercial units]\",\n    \"Code_18_421\": \"Land cover [Salt marshes]\",\n    \"Code_18_522\": \"Land cover [Estuaries]\",\n    \"Code_18_142\": \"Land cover [Sport and leisure facilities]\",\n    \"Code_18_141\": \"Land cover [Green urban areas]\",\n    \"Code_18_112\": \"Land cover [Discontinuous urban fabric]\",\n    \"Code_18_231\": \"Land cover [Pastures]\",\n    \"Code_18_311\": \"Land cover [Broad-leaved forest]\",\n    \"Code_18_131\": \"Land cover [Mineral extraction sites]\",\n    \"Code_18_123\": \"Land cover [Port areas]\",\n    \"Code_18_122\": \"Land cover [Road and rail networks and associated land]\",\n    \"Code_18_512\": \"Land cover [Water bodies]\",\n    \"Code_18_243\": \"Land cover [Land principally occupied by agriculture, with significant areas of natural vegetation]\",\n    \"Code_18_313\": \"Land cover [Mixed forest]\",\n    \"Code_18_412\": \"Land cover [Peat bogs]\",\n    \"Code_18_321\": \"Land cover [Natural grasslands]\",\n    \"Code_18_322\": \"Land cover [Moors and heathland]\",\n    \"Code_18_324\": \"Land cover [Transitional woodland-shrub]\",\n    \"Code_18_111\": \"Land cover [Continuous urban fabric]\",\n    \"Code_18_423\": \"Land cover [Intertidal flats]\",\n    \"Code_18_523\": \"Land cover [Sea and ocean]\",\n    \"Code_18_312\": \"Land cover [Coniferous forest]\",\n    \"Code_18_133\": \"Land cover [Construction sites]\",\n    \"Code_18_333\": \"Land cover [Sparsely vegetated areas]\",\n    \"Code_18_332\": \"Land cover [Bare rocks]\",\n    \"Code_18_411\": \"Land cover [Inland marshes]\",\n    \"Code_18_132\": \"Land cover [Dump sites]\",\n    \"Code_18_222\": \"Land cover [Fruit trees and berry plantations]\",\n    \"Code_18_242\": \"Land cover [Complex cultivation patterns]\",\n    \"Code_18_331\": \"Land cover [Beaches, dunes, sands]\",\n    \"Code_18_511\": \"Land cover [Water courses]\",\n    \"Code_18_334\": \"Land cover [Burnt areas]\",\n    \"Code_18_244\": \"Land cover [Agro-forestry areas]\",\n    \"Code_18_521\": \"Land cover [Coastal lagoons]\",\n}\n\ncorine_interpolated.columns = corine_interpolated.columns.map(corine_names)\n\nAssign the subset of classes that may be interesting for our purposes to OA dataframe.\n\ninteresting = [\n    \"Land cover [Discontinuous urban fabric]\",\n    \"Land cover [Continuous urban fabric]\",\n    \"Land cover [Non-irrigated arable land]\",\n    \"Land cover [Industrial or commercial units]\",\n    \"Land cover [Green urban areas]\",\n    \"Land cover [Pastures]\",\n    \"Land cover [Sport and leisure facilities]\",\n]\noa[interesting] = corine_interpolated[interesting].values\n\n\noa.to_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#morphometrics",
    "href": "code/02_models/03_england-wide-data.html#morphometrics",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.4 Morphometrics",
    "text": "H.4 Morphometrics\nGetting the file pre-processed based on the Urban Grammar\n\noa_morpho = pd.read_parquet(f\"{data_folder}/processed/morphometrics_oa.parquet\")\noa = oa.merge(oa_morpho, left_on=\"OA11CD\", right_index=True)\n\n\noa.to_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#cleanup",
    "href": "code/02_models/03_england-wide-data.html#cleanup",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.5 Cleanup",
    "text": "H.5 Cleanup\nDrop unusable rows\n\noa = gpd.read_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")\n\n\nto_drop = [\n    \"sdbPer\",\n    \"ssbElo\",\n    \"stcOri\",\n    \"sdcLAL\",\n    \"mdcAre\",\n    \"ltcAre\",\n    \"ltcWRE\",\n    \"mtdMDi\",\n    \"lcdMes\",\n    \"lddNDe\",\n    \"sddAre\",\n    \"mdsAre\",\n    \"ldsAre\",\n    \"lisCel\",\n    \"ldePer\",\n    \"lseCWA\",\n]\n\noa = oa.drop(columns=to_drop)\n\n\noa_clean = oa.dropna()\n\nRemove London data as London is a massive outlier that throws everything off.\n\nlondon = gpd.read_file(f\"{data_folder}/raw/london/OA_2011_London_gen_MHW.shp\")\n\n\noa_clean = oa_clean[~oa_clean.OA11CD.isin(london.OA11CD)]\n\n\noa_clean.set_index(\"OA11CD\").to_parquet(\n    f\"{data_folder}/processed/oa_data_england.parquet\"\n)"
  },
  {
    "objectID": "code/02_models/04a_air_quality_model_exploration.html#using-spatial-lag",
    "href": "code/02_models/04a_air_quality_model_exploration.html#using-spatial-lag",
    "title": "Appendix I — Air Quality prediction model exploration",
    "section": "I.1 Using spatial lag",
    "text": "I.1 Using spatial lag\nWe can add a lag into the mix. Let’s start with the Queen weights.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nsdsAre_lag\nsisBpM_lag\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n30280.039796\n0.064310\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n17776.067815\n0.054231\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n54958.015580\n0.058214\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n78050.340373\n0.062603\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n27462.223773\n0.067705\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n\n\n\n\n5 rows × 118 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars, data.air_quality_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 11min 1s, sys: 12min 4s, total: 23min 5s\nWall time: 14min 16s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.5538384955550043\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data.index)\nresiduals_lag = data.air_quality_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_lag)\n\n0.9191784959862161\n\n\nMSE got down from 1.34 to 0.92, so there’s a clear gain in including the lag.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nThe over and underprediction tendency still remains but the error is lower.\nThe main underpredicted area is south of the city along the highway - I belive that is due to relatively open OAs and no information on the highway in the explanatory variables.\nThe main overprediction happens in the green belt - there’s very few of these low values so the K-fold modelling exercise likely does not have enough low-value observations to learn it.\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nOverlay of original and a lagged model. We can see that the lagged one is more precise also here.\n\nax = residuals_lag.plot.hist(bins=50)\nresiduals.plot.hist(bins=50, ax=ax, color=\"r\", alpha=0.7)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThe average residuals:\n\nresiduals_lag.abs().mean(), residuals.abs().mean()\n\n(0.7127002931466176, 0.8861835077899286)\n\n\nInterestingly enough, both options have the same parameters of the best model.\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\n\nest.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\n\nI.1.1 Wider weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen3 = libpysal.weights.higher_order(queen, k=3, lower_order=True)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nqueen3.transform = \"R\"\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen3, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nsdsAre_lag\nsisBpM_lag\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n30280.039796\n0.064310\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n17776.067815\n0.054231\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n54958.015580\n0.058214\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n78050.340373\n0.062603\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n27462.223773\n0.067705\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n\n\n\n\n5 rows × 118 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars, data.air_quality_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 10min 48s, sys: 11min 32s, total: 22min 21s\nWall time: 13min 59s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.76202775278368\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data.index)\nresiduals_lag = data.air_quality_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_lag)\n\n0.5447792769804048\n\n\n\nresiduals_lag.abs().mean()\n\n0.51491983590584\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_lag,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "code/02_models/04a_air_quality_model_exploration.html#with-latent-representation-of-sentinel-2",
    "href": "code/02_models/04a_air_quality_model_exploration.html#with-latent-representation-of-sentinel-2",
    "title": "Appendix I — Air Quality prediction model exploration",
    "section": "I.2 With latent representation of Sentinel 2",
    "text": "I.2 With latent representation of Sentinel 2\nTry including the lagged latent representation from the postcode Sentinel paper.\n\nlatent_oa = pd.read_parquet(f\"{data_folder}/processed/sentinel/latent_oa.parquet\")\n\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nexvars_latent = pd.concat([exvars, latent_oa.drop(columns=\"geometry\")], axis=1)\n\n\nfor col in exvars_latent.columns.copy():\n    exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n        queen, exvars_latent[col]\n    )\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n\n\n\nexvars_latent = exvars_latent.copy()\n\n\nparameters = {\n    \"learning_rate\": (0.01, 0.05, 0.1),\n    \"max_iter\": [500],\n    \"max_bins\": (64, 128),\n}\n\nInitiate GridSearchCV with Histogram-based Gradient Boosting Regression Tree.\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_latent = GridSearchCV(regressor, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_latent.fit(exvars_latent, data.air_quality_index)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\nCPU times: user 3min 14s, sys: 2min 35s, total: 5min 49s\nWall time: 3min 26s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_latent.best_estimator_\n\nHistGradientBoostingRegressor(max_bins=128, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(max_bins=128, max_iter=500, random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_latent.best_score_\n\n0.6205192288572696\n\n\n\ny_pred_latent = cross_val_predict(\n    est_latent.best_estimator_, exvars_latent, data.air_quality_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_latent = pd.Series(y_pred_latent, index=data.index)\nresiduals_latent = data.air_quality_index - pred_latent\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_latent)\n\n0.8272652037619179\n\n\nMSE got down from 1.34 to 0.92, so there’s a clear gain in including the lag.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_latent, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_latent).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_latent,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_latent).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_latent.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\n\nI.2.1 Only Sentinel2\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_s2 = GridSearchCV(regressor, parameters, verbose=1)\n\n\n%%time\nest_s2.fit(latent_oa.drop(columns=\"geometry\"), data.air_quality_index)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\nCPU times: user 1min 32s, sys: 2min 19s, total: 3min 52s\nWall time: 2min 36s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_s2.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_s2.best_score_\n\n0.23497429291310237\n\n\n\ny_pred_s2 = cross_val_predict(\n    est_s2.best_estimator_,\n    latent_oa.drop(columns=\"geometry\"),\n    data.air_quality_index,\n    cv=5,\n)\n\nGet the prediction and residuals.\n\npred_s2 = pd.Series(y_pred_s2, index=data.index)\nresiduals_s2 = data.air_quality_index - pred_s2\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_s2)\n\n1.570113449347073\n\n\nMSE got down from 1.34 to 0.92, so there’s a clear gain in including the lag.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_s2, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_s2).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_s2,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_s2).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")"
  },
  {
    "objectID": "code/02_models/04b_air_quality_model_search.html",
    "href": "code/02_models/04b_air_quality_model_search.html",
    "title": "Appendix J — Air Quality prediction model search",
    "section": "",
    "text": "Loop and grid search for the optimal air quality model.\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\n\nimport contextily\nimport palettable.matplotlib as palmpl\nimport matplotlib.pyplot as plt\nimport mapclassify\nimport libpysal\n\nfrom utils import legendgram\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_14176/881383173.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas as gpd\n\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"geo_code\",\n        \"geometry\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n        \"greenspace_accessibility_index\",\n    ]\n)\n\nSpecify grid search parameters. We can limit the options based on previous exploration.\n\nparameters = {\"learning_rate\": (0.05, 0.1), \"max_iter\": [500], \"max_bins\": (64, 128)}\n\nDefine the simple weights matrices.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nweights = {\n    \"queen\": queen,\n    \"queen2\": libpysal.weights.higher_order(queen, k=2, lower_order=True),\n    \"queen3\": libpysal.weights.higher_order(queen, k=3, lower_order=True),\n    \"queen4\": libpysal.weights.higher_order(queen, k=4, lower_order=True),\n    \"queen5\": libpysal.weights.higher_order(queen, k=5, lower_order=True),\n    \"500m\": libpysal.weights.DistanceBand.from_dataframe(data, 500),\n    \"1000m\": libpysal.weights.DistanceBand.from_dataframe(data, 1000),\n    \"2000m\": libpysal.weights.DistanceBand.from_dataframe(data, 2000),\n}\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 110 disconnected components.\n There are 82 islands with ids: 47, 71, 72, 89, 263, 361, 364, 375, 376, 377, 378, 541, 642, 983, 993, 1092, 1220, 1295, 1339, 1343, 1345, 1383, 1406, 1640, 1756, 1772, 1809, 1851, 1944, 1958, 2124, 2148, 2181, 2182, 2188, 2195, 2214, 2222, 2223, 2237, 2265, 2277, 2281, 2283, 2307, 2361, 2485, 2493, 2594, 2686, 2766, 2809, 2825, 2868, 2940, 2980, 3091, 3094, 3112, 3146, 3191, 3197, 3207, 3223, 3235, 3276, 3397, 3400, 3415, 3419, 3423, 3427, 3451, 3475, 3488, 3528, 3555, 3577, 3707, 3723, 3743, 3778.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 16 disconnected components.\n There are 12 islands with ids: 89, 377, 378, 1944, 2182, 2277, 2493, 2594, 2868, 3146, 3223, 3528.\n  warnings.warn(message)\n\n\nUse Grid Search CV to find the best model for each weights option.\n\nmeta = {}\nfor name, W in weights.items():\n    W.transform = \"r\"\n    exvars = data.drop(\n        columns=[\n            \"geo_code\",\n            \"geometry\",\n            \"air_quality_index\",\n            \"house_price_index\",\n            \"jobs_accessibility_index\",\n            \"greenspace_accessibility_index\",\n        ]\n    )\n    for col in exvars.columns.copy():\n        exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(W, exvars[col])\n    regressor_lag = HistGradientBoostingRegressor(\n        random_state=0,\n    )\n    est_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n    est_lag.fit(exvars, data.air_quality_index)\n    meta[name] = {\"score\": est_lag.best_score_}\n    y_pred_lag = cross_val_predict(\n        est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n    )\n    pred_lag = pd.Series(y_pred_lag, index=data.index)\n    residuals_lag = data.air_quality_index - pred_lag\n    meta[name][\"mse\"] = mean_squared_error(data.air_quality_index, pred_lag)\n    meta[name][\"me\"] = residuals_lag.abs().mean()\n    meta[name][\"prediction\"] = pred_lag\n    meta[name][\"residuals\"] = residuals_lag\n    meta[name][\"model\"] = est_lag.best_estimator_\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 47, ' is an island (no neighbors)')\n('WARNING: ', 71, ' is an island (no neighbors)')\n('WARNING: ', 72, ' is an island (no neighbors)')\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 263, ' is an island (no neighbors)')\n('WARNING: ', 361, ' is an island (no neighbors)')\n('WARNING: ', 364, ' is an island (no neighbors)')\n('WARNING: ', 375, ' is an island (no neighbors)')\n('WARNING: ', 376, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 541, ' is an island (no neighbors)')\n('WARNING: ', 642, ' is an island (no neighbors)')\n('WARNING: ', 983, ' is an island (no neighbors)')\n('WARNING: ', 993, ' is an island (no neighbors)')\n('WARNING: ', 1092, ' is an island (no neighbors)')\n('WARNING: ', 1220, ' is an island (no neighbors)')\n('WARNING: ', 1295, ' is an island (no neighbors)')\n('WARNING: ', 1339, ' is an island (no neighbors)')\n('WARNING: ', 1343, ' is an island (no neighbors)')\n('WARNING: ', 1345, ' is an island (no neighbors)')\n('WARNING: ', 1383, ' is an island (no neighbors)')\n('WARNING: ', 1406, ' is an island (no neighbors)')\n('WARNING: ', 1640, ' is an island (no neighbors)')\n('WARNING: ', 1756, ' is an island (no neighbors)')\n('WARNING: ', 1772, ' is an island (no neighbors)')\n('WARNING: ', 1809, ' is an island (no neighbors)')\n('WARNING: ', 1851, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 1958, ' is an island (no neighbors)')\n('WARNING: ', 2124, ' is an island (no neighbors)')\n('WARNING: ', 2148, ' is an island (no neighbors)')\n('WARNING: ', 2181, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2188, ' is an island (no neighbors)')\n('WARNING: ', 2195, ' is an island (no neighbors)')\n('WARNING: ', 2214, ' is an island (no neighbors)')\n('WARNING: ', 2222, ' is an island (no neighbors)')\n('WARNING: ', 2223, ' is an island (no neighbors)')\n('WARNING: ', 2237, ' is an island (no neighbors)')\n('WARNING: ', 2265, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2281, ' is an island (no neighbors)')\n('WARNING: ', 2283, ' is an island (no neighbors)')\n('WARNING: ', 2307, ' is an island (no neighbors)')\n('WARNING: ', 2361, ' is an island (no neighbors)')\n('WARNING: ', 2485, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2686, ' is an island (no neighbors)')\n('WARNING: ', 2766, ' is an island (no neighbors)')\n('WARNING: ', 2809, ' is an island (no neighbors)')\n('WARNING: ', 2825, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 2940, ' is an island (no neighbors)')\n('WARNING: ', 2980, ' is an island (no neighbors)')\n('WARNING: ', 3091, ' is an island (no neighbors)')\n('WARNING: ', 3094, ' is an island (no neighbors)')\n('WARNING: ', 3112, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3191, ' is an island (no neighbors)')\n('WARNING: ', 3197, ' is an island (no neighbors)')\n('WARNING: ', 3207, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3235, ' is an island (no neighbors)')\n('WARNING: ', 3276, ' is an island (no neighbors)')\n('WARNING: ', 3397, ' is an island (no neighbors)')\n('WARNING: ', 3400, ' is an island (no neighbors)')\n('WARNING: ', 3415, ' is an island (no neighbors)')\n('WARNING: ', 3419, ' is an island (no neighbors)')\n('WARNING: ', 3423, ' is an island (no neighbors)')\n('WARNING: ', 3427, ' is an island (no neighbors)')\n('WARNING: ', 3451, ' is an island (no neighbors)')\n('WARNING: ', 3475, ' is an island (no neighbors)')\n('WARNING: ', 3488, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\n('WARNING: ', 3555, ' is an island (no neighbors)')\n('WARNING: ', 3577, ' is an island (no neighbors)')\n('WARNING: ', 3707, ' is an island (no neighbors)')\n('WARNING: ', 3723, ' is an island (no neighbors)')\n('WARNING: ', 3743, ' is an island (no neighbors)')\n('WARNING: ', 3778, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nAdd combined weights on top.\n\ncombined_weigts = {\n    \"queen500m\": libpysal.weights.w_union(weights[\"queen\"], weights[\"500m\"]),\n    \"queen1000m\": libpysal.weights.w_union(weights[\"queen\"], weights[\"1000m\"]),\n    \"queen2000m\": libpysal.weights.w_union(weights[\"queen\"], weights[\"2000m\"]),\n}\n\nFind models.\n\nfor name, W in combined_weigts.items():\n    W.transform = \"r\"\n    exvars = data.drop(\n        columns=[\n            \"geo_code\",\n            \"geometry\",\n            \"air_quality_index\",\n            \"house_price_index\",\n            \"jobs_accessibility_index\",\n            \"greenspace_accessibility_index\",\n        ]\n    )\n    for col in exvars.columns.copy():\n        exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(W, exvars[col])\n    regressor_lag = HistGradientBoostingRegressor(\n        random_state=0,\n    )\n    est_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n    est_lag.fit(exvars, data.air_quality_index)\n    meta[name] = {\"score\": est_lag.best_score_}\n    y_pred_lag = cross_val_predict(\n        est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n    )\n    pred_lag = pd.Series(y_pred_lag, index=data.index)\n    residuals_lag = data.air_quality_index - pred_lag\n    meta[name][\"mse\"] = mean_squared_error(data.air_quality_index, pred_lag)\n    meta[name][\"me\"] = residuals_lag.abs().mean()\n    meta[name][\"prediction\"] = pred_lag\n    meta[name][\"residuals\"] = residuals_lag\n    meta[name][\"model\"] = est_lag.best_estimator_\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nSave evaluation metrics as series.\n\nmse = pd.Series([vals[\"mse\"] for vals in meta.values()], index=meta.keys())\nme = pd.Series([vals[\"me\"] for vals in meta.values()], index=meta.keys())\nscore = pd.Series([vals[\"score\"] for vals in meta.values()], index=meta.keys())\n\nSort according to MSE. Lower is better.\n\nmse.sort_values()\n\nqueen2000m    0.505627\nqueen4        0.510048\n2000m         0.520661\nqueen5        0.523149\nqueen3        0.544779\n1000m         0.671224\nqueen1000m    0.677756\nqueen2        0.714267\nqueen500m     0.826434\nqueen         0.919178\n500m          0.947078\ndtype: float64\n\n\nSort according to ME. Lower is better.\n\nme.sort_values()\n\nqueen2000m    0.445121\n2000m         0.453738\nqueen4        0.481805\nqueen5        0.486240\nqueen3        0.514920\n1000m         0.551970\nqueen1000m    0.558655\nqueen2        0.601619\nqueen500m     0.658718\n500m          0.708802\nqueen         0.712700\ndtype: float64\n\n\nSort according to R2. Higher is better.\n\nscore.sort_values()\n\nqueen         0.553838\n500m          0.563087\nqueen500m     0.615798\nqueen2        0.690342\nqueen1000m    0.721429\n1000m         0.724661\nqueen3        0.762028\nqueen5        0.780816\nqueen4        0.782347\n2000m         0.803538\nqueen2000m    0.807554\ndtype: float64\n\n\nThe optimal model seems to use a combination of Queen weights and Distance Band 2000m. Let’s explore it.\nThe actual vs predicted values.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, meta[\"queen2000m\"][\"prediction\"], s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot the values using the original cmap.\n\nfrom shapely.geometry import box\n\nbds = data.total_bounds\nextent = gpd.GeoSeries(\n    [box((bds[0] - 7000), bds[1], bds[2] + 7000, bds[3])], crs=data.crs\n).to_crs(3857)\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=meta[\"queen2000m\"][\"prediction\"]).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    meta[\"queen2000m\"][\"prediction\"],\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=meta[\"queen2000m\"][\"residuals\"]).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nIt seems that large OAs are less precise. Green belt tends to be overpredicted whuile some more central areas underpredicted. However, the error across the area is minimal.\nWe can save the meta dict with all the data. The final model is part of that.\n\nimport pickle\n\n\nwith open(f\"{data_folder}/models/air_quality_meta.pickle\", \"wb\") as f:\n    pickle.dump(meta, f)\n\nSave just the model for easy inference.\n\nwith open(f\"{data_folder}/models/air_quality_model.pickle\", \"wb\") as f:\n    pickle.dump(meta[\"queen2000m\"][\"model\"], f)"
  },
  {
    "objectID": "code/02_models/04c_air_quality_model_final.html",
    "href": "code/02_models/04c_air_quality_model_final.html",
    "title": "Appendix K — Air quality model training",
    "section": "",
    "text": "Training of the final air quality model based on England-wide training data. We use all the urbanities and complete Tyne and Wear data.\n\nimport geopandas as gpd\nimport numpy as np\nimport pickle\nimport libpysal\nimport pandas as pd\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_2815/1880529722.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas as gpd\n\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet( f\"{data_folder}/processed/oa_data_england.parquet\").reset_index()\n\n\ndata\n\n\n\n\n\n\n\n\nOA11CD\ngeometry\nair_quality\nhouse_price\npopulation\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\n...\nsdsAre\nsisBpM\nmisCel\nltcRea\nldeAre\nlseCCo\nlseERI\nlteOri\nlteWNB\nlieWCe\n\n\n\n\n0\nE00024141\nPOLYGON ((371069.995 412699.817, 371071.042 41...\n15.934102\n1922.197510\n332\n0.990638\n6.726873\n11.046278\n6.136546\n2.492399\n...\n13975.666597\n0.056219\n19.954050\n35.246575\n1.256267e+06\n0.319418\n0.816373\n20.132149\n0.015252\n0.000419\n\n\n1\nE00024142\nPOLYGON ((372065.268 412451.708, 372209.845 41...\n15.448224\n1906.095472\n248\n0.001346\n3.223105\n5.636509\n3.545102\n1.128334\n...\n15410.342389\n0.100076\n28.093151\n44.830189\n3.925836e+05\n0.449345\n0.710619\n2.959019\n0.014953\n0.001140\n\n\n2\nE00024143\nPOLYGON ((371660.000 411501.000, 371713.600 41...\n16.867974\n1637.266845\n344\n0.012007\n16.303660\n5.735694\n25.856348\n10.764267\n...\n29083.154721\n0.091624\n32.059264\n41.927536\n2.570456e+05\n0.376142\n0.710968\n0.725296\n0.015635\n0.000501\n\n\n3\nE00024144\nPOLYGON ((372060.628 413005.554, 372135.500 41...\n15.428515\n2309.758910\n267\n0.835283\n5.659664\n9.330154\n5.139762\n2.095734\n...\n16525.316453\n0.136274\n41.319858\n36.449275\n5.520947e+05\n0.343661\n0.540198\n2.378640\n0.009774\n0.000410\n\n\n4\nE00024145\nPOLYGON ((371044.000 412456.000, 371054.856 41...\n15.934103\n1892.330673\n342\n1.115215\n10.030827\n14.307288\n10.770798\n3.715121\n...\n20519.501061\n0.090342\n34.058382\n35.644444\n3.432917e+04\n0.365621\n0.832452\n13.366394\n0.018709\n0.000634\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n150957\nW00010260\nPOLYGON ((267910.000 198012.000, 267999.940 19...\n14.179153\n1632.955752\n388\n110.687501\n300.948318\n27.233474\n287.566966\n110.478280\n...\n46212.801285\n0.090101\n25.266093\n46.838235\n2.605957e+05\n0.390833\n0.828420\n22.939585\n0.007081\n0.000552\n\n\n150958\nW00010261\nPOLYGON ((262156.208 196600.223, 262074.703 19...\n13.828992\n1834.401969\n350\n0.159798\n1.538887\n3.336814\n4.093612\n2.150986\n...\n26908.056789\n0.086915\n38.950354\n37.342857\n1.152950e+05\n0.481360\n0.734035\n38.749182\n0.010361\n0.000873\n\n\n150959\nW00010262\nPOLYGON ((263241.217 197440.210, 263271.904 19...\n13.670047\n1511.677538\n255\n0.461842\n4.608673\n10.359996\n12.534188\n6.675875\n...\n132838.218299\n0.047589\n25.111880\n48.400000\n5.917093e+05\n0.282517\n0.679996\n15.017141\n0.004100\n0.001458\n\n\n150960\nW00010263\nPOLYGON ((262156.208 196600.223, 262205.269 19...\n14.125627\n1679.802956\n266\n0.143086\n1.394383\n3.084563\n3.854233\n1.985713\n...\n32281.163838\n0.081168\n34.711174\n44.390625\n1.653941e+05\n0.432894\n0.656936\n42.673309\n0.005929\n0.000321\n\n\n150961\nW00010264\nPOLYGON ((268829.001 198038.000, 268708.179 19...\n14.219191\n2046.069031\n280\n0.699826\n46.711718\n2.613740\n5.020753\n6.687857\n...\n29597.647716\n0.067987\n18.902523\n46.270833\n1.619669e+05\n0.494464\n0.919064\n39.726963\n0.009803\n0.000314\n\n\n\n\n150962 rows × 63 columns\n\n\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"OA11CD\",\n        \"geometry\",\n        \"air_quality\",\n        \"house_price\",\n    ]\n)\n\nCreate weights of the order 5 identified as optimal.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\ndist2000 = libpysal.weights.DistanceBand.from_dataframe(data, 2000)\nW = libpysal.weights.w_union(queen,dist2000)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 51 disconnected components.\n There are 19 islands with ids: 1676, 2132, 3036, 21306, 33133, 34428, 42635, 42654, 68863, 72800, 74393, 105153, 108399, 134057, 140649, 141143, 141475, 144653, 149708.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 5289 disconnected components.\n There are 3592 islands with ids: 2922, 2953, 3591, 12424, 12425, 12427, 12742, 12744, 13181, 13182, 13278, 13409, 13960, 14168, 14924, 15712, 15729, 15731, 15733, 28156, 28170, 28272, 28288, 28309, 28480, 28638, 28648, 29700, 31495, 31496, 33251, 33357, 34067, 34099, 34140, 34428, 34748, 35053, 35055, 35056, 35057, 35217, 35218, 36381, 36383, 37762, 37767, 37769, 37786, 37804, 37809, 38033, 38074, 38075, 38076, 38077, 38086, 38087, 38088, 38089, 38091, 38094, 38099, 38104, 38115, 38116, 38117, 38118, 38119, 38121, 38236, 38237, 38251, 38252, 38260, 38261, 38277, 38279, 38283, 38294, 38295, 38296, 38318, 38320, 38322, 38323, 38324, 38335, 38339, 38340, 38392, 38393, 38422, 38469, 38547, 38557, 38558, 38561, 38564, 38572, 38586, 38587, 38588, 38758, 38759, 38760, 38761, 38762, 38763, 38766, 38767, 38769, 38790, 38791, 38792, 38793, 38797, 38799, 39246, 39248, 39249, 39358, 39373, 39374, 39381, 39397, 39402, 39497, 39499, 39508, 39509, 39527, 39591, 39593, 39664, 39671, 39739, 39769, 41874, 41875, 41876, 41885, 41889, 41890, 41910, 41912, 41914, 41916, 41917, 41918, 41956, 42873, 42881, 42882, 42883, 42884, 42885, 42886, 42902, 42903, 42904, 42905, 42906, 42919, 42920, 42921, 42922, 42923, 42945, 42946, 42950, 42951, 42953, 42954, 42957, 42958, 42959, 42970, 42971, 42972, 42973, 42974, 42975, 42979, 42980, 42981, 42985, 42986, 42987, 42988, 42997, 43006, 43007, 43008, 43009, 43010, 43015, 43020, 43055, 43129, 43130, 43131, 43132, 43135, 43136, 43137, 43138, 43139, 43140, 43141, 43145, 43146, 43151, 43154, 43158, 43159, 43160, 43161, 43162, 43163, 43166, 43170, 43171, 43172, 43182, 43183, 43185, 43189, 43190, 43191, 43375, 43376, 43379, 43382, 43385, 43395, 43396, 43397, 43402, 43444, 43453, 43538, 43539, 43557, 43564, 43858, 43860, 44734, 44749, 44784, 44932, 46469, 47014, 47056, 47111, 47112, 47223, 47224, 47229, 47563, 50113, 50360, 50670, 50807, 50808, 50861, 52981, 53797, 53833, 53836, 53845, 53919, 53927, 53928, 53929, 53932, 53933, 53934, 53935, 53936, 53988, 54002, 54003, 54004, 54010, 54019, 54106, 55304, 56271, 56273, 56281, 56282, 58658, 58662, 58663, 58683, 58688, 58689, 58959, 59162, 59301, 59304, 59416, 59425, 59426, 59657, 59813, 59814, 59815, 59816, 59821, 59822, 59823, 59826, 59828, 59829, 59830, 59831, 59841, 59917, 60296, 60301, 60444, 60448, 60452, 60453, 60455, 60456, 60502, 60504, 60532, 60537, 60572, 60573, 60623, 60629, 60630, 60634, 60640, 60641, 60704, 60705, 60706, 60713, 62048, 62052, 62054, 62059, 62063, 62066, 62067, 62069, 62080, 62081, 62117, 62133, 62159, 62163, 62181, 62204, 62223, 62246, 62247, 62252, 62256, 62286, 62335, 62357, 62393, 62409, 62434, 62437, 62502, 62511, 62570, 62571, 62572, 62573, 62577, 62617, 62629, 62700, 62712, 62718, 62719, 62876, 62877, 62878, 62895, 62896, 62900, 62914, 62943, 62950, 62958, 62997, 63004, 63007, 63023, 63050, 63057, 63058, 63059, 63061, 63099, 63125, 63137, 63139, 63265, 63272, 63318, 63364, 63374, 63544, 63650, 63660, 63726, 63728, 63730, 63741, 63744, 63745, 63958, 64107, 64109, 64111, 64112, 64113, 64139, 64141, 64180, 64209, 64285, 64288, 64290, 64291, 64292, 64296, 64444, 64448, 64449, 64828, 64834, 64869, 64870, 64871, 64872, 65134, 65156, 65225, 65231, 65232, 65234, 65238, 65415, 65421, 65498, 65506, 65663, 65671, 65672, 65677, 65678, 65679, 65680, 65705, 65706, 65710, 65711, 65767, 65769, 65771, 65780, 65801, 65809, 65810, 65813, 65950, 65955, 66101, 66108, 66130, 66131, 66132, 66138, 66268, 66398, 66402, 66406, 66489, 66490, 66500, 66508, 66509, 66510, 66515, 66516, 66517, 66518, 66519, 66520, 66521, 66522, 66525, 66590, 66610, 66611, 66612, 66613, 66614, 66615, 66620, 66643, 66644, 66648, 66650, 66651, 66671, 66675, 66704, 66705, 66706, 66717, 66721, 66727, 66729, 66731, 66732, 66733, 66734, 66735, 66736, 66737, 66745, 66770, 66771, 66772, 66773, 66857, 66860, 66861, 66866, 66883, 66957, 66961, 66962, 67063, 67139, 67218, 67241, 67253, 67254, 67258, 67273, 67341, 67352, 67354, 67355, 67356, 67357, 67358, 67386, 67387, 67388, 67389, 67390, 67391, 67392, 67393, 67394, 67395, 67401, 67402, 67403, 67454, 67455, 67456, 67474, 67476, 67478, 67479, 67573, 67584, 67585, 67587, 67595, 67597, 67600, 67601, 67602, 67603, 67604, 67605, 67606, 67607, 67608, 67609, 67611, 67612, 67613, 67614, 67845, 67941, 67985, 67993, 67995, 67997, 68022, 68023, 68024, 68025, 68051, 68056, 68057, 68058, 68059, 68060, 68061, 68062, 68070, 68071, 68072, 68073, 68074, 68075, 68076, 68077, 68078, 68125, 68126, 68127, 68129, 68187, 68213, 68222, 68224, 68301, 68302, 68303, 68314, 68376, 68377, 68379, 68425, 68426, 68427, 68434, 68435, 68436, 68440, 68451, 68452, 68453, 68454, 68458, 68459, 68460, 68461, 68462, 68463, 68464, 68468, 68474, 68475, 68476, 68479, 68480, 68481, 68482, 68483, 68484, 68485, 68487, 68488, 68489, 68490, 68497, 68498, 68499, 68500, 68503, 68504, 68506, 68507, 68508, 68509, 68510, 68511, 68512, 68513, 68518, 68520, 68521, 68524, 68525, 68526, 68527, 68528, 68529, 68532, 68559, 68586, 68587, 68588, 68589, 68594, 68595, 68596, 68597, 68598, 68599, 68600, 68601, 68602, 68603, 68605, 68606, 68607, 68608, 68609, 68627, 68628, 68630, 68631, 68639, 68653, 68657, 68662, 68667, 68670, 68671, 68672, 68694, 68695, 68696, 68697, 68698, 68699, 68797, 68798, 68812, 68813, 68819, 68842, 68843, 68844, 68845, 68848, 68856, 68863, 68864, 68865, 68870, 68871, 68873, 68876, 68877, 68878, 68879, 68880, 68926, 68927, 68928, 68929, 68930, 68931, 68932, 68938, 68996, 69324, 69325, 69963, 69964, 69972, 69975, 69976, 69981, 69987, 69988, 70012, 70014, 70015, 70023, 70024, 70025, 70026, 70050, 70051, 70106, 70107, 70108, 70109, 70110, 70123, 70637, 70645, 70649, 70669, 70725, 70771, 70805, 70814, 70819, 71291, 71294, 71308, 71423, 71426, 71428, 71429, 71461, 71475, 71476, 71477, 71478, 71479, 71487, 71492, 71605, 71652, 71658, 71659, 71667, 71668, 71669, 71670, 71671, 71672, 71702, 71703, 71705, 71784, 71785, 71786, 71787, 71788, 71821, 71824, 71825, 71826, 71827, 72203, 72204, 72205, 72212, 72213, 72214, 72215, 72222, 72231, 72238, 72239, 72240, 72241, 72242, 72272, 72278, 72292, 72295, 72334, 72345, 72346, 72351, 72358, 72359, 72360, 72361, 72362, 72363, 72369, 72377, 72379, 72385, 72386, 72389, 72390, 72409, 72410, 72411, 72413, 72415, 72433, 72434, 72436, 72438, 72439, 72440, 72444, 72445, 72446, 72447, 72490, 72491, 72493, 72495, 72496, 72497, 72498, 72502, 72504, 72511, 72516, 72517, 72597, 72600, 72609, 72613, 72629, 72631, 72635, 72636, 72637, 72638, 72639, 72654, 72655, 72656, 72686, 72687, 72688, 72692, 72707, 72708, 72709, 72711, 72714, 72715, 72716, 72721, 72734, 72735, 72736, 72740, 72741, 72744, 72746, 72785, 72791, 72792, 72793, 72794, 72795, 72797, 72801, 72802, 72811, 72892, 72893, 72895, 72914, 72969, 72970, 72971, 72972, 72973, 73009, 73063, 73064, 73136, 73206, 73207, 73218, 73237, 73241, 73268, 73305, 73311, 73321, 73328, 73381, 73387, 73390, 73415, 73450, 73451, 73452, 73453, 73454, 73457, 73458, 73459, 73465, 73466, 73467, 73468, 73469, 73470, 73471, 73472, 73475, 73476, 73486, 73487, 73496, 73497, 73498, 73499, 73501, 73502, 73503, 73505, 73508, 73533, 73534, 73535, 73537, 73538, 73539, 73541, 73542, 73546, 73569, 73570, 73571, 73572, 73573, 73574, 73582, 73583, 73584, 73599, 73600, 73601, 73602, 73603, 73616, 73619, 73620, 73626, 73627, 73628, 73629, 73630, 73631, 73632, 73633, 73634, 73635, 73636, 73642, 73643, 73644, 73645, 73650, 73651, 73652, 73656, 73657, 73658, 73659, 73660, 73661, 73676, 73687, 73689, 73690, 73691, 73735, 73744, 73745, 73872, 73985, 74020, 74021, 74022, 74023, 74028, 74029, 74030, 74031, 74032, 74087, 74189, 74198, 74199, 74200, 74201, 74210, 74250, 74251, 74252, 74253, 74257, 74258, 74269, 74277, 74286, 74290, 74291, 74367, 74369, 74387, 74395, 74400, 74401, 74405, 74419, 74451, 74509, 74510, 74513, 74515, 74516, 74544, 74548, 74600, 74603, 74610, 74627, 74629, 74635, 74639, 74640, 74666, 74723, 74727, 74729, 74730, 74733, 74734, 74739, 74753, 74755, 74759, 74760, 74761, 74762, 74763, 74779, 74786, 74787, 74790, 74791, 74792, 74793, 74794, 74799, 74805, 74807, 74841, 74842, 75357, 75452, 76253, 76254, 76259, 76393, 76394, 76400, 76401, 76402, 76403, 76404, 76414, 76425, 76427, 76428, 76429, 76430, 76437, 76438, 76443, 76444, 76445, 76446, 76447, 76448, 76456, 76457, 76563, 76564, 76565, 76566, 76569, 76570, 76572, 77283, 77311, 77316, 77415, 77416, 77480, 77487, 77591, 77638, 77649, 77650, 77658, 77679, 77680, 77681, 77828, 77878, 77887, 77889, 77900, 77982, 77985, 77986, 77992, 78026, 78029, 78035, 78095, 78104, 78105, 78135, 78182, 79017, 79125, 79126, 79127, 79134, 79140, 79158, 79172, 79173, 79174, 79177, 79208, 79214, 79222, 79377, 79442, 79852, 79884, 79885, 79888, 80328, 80339, 80413, 80562, 80931, 81066, 81083, 81450, 81462, 81479, 81572, 81596, 81601, 81650, 81951, 82176, 82317, 82384, 82385, 82451, 82533, 82541, 82550, 82568, 82569, 82572, 82576, 82577, 82579, 82587, 82967, 82970, 82975, 82978, 82980, 82989, 83018, 83032, 83034, 83036, 83037, 83038, 83040, 83041, 83042, 83043, 83044, 83105, 83106, 83107, 83108, 83109, 83110, 83111, 83125, 83130, 83131, 83132, 83133, 83134, 83135, 83137, 83138, 83170, 83173, 83177, 83178, 83179, 83180, 83181, 83185, 83187, 83211, 83212, 83214, 83215, 83216, 83217, 83218, 83274, 83275, 83278, 83348, 83349, 83448, 83449, 83450, 83474, 83476, 83490, 83860, 83990, 84005, 84006, 84007, 84008, 84080, 84164, 84167, 84317, 84318, 84331, 84342, 84343, 84356, 84359, 84360, 84362, 84382, 84431, 84432, 84552, 84553, 84573, 84657, 84689, 84729, 84872, 84874, 84875, 84876, 84877, 84895, 84974, 85015, 85016, 85019, 85020, 85026, 85027, 85028, 85047, 85049, 85050, 85085, 85087, 85194, 85212, 85218, 86260, 86393, 86896, 86907, 86908, 86912, 86913, 86928, 86946, 86948, 87000, 87005, 87072, 87073, 87076, 87300, 87689, 87741, 87744, 87796, 87800, 87801, 87802, 87807, 87808, 87810, 87896, 88022, 88025, 88026, 88027, 88028, 88082, 88084, 88091, 88136, 88138, 88275, 88294, 88300, 88318, 88319, 89009, 89147, 89152, 89155, 89171, 89236, 89314, 89318, 89368, 89377, 89740, 89790, 89793, 89794, 89796, 89920, 89922, 90107, 90108, 90404, 91645, 91668, 91669, 91671, 91672, 91673, 91706, 91756, 91781, 91836, 91881, 91931, 91975, 91978, 91979, 92134, 92135, 92678, 92693, 92707, 93459, 93472, 93474, 93534, 93568, 93571, 93572, 93574, 93697, 93714, 93715, 93720, 93882, 94333, 94334, 94337, 94339, 94340, 94423, 94480, 94704, 95571, 95581, 95591, 95594, 95855, 95897, 96409, 96603, 96651, 96662, 97026, 97031, 97035, 97147, 97156, 97164, 97165, 97167, 97169, 97329, 97333, 97335, 97337, 97524, 97896, 97897, 98074, 98075, 98076, 98081, 98082, 98083, 98105, 98126, 98127, 98128, 98205, 98207, 98823, 98824, 99160, 99161, 99241, 99364, 99366, 99478, 100037, 100223, 100224, 100225, 100226, 100231, 100234, 100292, 100309, 100312, 100316, 100415, 100417, 100423, 100424, 100429, 100432, 100451, 100457, 100574, 100780, 100781, 100811, 100813, 100817, 100818, 100819, 100821, 100823, 100824, 100919, 100920, 100925, 100926, 100927, 100928, 100936, 100937, 100938, 100939, 100947, 101183, 101393, 101410, 101436, 101451, 101472, 101526, 101527, 101528, 101581, 101587, 101590, 101600, 101613, 101618, 101634, 101636, 101638, 101641, 101642, 101644, 101645, 101654, 101656, 101696, 101697, 101701, 101702, 101703, 101704, 101712, 101713, 101718, 101720, 101721, 101722, 101724, 101725, 101726, 101727, 101728, 101754, 101762, 101777, 101795, 101796, 101797, 101798, 101799, 101800, 101802, 101892, 101897, 101898, 101906, 101907, 101908, 101927, 101928, 101929, 101930, 101931, 101932, 101934, 101935, 101936, 101970, 101972, 101975, 101976, 101977, 101978, 101979, 101997, 101998, 101999, 102001, 102002, 102003, 102016, 102017, 102018, 102297, 102302, 102303, 102306, 102356, 102360, 102368, 102381, 102383, 102388, 102395, 102404, 102428, 102429, 102433, 102434, 102441, 102442, 102443, 102444, 102447, 102450, 102505, 102506, 102507, 102508, 102509, 102511, 102512, 102607, 102608, 102609, 102620, 102630, 102637, 102647, 102658, 102700, 102706, 102709, 102802, 102809, 102818, 102823, 102841, 102872, 102875, 102876, 102879, 102904, 102985, 102995, 102997, 102998, 102999, 103001, 103066, 103071, 103077, 103078, 103079, 103080, 103081, 103082, 103086, 103087, 103092, 103093, 103135, 103207, 103252, 103259, 103260, 103261, 103262, 103266, 103269, 103279, 103280, 103300, 103301, 103303, 103306, 103307, 103308, 103362, 103365, 103367, 103370, 103371, 103372, 103376, 103377, 103378, 103386, 103398, 103401, 103406, 103407, 103408, 103421, 103440, 103441, 103448, 103450, 103451, 103459, 103490, 103495, 103497, 103498, 103499, 103502, 103514, 103515, 103518, 103519, 103532, 103533, 103534, 103535, 103555, 103617, 103618, 103629, 103635, 103645, 103650, 103655, 103657, 103661, 103662, 103663, 103664, 103665, 103673, 103709, 103723, 103748, 103762, 103766, 103819, 103848, 103850, 103851, 103852, 103857, 103858, 103859, 103860, 103890, 103891, 103892, 103893, 103895, 103896, 103909, 103950, 103954, 103972, 104045, 104046, 104047, 104051, 104052, 104054, 104055, 104471, 104472, 104609, 104613, 104638, 104651, 104660, 104668, 104671, 104707, 104756, 104801, 104826, 104829, 104854, 104888, 104903, 104904, 104905, 104906, 104907, 104908, 104909, 104912, 104942, 104943, 104996, 105008, 105009, 105010, 105014, 105019, 105020, 105022, 105026, 105038, 105069, 105071, 105072, 105079, 105083, 105086, 105087, 105089, 105093, 105099, 105105, 105106, 105107, 105109, 105115, 105117, 105141, 105142, 105143, 105155, 105156, 105165, 105287, 105288, 105289, 105290, 105381, 105385, 105387, 105398, 105406, 105427, 105428, 105429, 105435, 105440, 105874, 105883, 105884, 105885, 105901, 105907, 105929, 105930, 105940, 106030, 106053, 106152, 106224, 106237, 106445, 106491, 106492, 106498, 106499, 106500, 106503, 106504, 106536, 106566, 106586, 106593, 106600, 106601, 106611, 106612, 106622, 106634, 106636, 106637, 106641, 106642, 106643, 106644, 106646, 106650, 106652, 106701, 106706, 106709, 106711, 106717, 106723, 106740, 106862, 106941, 106943, 107089, 107140, 107141, 107143, 107144, 107145, 107780, 107782, 107783, 107784, 107835, 107957, 107962, 107992, 107999, 108002, 108004, 108005, 108006, 108007, 108250, 108307, 108308, 108309, 108310, 108311, 108312, 108313, 108314, 108315, 108316, 108321, 108326, 108327, 108328, 108338, 108339, 108347, 108349, 108357, 108358, 108359, 108360, 108361, 108364, 108367, 108368, 108373, 108374, 108375, 108385, 108392, 108393, 108394, 108395, 108397, 108399, 108400, 108406, 108409, 108410, 108411, 108412, 108415, 108455, 108737, 108738, 108739, 108740, 108741, 108752, 108756, 108811, 108835, 108855, 108856, 108859, 108860, 108862, 108863, 108881, 108882, 108883, 108893, 108895, 108906, 108907, 108912, 108914, 108915, 108916, 108935, 108977, 109016, 109017, 109019, 109020, 109021, 109025, 109026, 109040, 109041, 109042, 109043, 109044, 109045, 109046, 109048, 109051, 109054, 109055, 109056, 109057, 109058, 109277, 109278, 109279, 109280, 109282, 109283, 109284, 109311, 109312, 109313, 109319, 109320, 109321, 109322, 109340, 109341, 109349, 109350, 109351, 109352, 109355, 109360, 109361, 109366, 109367, 109368, 109369, 109370, 109380, 109444, 109445, 109446, 109447, 109448, 109451, 109455, 109481, 109483, 109486, 109487, 109488, 109489, 109492, 109493, 109494, 109520, 109527, 109532, 109535, 109536, 109540, 109544, 109545, 109555, 109558, 109560, 109601, 109602, 109603, 109607, 109623, 109629, 109630, 109632, 109638, 109639, 109651, 109652, 109674, 109675, 109676, 109679, 109680, 109681, 109682, 109683, 109704, 109707, 109710, 109722, 109726, 109727, 109728, 109729, 109730, 109731, 109733, 109774, 109775, 109782, 109869, 109870, 109874, 109875, 109877, 109879, 109968, 109970, 109971, 109974, 109982, 109987, 109989, 110013, 110014, 110018, 110019, 110024, 110045, 110051, 110054, 110055, 110058, 110066, 110155, 110156, 110205, 110207, 110208, 110210, 110213, 110214, 110215, 110216, 110220, 110223, 110226, 110249, 110250, 110251, 110252, 110258, 110259, 110260, 110261, 110262, 110289, 110290, 110296, 110299, 110301, 110304, 110315, 110316, 110317, 110318, 110319, 110320, 110322, 110325, 110329, 110333, 110335, 110336, 110337, 110338, 110340, 110341, 110342, 110343, 110344, 110345, 110346, 110347, 110381, 110382, 110383, 110384, 110388, 110389, 110394, 110395, 110396, 110397, 110400, 110403, 110404, 110405, 110406, 110407, 110408, 110409, 110410, 110411, 110412, 110413, 110414, 110415, 110418, 110419, 110421, 110427, 110429, 110430, 110431, 110432, 110433, 110434, 110510, 110515, 110517, 110522, 110523, 110524, 110528, 110529, 110531, 110537, 110538, 110539, 110540, 110541, 110542, 110544, 110545, 110546, 110547, 110548, 110553, 110557, 110558, 110559, 110560, 110561, 110562, 110611, 110613, 110616, 110617, 110618, 110647, 110648, 110657, 110658, 110661, 110662, 110711, 110725, 110758, 110829, 110830, 110843, 110929, 110933, 110934, 111005, 111062, 111063, 111075, 111078, 111158, 111172, 111178, 111179, 111587, 111589, 111590, 111675, 111712, 111741, 111746, 111748, 111752, 111763, 112484, 112996, 113070, 113071, 113074, 113096, 113100, 113101, 113134, 113149, 113161, 113194, 113201, 113262, 113274, 113275, 113515, 113567, 113569, 113570, 113622, 113625, 113639, 113883, 113889, 113892, 113909, 113914, 113923, 113972, 113986, 113987, 113991, 113994, 114007, 114019, 114020, 114417, 114459, 114504, 114577, 114769, 114772, 114774, 114928, 114934, 115056, 115057, 115061, 115167, 115168, 115169, 115171, 115174, 115332, 115336, 115337, 115338, 115352, 115364, 115481, 115545, 115546, 115547, 115548, 115549, 115557, 115558, 115559, 115560, 115561, 115565, 115567, 115568, 115580, 115581, 115582, 115592, 115614, 115615, 115620, 115621, 115625, 115626, 115643, 115644, 115645, 115650, 115666, 115669, 115675, 115676, 115677, 115678, 115679, 115681, 115683, 115684, 115685, 115722, 115729, 115730, 115731, 115732, 115741, 115752, 115753, 115755, 115778, 115779, 115803, 115808, 115870, 115871, 115874, 115875, 115896, 115907, 115913, 116048, 116050, 116097, 116099, 116102, 116103, 116104, 116106, 116107, 116108, 116109, 116112, 116113, 116150, 116153, 116187, 116193, 116194, 116195, 116199, 116200, 116201, 116203, 116208, 116209, 116210, 116258, 116260, 116261, 116269, 116270, 116271, 116272, 116273, 116275, 116276, 116277, 116280, 116281, 116287, 116288, 116291, 116292, 116293, 116309, 116313, 116316, 116325, 116329, 116330, 116331, 116332, 116333, 116334, 116335, 116336, 116337, 116338, 116339, 116384, 116385, 116386, 116389, 116390, 116391, 116392, 116395, 116397, 116398, 116399, 116416, 116417, 116418, 116576, 116577, 116579, 116589, 116599, 116600, 116603, 116604, 116608, 116609, 116610, 116613, 116617, 116618, 116619, 116701, 116703, 116704, 116749, 116926, 116936, 116937, 116947, 116950, 116951, 116977, 116978, 117067, 117077, 117092, 117098, 117099, 117105, 117106, 117107, 117108, 117117, 117149, 117166, 117181, 117235, 117313, 117329, 117386, 117429, 117431, 117437, 117438, 117463, 117480, 117483, 117634, 117635, 117636, 117671, 117673, 117674, 117679, 117689, 117690, 117692, 117698, 117939, 117940, 117941, 117964, 117967, 117968, 117969, 117970, 117975, 117980, 117985, 117987, 117991, 117992, 117993, 118020, 118021, 118022, 118024, 118029, 118035, 118036, 118045, 118046, 118047, 118048, 118063, 118364, 118365, 118367, 118395, 118396, 118397, 118456, 118538, 118657, 118688, 118689, 118873, 118875, 118906, 118913, 119165, 119173, 119175, 119178, 119181, 119539, 119580, 119607, 119652, 119660, 119742, 119747, 119748, 119780, 119781, 119783, 119785, 119794, 119795, 119843, 119845, 119849, 119850, 119962, 119965, 119966, 120009, 120010, 120011, 120018, 120034, 120042, 120193, 120257, 120284, 120285, 120286, 120287, 120288, 120309, 120310, 120317, 120318, 120392, 120393, 120394, 120395, 120396, 120397, 120398, 120667, 120668, 120669, 120674, 120675, 120676, 120698, 120699, 120777, 120802, 120810, 120815, 120837, 120861, 120863, 120864, 120950, 120966, 120968, 120998, 121018, 121079, 121467, 121471, 121473, 121506, 121543, 121544, 121545, 121551, 121565, 121568, 121569, 121573, 121575, 121582, 121610, 121697, 121707, 121718, 121726, 121734, 121736, 121737, 121762, 121763, 121766, 121775, 121784, 121791, 121792, 121798, 121800, 121910, 121973, 121975, 122048, 122049, 122050, 122055, 122056, 122077, 122078, 122079, 122080, 122082, 122187, 122195, 122196, 122201, 122205, 122299, 122312, 122316, 122317, 122321, 122322, 122326, 122327, 122328, 122329, 122330, 122334, 122335, 122378, 122394, 122395, 122397, 122398, 122422, 122423, 122444, 122445, 122486, 122490, 122494, 122651, 122771, 122772, 122773, 122775, 122776, 122779, 122783, 122784, 122829, 122831, 123966, 123967, 123972, 125425, 125572, 125584, 125601, 125645, 125646, 125647, 126324, 126340, 126348, 126823, 126903, 126960, 126961, 126995, 126996, 126998, 127073, 127076, 127098, 127106, 127138, 127139, 127142, 127160, 127161, 127163, 127168, 127171, 127201, 127224, 127229, 127264, 127265, 127410, 127418, 127425, 127426, 127533, 127535, 127608, 127696, 127735, 128138, 128139, 128579, 128582, 128586, 128587, 128726, 128732, 128733, 128739, 128740, 128772, 128788, 128794, 128795, 128878, 128879, 128880, 128920, 129251, 129270, 129297, 129307, 129341, 129342, 129346, 129472, 129760, 129985, 130006, 130388, 130391, 130399, 130403, 130426, 130427, 130430, 130469, 130474, 130512, 130514, 130515, 130516, 130517, 130518, 130524, 130525, 130539, 130542, 130543, 130544, 130552, 130553, 130566, 130569, 130570, 130573, 130588, 130602, 130606, 130614, 130617, 130635, 130649, 130700, 130844, 130847, 130848, 130865, 130867, 130868, 130894, 130895, 130899, 130900, 130921, 130931, 131075, 131078, 131081, 131088, 131101, 131126, 131129, 131151, 131178, 131179, 131181, 131183, 131184, 131185, 131197, 131198, 131201, 131202, 131203, 131271, 131272, 131276, 131278, 131279, 131280, 131281, 131282, 131283, 131284, 131289, 131297, 131299, 131300, 131301, 131304, 131309, 131315, 131316, 131348, 131416, 131431, 131507, 131508, 131513, 131515, 131516, 131536, 131537, 131541, 131552, 131957, 132029, 132035, 132036, 132086, 132087, 132088, 132092, 132093, 132116, 132117, 132119, 132152, 132184, 132185, 132188, 132192, 132193, 132194, 132251, 132953, 133049, 133065, 133066, 133144, 133155, 133170, 133171, 133251, 133443, 133632, 133918, 134086, 134218, 134219, 134464, 134932, 135404, 135409, 135420, 135425, 135441, 135449, 135751, 135753, 135754, 135820, 135850, 135955, 136140, 136141, 136170, 136184, 136212, 136535, 136564, 136565, 136567, 136723, 136730, 136735, 136736, 136749, 137405, 137456, 137457, 137613, 137899, 138280, 138321, 138326, 138380, 138607, 138608, 138714, 138934, 138987, 139137, 139140, 139189, 139391, 139554, 139601, 139905, 139906, 139941, 140032, 140076, 140946, 140947, 141195, 141196, 141216, 141217, 141219, 141221, 141288, 141289, 141296, 141297, 141298, 141299, 141301, 141302, 141303, 141317, 141323, 141328, 141332, 141333, 141347, 141348, 141354, 141385, 141415, 141416, 141417, 141422, 141434, 141440, 141461, 141470, 141471, 141472, 141473, 141474, 141475, 141476, 141477, 141485, 141486, 141487, 141490, 141491, 141508, 141520, 141521, 141522, 141540, 141541, 141558, 141568, 141569, 141580, 141583, 141586, 141587, 141588, 141589, 141594, 141600, 141601, 141602, 141603, 141607, 141611, 141612, 141618, 141625, 141634, 141635, 141639, 141685, 141718, 141719, 141720, 141748, 141759, 141763, 141771, 141772, 141773, 141787, 141810, 141811, 141812, 141813, 141816, 141824, 141825, 141826, 141827, 141899, 141902, 141927, 142014, 142015, 142016, 142017, 142026, 142027, 142028, 142029, 142030, 142149, 142150, 142151, 142169, 142170, 142171, 142172, 142173, 142174, 142175, 142176, 142177, 142189, 142193, 142194, 142232, 142233, 142234, 142236, 142237, 142238, 142239, 142240, 142241, 142242, 142243, 142251, 142252, 142258, 142261, 142264, 142278, 142279, 142280, 142284, 142613, 142614, 142616, 142958, 142989, 142990, 142991, 142992, 143040, 143042, 143047, 143166, 143167, 143217, 143259, 143260, 143262, 143284, 143290, 143378, 143379, 143380, 143381, 143382, 143383, 143384, 143386, 143387, 143389, 143390, 143391, 143393, 143394, 143395, 143399, 143408, 143409, 143413, 143415, 143416, 143418, 143420, 143421, 143436, 143438, 143439, 143441, 143442, 143443, 143445, 143447, 143449, 143450, 143452, 143453, 143454, 143455, 143456, 143457, 143458, 143459, 143461, 143462, 143463, 143469, 143470, 143471, 143472, 143473, 143481, 143483, 143484, 143495, 143499, 143500, 143501, 143502, 143505, 143506, 143507, 143508, 143509, 143510, 143511, 143512, 143513, 143530, 143531, 143532, 143533, 143535, 143541, 143542, 143543, 143546, 143551, 143552, 143553, 143554, 143556, 143557, 143559, 143560, 143561, 143562, 143563, 143564, 143565, 143567, 143579, 143580, 143581, 143582, 143583, 143584, 143585, 143586, 143587, 143588, 143595, 143596, 143598, 143600, 143601, 143603, 143604, 143613, 143614, 143615, 143617, 143619, 143620, 143621, 143622, 143625, 143626, 143627, 143628, 143629, 143663, 143664, 143665, 143666, 143667, 143675, 143677, 143684, 143685, 143686, 143715, 143716, 143717, 143718, 143719, 143724, 143727, 143730, 143731, 143735, 143762, 143763, 143767, 143837, 143838, 143839, 143841, 143850, 143851, 143852, 143853, 143854, 143855, 143860, 143881, 143884, 143894, 143898, 143899, 143900, 143901, 143912, 143915, 143917, 143918, 143920, 143921, 143922, 143923, 143924, 143925, 143926, 143930, 143932, 143933, 143934, 143935, 143936, 143937, 143938, 143940, 143941, 143948, 143949, 143950, 143951, 143953, 143954, 143955, 143956, 143957, 143958, 143959, 143960, 143963, 143964, 143965, 143966, 143972, 143975, 143976, 143979, 143982, 143993, 143999, 144001, 144002, 144005, 144006, 144007, 144008, 144009, 144010, 144011, 144012, 144024, 144025, 144030, 144035, 144038, 144039, 144040, 144043, 144044, 144045, 144049, 144050, 144051, 144052, 144056, 144057, 144064, 144120, 144121, 144122, 144124, 144142, 144143, 144149, 144153, 144154, 144164, 144165, 144166, 144169, 144170, 144171, 144172, 144173, 144185, 144186, 144187, 144328, 144332, 144334, 144335, 144336, 144342, 144344, 144359, 144361, 144362, 144378, 144390, 144392, 144393, 144396, 144399, 144401, 144404, 144405, 144510, 144511, 144513, 144517, 144518, 144519, 144520, 144521, 144522, 144525, 144526, 144527, 144528, 144529, 144530, 144531, 144532, 144533, 144534, 144535, 144536, 144647, 144652, 144657, 144658, 144659, 144660, 144661, 144662, 144663, 144664, 144665, 144671, 144672, 144687, 144688, 144689, 144703, 144706, 144707, 144708, 144709, 144710, 144711, 144714, 144715, 144716, 144717, 144718, 144719, 144721, 144724, 144725, 144727, 144730, 144731, 144732, 144734, 144741, 144762, 144770, 144775, 144795, 144796, 144802, 144805, 144807, 144844, 144845, 144857, 144886, 144887, 144894, 144895, 144905, 144908, 144931, 144932, 144933, 144934, 144935, 144936, 144967, 145173, 145174, 145177, 145178, 145179, 145180, 145181, 145305, 145308, 145900, 145927, 145988, 146001, 146036, 146768, 146809, 146830, 146831, 147381, 147824, 148298, 148900, 148911, 148929, 148932, 148933, 148934, 148937, 148938, 148983, 148984, 149006, 149007, 149008, 149011, 149012, 149013, 149015, 149016, 149019, 149020, 149027, 149050, 149081, 149092, 149129, 149145, 149150, 149152, 149399, 149401, 150521, 150522, 150541, 150542, 150566, 150742.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 14 disconnected components.\n There are 4 islands with ids: 34428, 68863, 108399, 141475.\n  warnings.warn(message)\n\n\nCompute spatial lag.\n\nW.transform = \"r\"\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(W, exvars[col])\n\nCreate a filter to use only urbanities and Tyne and Wear.\n\ntyne_wear = gpd.read_file(f\"{data_folder}/processed/OA_TyneWear.gpkg\")\nsignature_key = pd.read_csv(\"https://figshare.com/ndownloader/files/30904894\")\ndata_w_type = data.merge(signature_key[[\"OA11CD\", \"primary_type\"]], on=\"OA11CD\", how=\"left\")\nmask_urbanity = data_w_type.primary_type.str.contains(\"urbanity\")\nmask_tw = data.OA11CD.isin(tyne_wear.geo_code)\nmask = (mask_urbanity | mask_tw)\n\nInitialise the model.\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0, max_bins=64, max_iter=1000\n)\n\nTrain the model.\n\nregressor.fit(exvars[mask], data.air_quality[mask])\n\nHistGradientBoostingRegressor(max_bins=64, max_iter=1000, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(max_bins=64, max_iter=1000, random_state=0)\n\n\nTest the prediction.\n\nregressor.predict(exvars.iloc[:10])\n\narray([16.08597591, 16.18028682, 16.75840614, 16.83340318, 16.94072672,\n       16.30293912, 16.42741979, 16.588092  , 16.49096282, 16.6718441 ])\n\n\nSave to file.\n\nwith open(f\"{data_folder}/models/air_quality_model_nc_urbanities.pickle\", \"wb\") as f:\n    pickle.dump(regressor, f)"
  },
  {
    "objectID": "code/02_models/05a_house_price_model_exploration.html#using-spatial-lag",
    "href": "code/02_models/05a_house_price_model_exploration.html#using-spatial-lag",
    "title": "Appendix L — House Price prediction model exploration",
    "section": "L.1 Using spatial lag",
    "text": "L.1 Using spatial lag\nWe can add a lag into the mix. Let’s start with the Queen weights.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nsdsAre_lag\nsisBpM_lag\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n30280.039796\n0.064310\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n17776.067815\n0.054231\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n54958.015580\n0.058214\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n78050.340373\n0.062603\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n27462.223773\n0.067705\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n\n\n\n\n5 rows × 118 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars[mask], data[mask].house_price_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 13min 29s, sys: 18min 57s, total: 32min 27s\nWall time: 18min 13s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.3852767425712931\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data[mask].index)\nresiduals_lag = data[mask].house_price_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_lag)\n\n201453.6577863544\n\n\nMSE got down from 248704 to 201453 but that still seems to be a bit high (mean error is 448 from 1500 mean value).\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(\n    data[\"house_price_index\"].fillna(method=\"pad\").values, k=20\n).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nOverlay of original and a lagged model. No clear indication that the lagged one would be vastly superior.\n\nax = residuals_lag.plot.hist(bins=50)\nresiduals.plot.hist(bins=50, ax=ax, color=\"r\", alpha=0.7)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThe average residuals:\n\nresiduals_lag.abs().mean(), residuals.abs().mean()\n\n(345.0198428517582, 381.3997073048114)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)\n\n\n\nest.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)\n\n\n\nL.1.1 Wider weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen3 = libpysal.weights.higher_order(queen, k=3, lower_order=True)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nqueen3.transform = \"R\"\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen3, exvars[col])\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars[mask], data.house_price_index[mask])\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 10min 36s, sys: 11min 30s, total: 22min 6s\nWall time: 13min 58s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.4753837295272674\n\n\n\n%%time\nest_lag.best_estimator_.predict(exvars[mask])\n\nCPU times: user 147 ms, sys: 32.8 ms, total: 180 ms\nWall time: 86 ms\n\n\narray([1731.20448464, 2830.44054758, 1613.82686231, ..., 1590.77525305,\n       2612.76409275, 1496.54246374])\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data[mask].index)\nresiduals_lag = data[mask].house_price_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_lag)\n\n168855.00058162943\n\n\n\nresiduals_lag.abs().mean()\n\n314.2761789390005\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(data[\"house_price_index\"].dropna().values, k=10).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_lag,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "code/02_models/05a_house_price_model_exploration.html#include-accessibilities",
    "href": "code/02_models/05a_house_price_model_exploration.html#include-accessibilities",
    "title": "Appendix L — House Price prediction model exploration",
    "section": "L.2 Include accessibilities",
    "text": "L.2 Include accessibilities\nWe can try to include green space and jobs accessibility, as those are likely to affect the house price. We create a lagged model with these two variables on top of explanatory variables.\n\nexvars = data.drop(\n    columns=[\"geo_code\", \"geometry\", \"air_quality_index\", \"house_price_index\"]\n)\n\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\njobs_accessibility_index_lag\ngreenspace_accessibility_index_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n1582.500000\n2.650345e+05\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n10238.400000\n1.329411e+06\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n1211.000000\n5.170833e+05\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n2275.500000\n3.185030e+05\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n1480.142857\n3.063273e+05\n\n\n\n\n5 rows × 122 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars[mask], data[mask].house_price_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 19min 33s, sys: 33min 44s, total: 53min 17s\nWall time: 26min 23s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.40067474727924707\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data[mask].index)\nresiduals_lag = data[mask].house_price_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_lag)\n\n196944.98165903118\n\n\nMSE is only marginally better than before.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(\n    data[\"house_price_index\"].fillna(method=\"pad\").values, k=20\n).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_lag,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nNot great so far, it seems that our set of explanatory variables is not explanatory enough to predict house prices in this way…"
  },
  {
    "objectID": "code/02_models/05a_house_price_model_exploration.html#with-latent-representation-of-sentinel-2",
    "href": "code/02_models/05a_house_price_model_exploration.html#with-latent-representation-of-sentinel-2",
    "title": "Appendix L — House Price prediction model exploration",
    "section": "L.3 With latent representation of Sentinel 2",
    "text": "L.3 With latent representation of Sentinel 2\nTry including the lagged latent representation from the postcode Sentinel paper.\n\nlatent_oa = pd.read_parquet(f\"{data_folder}/processed/sentinel/latent_oa.parquet\")\n\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nexvars_latent = pd.concat([exvars, latent_oa.drop(columns=\"geometry\")], axis=1)\n\n\nfor col in exvars_latent.columns.copy():\n    exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n        queen, exvars_latent[col]\n    )\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n\n\n\nexvars_latent = exvars_latent.copy()\n\n\nparameters = {\n    \"learning_rate\": (0.01, 0.05, 0.1),\n    \"max_iter\": [500],\n    \"max_bins\": (64, 128),\n}\n\nInitiate GridSearchCV with Histogram-based Gradient Boosting Regression Tree.\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_latent = GridSearchCV(regressor, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_latent.fit(exvars_latent[mask], data[mask].house_price_index)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\nCPU times: user 3min 12s, sys: 2min 41s, total: 5min 53s\nWall time: 3min 30s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_latent.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_latent.best_score_\n\n0.42214079367595386\n\n\n\ny_pred_latent = cross_val_predict(\n    est_latent.best_estimator_, exvars_latent[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_latent = pd.Series(y_pred_latent, index=data[mask].index)\nresiduals_latent = data[mask].house_price_index - pred_latent\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_latent)\n\n190508.98759834783\n\n\nMSE is only marginally better than before.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_latent, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(data[\"house_price_index\"].dropna().values, k=10).bins\n\ndata.assign(pred=pred_latent).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_latent,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_latent).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_latent.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(pred_latent.dropna().values, k=10).bins\n\ndata.assign(pred=pred_latent).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_latent,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(0, pred_latent.max()),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")"
  },
  {
    "objectID": "code/02_models/05b_house_price_model_search.html",
    "href": "code/02_models/05b_house_price_model_search.html",
    "title": "Appendix M — House price prediction model search",
    "section": "",
    "text": "Loop and grid search for the optimal house price model.\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\n\nimport contextily\nimport palettable.matplotlib as palmpl\nimport matplotlib.pyplot as plt\nimport mapclassify\nimport libpysal\n\nfrom utils import legendgram\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"geo_code\",\n        \"geometry\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n        \"greenspace_accessibility_index\",\n    ]\n)\n\nSpecify grid search parameters. We can limit the options based on previous exploration.\n\nparameters = {\"learning_rate\": (0.05, 0.1), \"max_iter\": [500], \"max_bins\": (64, 128)}\n\nDefine the simple weights matrices.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nweights = {\n    \"queen\": queen,\n    \"queen2\": libpysal.weights.higher_order(queen, k=2, lower_order=True),\n    \"queen3\": libpysal.weights.higher_order(queen, k=3, lower_order=True),\n    \"queen4\": libpysal.weights.higher_order(queen, k=4, lower_order=True),\n    \"queen5\": libpysal.weights.higher_order(queen, k=5, lower_order=True),\n    \"500m\": libpysal.weights.DistanceBand.from_dataframe(data, 500),\n    \"1000m\": libpysal.weights.DistanceBand.from_dataframe(data, 1000),\n    \"2000m\": libpysal.weights.DistanceBand.from_dataframe(data, 2000),\n}\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 110 disconnected components.\n There are 82 islands with ids: 47, 71, 72, 89, 263, 361, 364, 375, 376, 377, 378, 541, 642, 983, 993, 1092, 1220, 1295, 1339, 1343, 1345, 1383, 1406, 1640, 1756, 1772, 1809, 1851, 1944, 1958, 2124, 2148, 2181, 2182, 2188, 2195, 2214, 2222, 2223, 2237, 2265, 2277, 2281, 2283, 2307, 2361, 2485, 2493, 2594, 2686, 2766, 2809, 2825, 2868, 2940, 2980, 3091, 3094, 3112, 3146, 3191, 3197, 3207, 3223, 3235, 3276, 3397, 3400, 3415, 3419, 3423, 3427, 3451, 3475, 3488, 3528, 3555, 3577, 3707, 3723, 3743, 3778.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 16 disconnected components.\n There are 12 islands with ids: 89, 377, 378, 1944, 2182, 2277, 2493, 2594, 2868, 3146, 3223, 3528.\n  warnings.warn(message)\n\n\nAdd combined weights on top.\n\nweights[\"queen500m\"] = libpysal.weights.w_union(weights[\"queen\"], weights[\"500m\"])\nweights[\"queen1000m\"] = libpysal.weights.w_union(weights[\"queen\"], weights[\"1000m\"])\nweights[\"queen2000m\"] = libpysal.weights.w_union(weights[\"queen\"], weights[\"2000m\"])\n\nGet a mask to ignore missing values.\n\nmask = data.house_price_index.notna()\n\nUse Grid Search CV to find the best model for each weights option.\n\nmeta = {}\nfor name, W in weights.items():\n    W.transform = \"r\"\n    exvars = data.drop(\n        columns=[\n            \"geo_code\",\n            \"geometry\",\n            \"air_quality_index\",\n            \"house_price_index\",\n            \"jobs_accessibility_index\",\n            \"greenspace_accessibility_index\",\n        ]\n    )\n    for col in exvars.columns.copy():\n        exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(W, exvars[col])\n    regressor_lag = HistGradientBoostingRegressor(\n        random_state=0,\n    )\n    est_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n    est_lag.fit(exvars[mask], data.house_price_index[mask])\n    meta[name] = {\"score\": est_lag.best_score_}\n    y_pred_lag = cross_val_predict(\n        est_lag.best_estimator_, exvars[mask], data.house_price_index[mask], cv=5\n    )\n    pred_lag = pd.Series(y_pred_lag, index=data.index[mask])\n    residuals_lag = data.house_price_index[mask] - pred_lag\n    meta[name][\"mse\"] = mean_squared_error(data.house_price_index[mask], pred_lag)\n    meta[name][\"me\"] = residuals_lag.abs().mean()\n    meta[name][\"prediction\"] = pred_lag\n    meta[name][\"residuals\"] = residuals_lag\n    meta[name][\"model\"] = est_lag.best_estimator_\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 47, ' is an island (no neighbors)')\n('WARNING: ', 71, ' is an island (no neighbors)')\n('WARNING: ', 72, ' is an island (no neighbors)')\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 263, ' is an island (no neighbors)')\n('WARNING: ', 361, ' is an island (no neighbors)')\n('WARNING: ', 364, ' is an island (no neighbors)')\n('WARNING: ', 375, ' is an island (no neighbors)')\n('WARNING: ', 376, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 541, ' is an island (no neighbors)')\n('WARNING: ', 642, ' is an island (no neighbors)')\n('WARNING: ', 983, ' is an island (no neighbors)')\n('WARNING: ', 993, ' is an island (no neighbors)')\n('WARNING: ', 1092, ' is an island (no neighbors)')\n('WARNING: ', 1220, ' is an island (no neighbors)')\n('WARNING: ', 1295, ' is an island (no neighbors)')\n('WARNING: ', 1339, ' is an island (no neighbors)')\n('WARNING: ', 1343, ' is an island (no neighbors)')\n('WARNING: ', 1345, ' is an island (no neighbors)')\n('WARNING: ', 1383, ' is an island (no neighbors)')\n('WARNING: ', 1406, ' is an island (no neighbors)')\n('WARNING: ', 1640, ' is an island (no neighbors)')\n('WARNING: ', 1756, ' is an island (no neighbors)')\n('WARNING: ', 1772, ' is an island (no neighbors)')\n('WARNING: ', 1809, ' is an island (no neighbors)')\n('WARNING: ', 1851, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 1958, ' is an island (no neighbors)')\n('WARNING: ', 2124, ' is an island (no neighbors)')\n('WARNING: ', 2148, ' is an island (no neighbors)')\n('WARNING: ', 2181, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2188, ' is an island (no neighbors)')\n('WARNING: ', 2195, ' is an island (no neighbors)')\n('WARNING: ', 2214, ' is an island (no neighbors)')\n('WARNING: ', 2222, ' is an island (no neighbors)')\n('WARNING: ', 2223, ' is an island (no neighbors)')\n('WARNING: ', 2237, ' is an island (no neighbors)')\n('WARNING: ', 2265, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2281, ' is an island (no neighbors)')\n('WARNING: ', 2283, ' is an island (no neighbors)')\n('WARNING: ', 2307, ' is an island (no neighbors)')\n('WARNING: ', 2361, ' is an island (no neighbors)')\n('WARNING: ', 2485, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2686, ' is an island (no neighbors)')\n('WARNING: ', 2766, ' is an island (no neighbors)')\n('WARNING: ', 2809, ' is an island (no neighbors)')\n('WARNING: ', 2825, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 2940, ' is an island (no neighbors)')\n('WARNING: ', 2980, ' is an island (no neighbors)')\n('WARNING: ', 3091, ' is an island (no neighbors)')\n('WARNING: ', 3094, ' is an island (no neighbors)')\n('WARNING: ', 3112, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3191, ' is an island (no neighbors)')\n('WARNING: ', 3197, ' is an island (no neighbors)')\n('WARNING: ', 3207, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3235, ' is an island (no neighbors)')\n('WARNING: ', 3276, ' is an island (no neighbors)')\n('WARNING: ', 3397, ' is an island (no neighbors)')\n('WARNING: ', 3400, ' is an island (no neighbors)')\n('WARNING: ', 3415, ' is an island (no neighbors)')\n('WARNING: ', 3419, ' is an island (no neighbors)')\n('WARNING: ', 3423, ' is an island (no neighbors)')\n('WARNING: ', 3427, ' is an island (no neighbors)')\n('WARNING: ', 3451, ' is an island (no neighbors)')\n('WARNING: ', 3475, ' is an island (no neighbors)')\n('WARNING: ', 3488, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\n('WARNING: ', 3555, ' is an island (no neighbors)')\n('WARNING: ', 3577, ' is an island (no neighbors)')\n('WARNING: ', 3707, ' is an island (no neighbors)')\n('WARNING: ', 3723, ' is an island (no neighbors)')\n('WARNING: ', 3743, ' is an island (no neighbors)')\n('WARNING: ', 3778, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nSave evaluation metrics as series.\n\nmse = pd.Series([vals[\"mse\"] for vals in meta.values()], index=meta.keys())\nme = pd.Series([vals[\"me\"] for vals in meta.values()], index=meta.keys())\nscore = pd.Series([vals[\"score\"] for vals in meta.values()], index=meta.keys())\n\nSort according to MSE. Lower is better.\n\nmse.sort_values()\n\nqueen5        160164.675606\n2000m         160245.606116\nqueen2000m    160766.019742\n1000m         164343.481698\nqueen1000m    165435.671928\nqueen4        167802.714113\nqueen3        170864.677687\nqueen2        177496.106149\nqueen500m     185800.082368\n500m          186592.783587\nqueen         201453.657786\ndtype: float64\n\n\nSort according to ME. Lower is better.\n\nme.sort_values()\n\n2000m         304.115638\nqueen2000m    305.246902\nqueen5        306.013012\n1000m         310.392479\nqueen4        311.826728\nqueen1000m    312.101787\nqueen3        317.151294\nqueen2        324.171353\nqueen500m     330.578553\n500m          332.239067\nqueen         345.019843\ndtype: float64\n\n\nSort according to R2. Higher is better.\n\nscore.sort_values()\n\nqueen         0.385277\n500m          0.431761\nqueen500m     0.433833\nqueen2        0.454928\nqueen3        0.468809\nqueen4        0.480325\n1000m         0.486258\nqueen1000m    0.489035\n2000m         0.497828\nqueen2000m    0.498994\nqueen5        0.504176\ndtype: float64\n\n\nThe optimal model seems to use either Queen 5, 2000m or a combination. As the original distribution of prices can be a bit bumpy, let’s stick to Q5. Let’s explore it.\nThe actual vs predicted values.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], meta[\"queen5\"][\"prediction\"], s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot the values using the original cmap.\n\nfrom shapely.geometry import box\n\nbds = data.total_bounds\nextent = gpd.GeoSeries(\n    [box((bds[0] - 7000), bds[1], bds[2] + 7000, bds[3])], crs=data.crs\n).to_crs(3857)\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(data[\"house_price_index\"].dropna().values, k=10).bins\n\ndata.assign(pred=meta[\"queen5\"][\"prediction\"]).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    meta[\"queen5\"][\"prediction\"],\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=meta[\"queen5\"][\"residuals\"]).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nmeta[\"queen5\"][\"residuals\"].plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThere doesn’t seem to be any general pattern in where we over- and where underpredict. The general tendency seems to be captured and a comparison against this base model shall work in the final app and scenario building.\nWe can save the meta dict with all the data. The final model is part of that.\n\nimport pickle\n\n\nwith open(f\"{data_folder}/models/house_price_meta.pickle\", \"wb\") as f:\n    pickle.dump(meta, f)\n\nSave just the model for easy inference.\n\nwith open(f\"{data_folder}/models/house_price_model.pickle\", \"wb\") as f:\n    pickle.dump(meta[\"queen5\"][\"model\"], f)"
  },
  {
    "objectID": "code/02_models/05c_house_price_model_final.html",
    "href": "code/02_models/05c_house_price_model_final.html",
    "title": "Appendix N — House price model training",
    "section": "",
    "text": "Training of the final house price model based on England-wide training data.\n\nimport geopandas as gpd\nimport numpy as np\nimport pickle\nimport libpysal\nimport pandas as pd\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_509/1880529722.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas as gpd\n\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet( f\"{data_folder}/processed/oa_data_england.parquet\")\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"geometry\",\n        \"air_quality\",\n        \"house_price\",\n    ]\n)\n\nCreate weights of the order 5 identified as optimal.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen5 = libpysal.weights.higher_order(queen, k=5, lower_order=True)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 51 disconnected components.\n There are 19 islands with ids: 1676, 2132, 3036, 21306, 33133, 34428, 42635, 42654, 68863, 72800, 74393, 105153, 108399, 134057, 140649, 141143, 141475, 144653, 149708.\n  warnings.warn(message)\n\n\nCompute spatial lag.\n\nqueen5.transform = \"r\"\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen5, exvars[col])\n\n('WARNING: ', 1676, ' is an island (no neighbors)')\n('WARNING: ', 2132, ' is an island (no neighbors)')\n('WARNING: ', 3036, ' is an island (no neighbors)')\n('WARNING: ', 21306, ' is an island (no neighbors)')\n('WARNING: ', 33133, ' is an island (no neighbors)')\n('WARNING: ', 34428, ' is an island (no neighbors)')\n('WARNING: ', 42635, ' is an island (no neighbors)')\n('WARNING: ', 42654, ' is an island (no neighbors)')\n('WARNING: ', 68863, ' is an island (no neighbors)')\n('WARNING: ', 72800, ' is an island (no neighbors)')\n('WARNING: ', 74393, ' is an island (no neighbors)')\n('WARNING: ', 105153, ' is an island (no neighbors)')\n('WARNING: ', 108399, ' is an island (no neighbors)')\n('WARNING: ', 134057, ' is an island (no neighbors)')\n('WARNING: ', 140649, ' is an island (no neighbors)')\n('WARNING: ', 141143, ' is an island (no neighbors)')\n('WARNING: ', 141475, ' is an island (no neighbors)')\n('WARNING: ', 144653, ' is an island (no neighbors)')\n('WARNING: ', 149708, ' is an island (no neighbors)')\n\n\nCreate mask to ignore missing values in training.\n\nmask = data.house_price.notna()\n\nInitialise the model.\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0, max_bins=128, max_iter=1000\n)\n\nTrain the model.\n\nregressor.fit(exvars[mask], np.log(data.house_price[mask]))\n\nHistGradientBoostingRegressor(max_bins=128, max_iter=1000, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(max_bins=128, max_iter=1000, random_state=0)\n\n\nTest the prediction.\n\nregressor.predict(exvars.iloc[:10])\n\narray([7.43681357, 7.52412204, 7.34451913, 7.64584292, 7.45234704,\n       7.7307261 , 7.74621834, 7.5703991 , 7.53491617, 7.4739106 ])\n\n\nSave to file.\n\nwith open(f\"{data_folder}/models/house_price_predictor_england_no_london.pickle\", \"wb\") as f:\n    pickle.dump(regressor, f)"
  },
  {
    "objectID": "code/03_prediction/01_summary.html#non-morphological-data",
    "href": "code/03_prediction/01_summary.html#non-morphological-data",
    "title": "Appendix O — Create summary data for signatures",
    "section": "O.1 Non-morphological data",
    "text": "O.1 Non-morphological data\nKey:\n\nfn_key = {\n    \"Workplace population [Agriculture, energy and water]\": \"A, B, D, E. Agriculture, energy and water\",\n    \"Workplace population [Manufacturing]\": \"C. Manufacturing\",\n    \"Workplace population [Construction]\": \"F. Construction\",\n    \"Workplace population [Distribution, hotels and restaurants]\": \"G, I. Distribution, hotels and restaurants\",\n    \"Workplace population [Transport and communication]\": \"H, J. Transport and communication\",\n    \"Workplace population [Financial, real estate, professional and administrative activities]\": \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n    \"Workplace population [Public administration, education and health]\": \"O,P,Q. Public administration, education and health\",\n    \"Workplace population [Other]\": \"R, S, T, U. Other\",\n}\n\nRead and merge labels (and areas).\n\nfunction = (\n    dd.read_parquet(\n        \"signatures_gb/function\",\n        columns=[\n            \"hindex\",\n            \"Population\",\n            \"Land cover [Non-irrigated arable land]\",\n            \"Land cover [Industrial or commercial units]\",\n            \"Land cover [Sport and leisure facilities]\",\n            \"Land cover [Green urban areas]\",\n            \"Land cover [Discontinuous urban fabric]\",\n            \"Land cover [Pastures]\",\n            \"Land cover [Continuous urban fabric]\",\n        ]\n        + list(fn_key),\n    )\n    .compute()\n    .set_index(\"hindex\")\n)\n\nfunction = function.merge(\n    signature_type[[\"type\"]], how=\"left\", left_index=True, right_index=True\n).merge(form[[\"sdcAre\"]], how=\"left\", left_index=True, right_index=True)\nfunction = function.rename(columns=fn_key)\n\nNormalize subset by area.\n\nsubset = [\n    \"Population\",\n    \"A, B, D, E. Agriculture, energy and water\",\n    \"C. Manufacturing\",\n    \"F. Construction\",\n    \"G, I. Distribution, hotels and restaurants\",\n    \"H, J. Transport and communication\",\n    \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n    \"O,P,Q. Public administration, education and health\",\n    \"R, S, T, U. Other\",\n]\nfunction[subset] = function[subset].divide(function.sdcAre, axis=0)\n\nGroup by type, get quartiles and IQR.\n\ngrouper = function.groupby(\"type\")\nmedian = grouper.quantile(0.5)\nq25 = grouper.quantile(0.25)\nq75 = grouper.quantile(0.75)\niqr = q75 - q25\nmedian = median.rename(index=renamer)\niqr = iqr.rename(index=renamer)\n\nSave.\n\nmedian.drop(columns=\"sdcAre\").to_parquet(\n    f\"{data_folder}/sampling/median_function.parquet\"\n)\niqr.drop(columns=\"sdcAre\").to_parquet(f\"{data_folder}/sampling/iqr_function.parquet\")"
  },
  {
    "objectID": "code/03_prediction/02_sample_exvars.html",
    "href": "code/03_prediction/02_sample_exvars.html",
    "title": "Appendix P — Sample explanatory variables based on empirical data and custom scenario definition",
    "section": "",
    "text": "This notebook contains code to generate explanatory variables for an output area based on 4 sliders:\n\nLevel of urbanity\nUse\nGreenspace\nJob types\n\nThe data are either adapted from the original observed value (if the level of urbanity does not change) or sampled from the signature types from across the GB.\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data informing the distributions.\n\nmedian_form = pd.read_parquet(f\"{data_folder}/sampling/median_form.parquet\")\niqr_form = pd.read_parquet(f\"{data_folder}/sampling/iqr_form.parquet\")\nmedian_function = pd.read_parquet(f\"{data_folder}/sampling/median_function.parquet\")\niqr_function = pd.read_parquet(f\"{data_folder}/sampling/iqr_function.parquet\")\noa = (\n    gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n    .set_index(\"geo_code\")\n    .rename(columns={\"population_estimate\": \"population\"})\n)\noa_key = pd.read_parquet(f\"{data_folder}/sampling/oa_key.parquet\")\n\nGet OA areas for area-weighted variables.\n\noa_area = oa.area\n\nDefine a sampling method.\n\ndef _form(signature_type, variable, random_seed):\n    \"\"\"Get values for form variables\n\n    Values are sampled from a normal distribution around\n    median of a variable per signature type. The spread is\n    defined as 1/5 of interquartile range.\n    \"\"\"\n    rng = np.random.default_rng(random_seed)\n    return rng.normal(\n        median_form.loc[signature_type, variable],\n        iqr_form.loc[signature_type, variable] / 5,\n    )\n\n\ndef _function(signature_type, variable, random_seed):\n    \"\"\"Get values for function variables\n\n    Values are sampled from a normal distribution around\n    median of a variable per signature type. The spread is\n    defined as 1/5 of interquartile range.\n    \"\"\"\n    rng = np.random.default_rng(random_seed)\n    return rng.normal(\n        median_function.loc[signature_type, variable],\n        iqr_function.loc[signature_type, variable] / 5,\n    )\n\n\ndef _populations(defaults, index):\n    \"\"\"Balance residential and workplace population\n\n    Workplace population and residential population are treated 1:1 and\n    are re-allocated based on the index. The proportion of workplace categories\n    is not changed.\n    \"\"\"\n    if not -1 &lt;= index &lt;= 1:\n        raise ValueError(f\"use index must be in a range -1...1. {index} given.\")\n    jobs = [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ]\n    n_jobs = defaults[jobs].sum()\n    if index &lt; 0:\n        difference = index * n_jobs\n    else:\n        difference = index * defaults.population\n    new_n_jobs = n_jobs + difference\n    defaults.population = defaults.population - difference\n    multiplier = new_n_jobs / n_jobs\n    defaults[jobs] = defaults[jobs] * multiplier\n    return defaults\n\n\ndef _greenspace(defaults, index):\n    \"\"\"Allocate greenspace to OA\n\n    Allocate publicly accessible formal greenspace to OA. Defines a portion\n    of OA that is covered by gren urban areas. Realistic values are be fairly\n    low. The value affects populations and other land cover classes.\n    \"\"\"\n    if not 0 &lt;= index &lt;= 1:\n        raise ValueError(f\"greenspace index must be in a range 0...1. {index} given.\")\n    greenspace_orig = defaults[\"Land cover [Green urban areas]\"]\n    newly_allocated_gs = index - greenspace_orig\n    defaults = defaults * (1 - newly_allocated_gs)\n    defaults[\"Land cover [Green urban areas]\"] = index\n    return defaults\n\n\ndef _job_types(defaults, index):\n    \"\"\"Balance job types\n\n    Balance job types between manual and white collar workplace categories.\n    Index represents the proportion of white collar jobs in an area. The\n    total sum of FTEs is not changed.\n\n    The service category is not affected under an assumption that both white\n    and blue collar workers need the same amount of services to provide food etc.\n    \"\"\"\n    if not 0 &lt;= index &lt;= 1:\n        raise ValueError(f\"job_types index must be in a range 0...1. {index} given.\")\n    blue = [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"H, J. Transport and communication\",\n    ]\n    white = [\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n    ]\n    blue_collar = defaults[blue].sum()\n    white_collar = defaults[white].sum()\n    total = blue_collar + white_collar\n    orig_proportion = white_collar / total\n\n    new_blue = total * (1 - index)\n    new_white = total * index\n\n    blue_diff = new_blue / blue_collar\n    white_diff = new_white / white_collar\n\n    defaults[blue] = defaults[blue] * blue_diff\n    defaults[white] = defaults[white] * white_diff\n\n    return defaults\n\n\ndef get_signature_values(\n    oa_code: str,\n    signature_type: str = None,\n    use: float = 0,\n    greenspace: float = None,\n    job_types: float = None,\n    random_seed: int = None,\n):\n    \"\"\"Generate explanatory variables based on a scenario\n\n    Generates values for explanatory variables based on empirical data derived\n    from the Urban Grammar project and a scenario definition based on a\n    Urban Grammar signature type, land use balance, greenspace allocation\n    and a job type balance.\n\n    If the target ``signature_type`` differs from the one already allocated\n    to OA, the data is sampled from the distribution from the whole GB. If\n    they are equal, the existing values measured in place are used. That allows\n    playing with other variables without changing the form.\n\n    Parameters\n    ----------\n    oa_code : string\n        String representing the OA code, e.g. ``\"E00042707\"``.\n\n    signature_type : string\n        String representing signature type. See below the possible options\n        and their relationship to the level of urbanity.\n\n            0: 'Wild countryside',\n            1: 'Countryside agriculture',\n            2: 'Urban buffer',\n            3: 'Warehouse/Park land',\n            4: 'Open sprawl',\n            5: 'Disconnected suburbia',\n            6: 'Accessible suburbia',\n            7: 'Connected residential neighbourhoods',\n            8: 'Dense residential neighbourhoods',\n            9: 'Gridded residential quarters',\n            10: 'Dense urban neighbourhoods',\n            11: 'Local urbanity',\n            12: 'Regional urbanity',\n            13: 'Metropolitan urbanity',\n            14: 'Concentrated urbanity',\n            15: 'Hyper concentrated urbanity',\n\n    use : float, optional\n        Float in a range -1...1 reflecting the land use balance between\n        fully residential (-1) and fully commercial (1). Defautls to 0,\n        a value derived from signatures. For values &lt; 0, we are allocating\n        workplace population to residential population. For values &gt; 0, we\n        are allocating residential population to workplace population.\n        Extremes are allowed but are not realistic, in most cases.\n    greenspace : float, optional\n        Float in a range 0...1 reflecting the amount of greenspace in the\n        area. 0 representes no accessible greenspace, 1 represents whole\n        area covered by a greenspace. This value will proportionally affect\n        the amounts of jobs and population.\n    job_types : float, optional\n        Float in a range 0...1 reflecting the balance of job types in the\n        area between entirely blue collar jobs (0) and entirely white collar\n        jobs (1).\n    random_seed : int, optional\n        Random seed\n\n    Returns\n    -------\n    Series\n    \"\"\"\n    orig_type = oa_key.primary_type[oa_code]\n    if signature_type is not None and orig_type != signature_type:\n        form = pd.Series(\n            [_form(signature_type, var, random_seed) for var in median_form.columns],\n            index=median_form.columns,\n            name=oa_code,\n        ).abs()\n\n        defaults = pd.Series(\n            [\n                _function(signature_type, var, random_seed)\n                for var in median_function.columns\n            ],\n            index=median_function.columns,\n            name=oa_code,\n        ).abs()\n\n        area_weighted = [\n            \"population\",\n            \"A, B, D, E. Agriculture, energy and water\",\n            \"C. Manufacturing\",\n            \"F. Construction\",\n            \"G, I. Distribution, hotels and restaurants\",\n            \"H, J. Transport and communication\",\n            \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n            \"O,P,Q. Public administration, education and health\",\n            \"R, S, T, U. Other\",\n        ]\n        defaults[area_weighted] = defaults[area_weighted] * oa_area[oa_code]\n\n    else:\n        form = oa.loc[oa_code][median_form.columns]\n        defaults = oa.loc[oa_code][median_function.columns]\n\n    # population\n    if use != 0:\n        defaults = _populations(defaults, index=use)\n\n    # greenspace\n    if greenspace:\n        defaults = _greenspace(defaults, greenspace)\n\n    if job_types:\n        defaults = _job_types(defaults, job_types)\n    return pd.concat([form, defaults])\n\nExample:\nSet the OA we are interested in.\n\noa_code = \"E00042271\"\n\nCheck the signature type of the OA.\n\noa_key.primary_type[oa_code]\n\n'Dense urban neighbourhoods'\n\n\nThis is the actual value with no changes.\n\nget_signature_values(\n    oa_code,\n)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                                 391\nA, B, D, E. Agriculture, energy and water                                             0.685492\nC. Manufacturing                                                                      0.097322\nF. Construction                                                                       7.038725\nG, I. Distribution, hotels and restaurants                                           21.731618\nH, J. Transport and communication                                                    22.106873\nK, L, M, N. Financial, real estate, professional and administrative activities       10.598732\nO,P,Q. Public administration, education and health                                   74.328726\nR, S, T, U. Other                                                                     9.858182\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.509093\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.0\nLand cover [Discontinuous urban fabric]                                               0.490907\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\nStay within the same signature type and change only use.\n\nMore residential\n\n\nget_signature_values(oa_code, use=-0.5)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                          464.222835\nA, B, D, E. Agriculture, energy and water                                             0.342746\nC. Manufacturing                                                                      0.048661\nF. Construction                                                                       3.519362\nG, I. Distribution, hotels and restaurants                                           10.865809\nH, J. Transport and communication                                                    11.053436\nK, L, M, N. Financial, real estate, professional and administrative activities        5.299366\nO,P,Q. Public administration, education and health                                   37.164363\nR, S, T, U. Other                                                                     4.929091\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.509093\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.0\nLand cover [Discontinuous urban fabric]                                               0.490907\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\n\nLess residential, more jobs\n\n\nget_signature_values(oa_code, use=0.4)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                               234.6\nA, B, D, E. Agriculture, energy and water                                              1.41758\nC. Manufacturing                                                                      0.201259\nF. Construction                                                                      14.555892\nG, I. Distribution, hotels and restaurants                                           44.940396\nH, J. Transport and communication                                                    45.716413\nK, L, M, N. Financial, real estate, professional and administrative activities        21.91789\nO,P,Q. Public administration, education and health                                  153.709788\nR, S, T, U. Other                                                                    20.386452\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.509093\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.0\nLand cover [Discontinuous urban fabric]                                               0.490907\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\n\nMore residential and more greenspace.\n\nCheck current greenspace first.\n\nget_signature_values(\n    oa_code,\n)[\"Land cover [Green urban areas]\"]\n\n0.0\n\n\nNothing. Allocate 20% of area\n\nget_signature_values(oa_code, use=0.4, greenspace=0.2)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                              187.68\nA, B, D, E. Agriculture, energy and water                                             1.134064\nC. Manufacturing                                                                      0.161007\nF. Construction                                                                      11.644714\nG, I. Distribution, hotels and restaurants                                           35.952317\nH, J. Transport and communication                                                     36.57313\nK, L, M, N. Financial, real estate, professional and administrative activities       17.534312\nO,P,Q. Public administration, education and health                                   122.96783\nR, S, T, U. Other                                                                    16.309162\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.407275\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.2\nLand cover [Discontinuous urban fabric]                                               0.392725\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\nChange job type allocation towards more blue collar jobs.\n\nget_signature_values(\n    oa_code,\n    use=0.4,\n    greenspace=0.2,\n    job_types=0.2,\n)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                              187.68\nA, B, D, E. Agriculture, energy and water                                             3.481745\nC. Manufacturing                                                                      0.494316\nF. Construction                                                                      35.751011\nG, I. Distribution, hotels and restaurants                                           35.952317\nH, J. Transport and communication                                                   112.284974\nK, L, M, N. Financial, real estate, professional and administrative activities         4.74268\nO,P,Q. Public administration, education and health                                   33.260332\nR, S, T, U. Other                                                                    16.309162\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.407275\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.2\nLand cover [Discontinuous urban fabric]                                               0.392725\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\nChange the signature type (a proxy for a level of urbanity).\n\nget_signature_values(\n    oa_code,\n    signature_type=\"Local urbanity\",\n    use=0.4,\n    greenspace=0.2,\n    job_types=0.2,\n)\n\nsdbAre                                                                              422.259877\nsdbCoA                                                                                0.000000\nssbCCo                                                                                0.408337\nssbCor                                                                                3.769432\nssbSqu                                                                                0.403360\nssbERI                                                                                0.982311\nssbCCM                                                                               18.595822\nssbCCD                                                                                0.135386\nstbOri                                                                                8.222212\nsdcAre                                                                             1663.577623\nsscCCo                                                                                0.438507\nsscERI                                                                                0.984578\nsicCAR                                                                                0.229900\nstbCeA                                                                                1.659903\nmtbAli                                                                                2.924640\nmtbNDi                                                                               15.906085\nmtcWNe                                                                                0.032347\nltbIBD                                                                               21.239290\nsdsSPW                                                                               26.420297\nsdsSWD                                                                                3.844587\nsdsSPO                                                                                0.384913\nsdsLen                                                                              114.906193\nsssLin                                                                                0.991167\nldsMSL                                                                             1938.858230\nmtdDeg                                                                                3.000000\nlinP3W                                                                                0.773400\nlinP4W                                                                                0.092012\nlinPDE                                                                                0.144101\nlcnClo                                                                                0.000002\nldsCDL                                                                              210.710475\nxcnSCl                                                                                0.001760\nlinWID                                                                                0.034503\nstbSAl                                                                                3.207152\nsdsAre                                                                            18131.928008\nsisBpM                                                                                0.048301\nmisCel                                                                               15.195866\nltcRea                                                                               49.395492\nldeAre                                                                            19916.510706\nlseCCo                                                                                0.347203\nlseERI                                                                                0.865735\nlteOri                                                                               19.688769\nlteWNB                                                                                0.004753\nlieWCe                                                                                0.000877\npopulation                                                                          432.770089\nA, B, D, E. Agriculture, energy and water                                            10.577324\nC. Manufacturing                                                                     18.857491\nF. Construction                                                                      96.518541\nG, I. Distribution, hotels and restaurants                                           92.453344\nH, J. Transport and communication                                                   128.179567\nK, L, M, N. Financial, real estate, professional and administrative activities       30.728056\nO,P,Q. Public administration, education and health                                   32.805174\nR, S, T, U. Other                                                                    24.494562\nLand cover [Non-irrigated arable land]                                                0.000000\nLand cover [Industrial or commercial units]                                           0.000000\nLand cover [Sport and leisure facilities]                                             0.000000\nLand cover [Green urban areas]                                                        0.200000\nLand cover [Discontinuous urban fabric]                                               0.614254\nLand cover [Pastures]                                                                 0.000000\nLand cover [Continuous urban fabric]                                                  0.042641\nName: E00042271, dtype: float64"
  },
  {
    "objectID": "code/03_prediction/03_inference.html#air-quality",
    "href": "code/03_prediction/03_inference.html#air-quality",
    "title": "Appendix Q — Air quality and house price model inference",
    "section": "Q.1 Air quality",
    "text": "Q.1 Air quality\nLoad the sklearn model\n\nwith open(f\"{data_folder}/models/air_quality_model.pickle\", \"rb\") as f:\n    air_quality = pickle.load(f)\n\nCreate spatial weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\n_2k = libpysal.weights.DistanceBand.from_dataframe(data, 2000)\nW = libpysal.weights.w_union(queen, _2k)\nW.transform = \"r\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\nCreate object.\n\naqm = Model(W, air_quality)\n\nSave the custom predictor class to a pickle.\n\nwith open(f\"{data_folder}/models/air_quality_predictor.pickle\", \"wb\") as f:\n    pickle.dump(aqm, f)\n\n\nQ.1.1 England-wide model\nLoad the sklearn model\n\nwith open(f\"{data_folder}/models/air_quality_model_nc_urbanities.pickle\", \"rb\") as f:\n    air_quality = pickle.load(f)\n\nCreate spatial weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nW = libpysal.weights.higher_order(queen, k=5, lower_order=True, silence_warnings=True)\nW.transform = \"r\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\nCreate object.\n\naqm = Model(W, air_quality)\n\nSave the custom predictor class to a pickle.\n\nwith open(\n    f\"{data_folder}/models/air_quality_predictor_nc_urbanities.pickle\", \"wb\"\n) as f:\n    pickle.dump(aqm, f)"
  },
  {
    "objectID": "code/03_prediction/03_inference.html#house-price",
    "href": "code/03_prediction/03_inference.html#house-price",
    "title": "Appendix Q — Air quality and house price model inference",
    "section": "Q.2 House price",
    "text": "Q.2 House price\nLoad the sklearn model\n\nwith open(f\"{data_folder}/models/house_price_model.pickle\", \"rb\") as f:\n    house_price = pickle.load(f)\n\nCreate spatial weights\n\nq5 = libpysal.weights.higher_order(queen, k=5, lower_order=True)\nq5.transform = \"r\"\n\nCreate a wrapper class computing the lag.\n\nhpm = Model(q5, house_price)\n\nSave the custom predictor class to a pickle.\n\nwith open(f\"{data_folder}/models/house_price_predictor.pickle\", \"wb\") as f:\n    pickle.dump(hpm, f)\n\n\nQ.2.1 England-wide model\nLoad the sklearn model\n\nwith open(\n    f\"{data_folder}/models/house_price_model_england_no_london.pickle\", \"rb\"\n) as f:\n    house_price = pickle.load(f)\n\nCreate a wrapper class computing the lag.\n\nhpm = Model(W, house_price)\n\nSave the custom predictor class to a pickle.\n\nwith open(\n    f\"{data_folder}/models/house_price_predictor_england_no_london.pickle\", \"wb\"\n) as f:\n    pickle.dump(hpm, f)"
  },
  {
    "objectID": "code/03_prediction/03_inference.html#using-the-class-for-prediction",
    "href": "code/03_prediction/03_inference.html#using-the-class-for-prediction",
    "title": "Appendix Q — Air quality and house price model inference",
    "section": "Q.3 Using the class for prediction",
    "text": "Q.3 Using the class for prediction\nTo use the class for prediction, load the pickle and call predict on a data frame with explanatory variables (either default or reflecting a scenario).\n\nwith open(f\"{data_folder}/models/air_quality_predictor.pickle\", \"rb\") as f:\n    aqm2 = pickle.load(f)\n\n\naqm2.predict(exvars)\n\narray([17.19278662, 16.43954378, 17.48423016, ..., 16.7559517 ,\n       12.60627689, 17.31309272])\n\n\nExactly the same would it be for the house price model."
  },
  {
    "objectID": "code/03_prediction/04_acc_class.html",
    "href": "code/03_prediction/04_acc_class.html",
    "title": "Appendix R — Accessibility API",
    "section": "",
    "text": "Prototyping of a wrap of accessibility computation into custom classes with simple API.\n\nimport geopandas as gpd\nimport pandas as pd\nimport xarray as xr\nimport numpy as np\nimport joblib\n\nfrom demoland_engine.indicators import Accessibility\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the travel time matrix.\n\nttm = pd.read_parquet(f\"{data_folder}/processed/accessibility/ttm_complete.parquet\")\n\nConvert the matrix to boolean xarray.DataArray.\n\nttm = ttm.set_index([\"from_id\", \"to_id\"])\nttm.columns = [\"transit\", \"car\", \"bike\", \"walk\"]\nttm.columns.name = \"mode\"\nttm_arr = xr.DataArray.from_series(ttm.stack())\nttm_15 = ttm_arr &lt;= 15\nttm_15.name = \"ttm_15\"\n\nLoad Workplace zone population.\n\nwpz_population = (\n    pd.read_csv(\n        f\"{data_folder}/processed/accessibility/wpz_tynewear_occupation_edited.csv\"\n    )\n    .rename(columns={\"wpz11cd\": \"to_id\"})\n    .set_index(\"to_id\")[\"pop\"]\n)\n\nMerge with the traveltime matrix to create a baseline.\n\nda = xr.DataArray.from_series(wpz_population)\nda.name = \"wpz_population\"\nbaseline = xr.merge([ttm_15, da])\nbaseline[\"wpz_population\"] = baseline[\"wpz_population\"].fillna(0)\n\n\nbaseline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:         (from_id: 3795, to_id: 9254, mode: 4)\nCoordinates:\n  * from_id         (from_id) object 'E00041363' 'E00041364' ... 'E00175605'\n  * to_id           (to_id) object 'E00041363' ... 'idFFE0D1A6-2B10-40AE-8E6E...\n  * mode            (mode) object 'transit' 'car' 'bike' 'walk'\nData variables:\n    ttm_15          (from_id, to_id, mode) bool True True True ... False False\n    wpz_population  (to_id) float64 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0xarray.DatasetDimensions:from_id: 3795to_id: 9254mode: 4Coordinates: (3)from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)to_id(to_id)object'E00041363' ... 'idFFE0D1A6-2B10...array(['E00041363', 'E00041364', 'E00041366', ...,\n       'idFFCECD51-C8D3-4DEF-9258-88B0240A502B',\n       'idFFD6A162-EFB8-4A17-9B2E-8D500B56A33F',\n       'idFFE0D1A6-2B10-40AE-8E6E-0AEB92375117'], dtype=object)mode(mode)object'transit' 'car' 'bike' 'walk'array(['transit', 'car', 'bike', 'walk'], dtype=object)Data variables: (2)ttm_15(from_id, to_id, mode)boolTrue True True ... False Falsearray([[[ True,  True,  True,  True],\n        [ True,  True,  True,  True],\n        [ True,  True,  True,  True],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[ True,  True,  True,  True],\n        [ True,  True,  True,  True],\n        [False,  True,  True, False],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[ True,  True,  True,  True],\n        [False,  True,  True, False],\n        [ True,  True,  True,  True],\n        ...,\n...\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[False,  True, False, False],\n        [False,  True, False, False],\n        [False,  True, False, False],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]]])wpz_population(to_id)float640.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0array([0., 0., 0., ..., 0., 0., 0.])Indexes: (3)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))to_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'idFF5D69F1-E02B-46EB-B28A-436049D483B2',\n       'idFF6C80EE-CC17-45BD-89D1-E08AA8187F98',\n       'idFF6ED153-BC31-4E74-8302-600261B247FA',\n       'idFF81A662-7FFC-4989-A89E-D153951A918C',\n       'idFF9931E9-5ABB-40DF-A188-B16D9A91730A',\n       'idFFA410C6-1645-4197-A7CA-8D2426EB327B',\n       'idFFA88491-3DDA-4831-A84C-5E3C714C456A',\n       'idFFCECD51-C8D3-4DEF-9258-88B0240A502B',\n       'idFFD6A162-EFB8-4A17-9B2E-8D500B56A33F',\n       'idFFE0D1A6-2B10-40AE-8E6E-0AEB92375117'],\n      dtype='object', name='to_id', length=9254))modePandasIndexPandasIndex(Index(['transit', 'car', 'bike', 'walk'], dtype='object', name='mode'))Attributes: (0)\n\n\nLoad greenspace data.\n\nacc_greenspace = pd.read_csv(\n    f\"{data_folder}/processed/accessibility/acc_greenspace_allmodes_15min_tynewear.csv\",\n    index_col=0,\n)\n\n\nacc_greenspace.columns = [\"transit\", \"car\", \"bike\", \"walk\"]\n\n\ngreenspace = xr.DataArray.from_series(acc_greenspace.stack()).rename(\n    {\"level_1\": \"mode\"}\n)\ngreenspace.name = \"green_accessibility\"\n\n\nbaseline = xr.merge([baseline, greenspace])\nbaseline[\"green_accessibility\"] = baseline[\"green_accessibility\"].fillna(0)\n\nCreate a wrapper class.\n\nacc = Accessibility(baseline)\n\nTest the object on example input.\nCreate random OA data.\n\noa_data = pd.Series(\n    np.random.randint(-100, 100, len(baseline.from_id)),\n    index=baseline.from_id.values,\n    name=\"oa\",\n)\noa_data.index.name = \"to_id\"\n\n\noa_data\n\nto_id\nE00041363    54\nE00041364    91\nE00041366    25\nE00041367    12\nE00041368    90\n             ..\nE00175601   -92\nE00175602   -48\nE00175603     4\nE00175604   -84\nE00175605   -35\nName: oa, Length: 3795, dtype: int64\n\n\nCompute accessibility.\n\nacc.job_accessibility(oa_data, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'combined' (from_id: 3795)&gt;\narray([ 9495., 10409.,  2972., ...,   395.,  1370.,   502.])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArray'combined'from_id: 37959.495e+03 1.041e+04 2.972e+03 8.643e+03 ... 963.0 395.0 1.37e+03 502.0array([ 9495., 10409.,  2972., ...,   395.,  1370.,   502.])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)\n\n\n\nnew_green = pd.Series(\n    np.random.randint(-10000, 10000, len(baseline.from_id)),\n    index=baseline.from_id.values,\n    name=\"oa\",\n)\nnew_green.index.name = \"to_id\"\n\n\nacc.greenspace_accessibility(new_green, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (from_id: 3795)&gt;\narray([ 484420.89095001,  519545.30120002,  297545.45855   , ...,\n       1828516.73930004, 1742541.35770002,   54213.1107    ])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArrayfrom_id: 37954.844e+05 5.195e+05 2.975e+05 ... 1.829e+06 1.743e+06 5.421e+04array([ 484420.89095001,  519545.30120002,  297545.45855   , ...,\n       1828516.73930004, 1742541.35770002,   54213.1107    ])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)\n\n\nSave the custom class to a compressed joblib.\n\nwith open(f\"{data_folder}/models/accessibility.joblib\", \"wb\") as f:\n    joblib.dump(acc, f, compress=True)\n\nTest loaded class\n\nwith open(f\"{data_folder}/models/accessibility.joblib\", \"rb\") as f:\n    acc2 = joblib.load(f)\n\n\nacc2\n\n&lt;demoland_engine.indicators.Accessibility at 0x175c43290&gt;\n\n\n\noa_data = pd.Series(\n    np.random.randint(-100, 100, len(acc2.baseline.from_id)),\n    index=acc2.baseline.from_id.values,\n    name=\"oa\",\n)\noa_data.index.name = \"to_id\"\nacc2.job_accessibility(oa_data, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'combined' (from_id: 3795)&gt;\narray([ 9051., 10132.,  2795., ...,   973.,  1547.,  -303.])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArray'combined'from_id: 37959.051e+03 1.013e+04 2.795e+03 8.589e+03 ... 973.0 1.547e+03 -303.0array([ 9051., 10132.,  2795., ...,   973.,  1547.,  -303.])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)\n\n\n\nnew_green = pd.Series(\n    np.random.randint(-10000, 10000, len(acc2.baseline.from_id)),\n    index=acc2.baseline.from_id.values,\n    name=\"oa\",\n)\nnew_green.index.name = \"to_id\"\nacc2.greenspace_accessibility(new_green, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (from_id: 3795)&gt;\narray([ 489257.89095001,  528378.30120002,  307546.45855   , ...,\n       1775474.73930004, 1768524.35770002,   48372.1107    ])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArrayfrom_id: 37954.893e+05 5.284e+05 3.075e+05 ... 1.775e+06 1.769e+06 4.837e+04array([ 489257.89095001,  528378.30120002,  307546.45855   , ...,\n       1775474.73930004, 1768524.35770002,   48372.1107    ])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)"
  }
]