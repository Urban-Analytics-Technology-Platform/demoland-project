[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DemoLand",
    "section": "",
    "text": "This project develops a modelling system which is able to quantify several competing aspects of land use in a given urban environment as it currently exists (baseline) and build scenarios under changes that affect the distribution of such land use. It comprises a sequence of models designed to predict the impact of land use changes following large-scale planning decisions on the subset of indicators reflecting the quality of life. At the same time, it aims to determine the optimal land use composition given set indicator levels using neural networks.\nThe project is a partnership between the Geospatial Commission and The Alan Turing Institute, working with Newcastle City Council to develop a modelling system that leverages data science and AI to support decision-making in land use policy. In particular, the project provides tools to help strategic planning by helping decision-makers explore large-scale changes in land use through: - Evaluation of the impact on their policy priorities (house prices, air quality, accessibility to jobs and green space) - Use of machine learning and AI to suggest interventions to achieve policy outcomes."
  },
  {
    "objectID": "book/intro.html#summary",
    "href": "book/intro.html#summary",
    "title": "2  Intro",
    "section": "2.1 Summary",
    "text": "2.1 Summary\n\n\n\n\n\n\n\nObjectives\nDevelop modelling system to quantify features of land use in urban environment\n\n\nOutput\nPrediction of the quality of life indicators following a modelled scenarios of development\nstudy case: Tyne and Wear County (Local Authorities: Gateshead, Newcastle Upon Tyne, North Tyneside, South Tyneside, Sunderland)\n\n\nHow\nCreating key indicators for assessing a baseline scenario and future scenarios\n\n\nDuration\n6 months"
  },
  {
    "objectID": "book/intro.html#project-aims",
    "href": "book/intro.html#project-aims",
    "title": "2  Intro",
    "section": "2.2 Project aims",
    "text": "2.2 Project aims\nThe project aims to provide insight into the impact of policies affecting land use in cities across the UK, piloting on the case of Tyne and Wear. It:\n\nDerives indicators of quality of life.\nDevelops machine learning models able to predict the impact of land use changes on such indicators.\nAnd inversely, develops a neural network able to predict the required land use change to reach target levels of QoL indicators.\n\nAll these technological components are presented in an interactive tool allowing quick and easy exploration of impacts aimed at policymakers."
  },
  {
    "objectID": "book/intro.html#explaining-the-science",
    "href": "book/intro.html#explaining-the-science",
    "title": "2  Intro",
    "section": "2.3 Explaining the science",
    "text": "2.3 Explaining the science\nThe project defines four indicators related to the quality of life, capturing selected dimensions of the environment, society and economy: air pollution, house price, jobs accessibility, and green space accessibility.\nWhile air pollution and house price are composed of the observed values, accessibility metrics were generated for this project.\nFor air pollution and house prices, the project develops machine learning-based, predictive models based on land use variables derived from the Urban Grammar project to allow assessment of land use change.\nThe accessibility metric to two different opportunities (jobs and green space) is calculated for four modes of transport (walking, bicycle, vehicles, public transit) between a relevant set of origins and destinations at the UK census Output Area level.\nThe project is done in collaboration with the Geospatial Commission and Newcastle City Council (NCC). It defines development scenarios of the Tyne and Wear county, for which it reports predicted changes of selected indicators, allowing assessment of a proposed land use change based on machine learning.\nThe outputs are presented in an interactive visual web-based environment allowing quick comparison and presentation for policymakers and professionals."
  },
  {
    "objectID": "book/intro.html#quality-of-life-indicators",
    "href": "book/intro.html#quality-of-life-indicators",
    "title": "2  Intro",
    "section": "2.4 Quality of life indicators",
    "text": "2.4 Quality of life indicators\nThe project defies four measurable indicators that can be linked to quality of life.\n\nAir quality\nGreen space accessibility\nHouse prices\nJobs accessibility\n\nThe indicators are first modelled for a baseline scenario reflecting the current land use in the study area. Indicators are modelled using a predictive regression models based on the land use features will that used as explanatory variables and measured using the multimodal accessibility model. See the Methodology for details."
  },
  {
    "objectID": "book/intro.html#scenarios-of-development",
    "href": "book/intro.html#scenarios-of-development",
    "title": "2  Intro",
    "section": "2.5 Scenarios of development",
    "text": "2.5 Scenarios of development\nOnce the baseline is created, the project defines a number scenarios of future development (e.g. densification of city centre or land release in the green belt area) and asses the effect of those scenarios on the quality of life indicators.\nSee the chapter on scenarios for details and the interactive mapping tool to explore the results."
  },
  {
    "objectID": "book/data_sources.html#list-of-data-source",
    "href": "book/data_sources.html#list-of-data-source",
    "title": "3  Data Sources",
    "section": "3.1 List of data source",
    "text": "3.1 List of data source\nBelow is a list of data sources organised by the purpose\n\n3.1.1 Project-wide\n\nOS | 2011 Census Output Areas | link\n\n\n\n3.1.2 Accessibility indicators\n\nONS | Population Weighted Centroids (PWC) of Output Areas | link\nONS | Workplace zones | link\nOS | Open Greenspace | link\nOSM | OpenStreetMap network data | link\nITS Leeds | UK GTFS data | link\n\n\n\n3.1.3 Air quality\n\nDEFRA | UK AIR | link\n\n\n\n3.1.4 House price\n\nChi et al. | A new attribute-linked residential property price dataset for England and Wales 2011-2019 | link\n\n\n\n3.1.5 Explanatory variables for modelling\n\nONS | Population estimates | link\nONS | Workplace population | link\nCopernicus | CORINE Land Cover classification | link\nThe Alan Turing Institute | Spatial Signatures of Great Britain | link"
  },
  {
    "objectID": "book/method.html#building-indicators",
    "href": "book/method.html#building-indicators",
    "title": "4  Methodology",
    "section": "4.1 Building Indicators",
    "text": "4.1 Building Indicators\nThe original list of indicators (see Intro) was pruned after an initial review of the available data, software, interests of the stakeholders.We resolved to 4 indicators of interest:\n\nAir quality\nGreen space accessibility\nHouse prices\nJobs accessibility\n\nFor each of these indicators, we have detailed below how they are generated. This step represents the basic scenario.\nWe have two different types of indicators: while air quality and house prices are “actual data” measurements, accessibility is estimated and generated in a deterministic way, so it can’t be directly modelled/predicted using some explanatory variables.\n\n4.1.1 Accessibility calculation\nFor accessibility computation, we consider the potential accessibility (cumulative) to green spaces and jobs, that represent the opportunities. We consider a set of (punctual) origins and opportunities’ destinations and the time it takes to move between them on the road network, given different transport modes. In order to calculate the time it takes from each origin to reach each destination, we need to have a Time Travel Matrix (TTM) between origins and destinations for different means of transport.To build this, we can generate a graph/network starting from two data sources: data for the road network and data for public transportation schedules. Once a TTM is built between one set of origins and one set of destinations for a given mode of transport, we can run accessibility analysis on it.\n\nOrigins - For the scope of the project, we consider as origins the coordinates of the Population Weighted Centroids (PWC) of Output Areas (OA).\nDestinations - Per each destination we need to provide the coordinates and a datum for the opportunity (or ‘supply’, in our case this is the land use), for example the number of jobs available (other opportutnities could be number of schools, health centres, other types of countable features).\nFor jobs count we consider as destinations the PWC of the working population per Workplace Zones (WPZ) from the 2011 Census (data available and definition). As an opportunity measure (the job counts) we make the approximation that the number of workers equals the number of jobs available. This is an approximation we take in order to preserve the spatial location of the jobs, that is relevant in the calculation of the time travel matrix. In fact the number of jobs in the Census is at OAs level, but the PWC for this aggregated level can be off-centered in relation to where people work.\nFor green spaces accessibility we use the open data set available from Ordnance Survey (OS). In particular as a first approximation we consider the layer “Access points” from this datum. This gives the coordinates of the access points to green spaces in all Great Britain, which we take as destinations.As an opportunity measure we count the areas of the greenspace (layer “Sites”) to which these points give access to, see below for details on how we consider the cumulative opportunity of this datum.\n\n1. Indicator definition\n\nJobs accessibility\n\n\nNumber of jobs accessible by public transport and walking in 15 minutes\n\n\nGreen spaces accessibility\n\n\nSum of the areas of green space sites reachable within 15 minutes\n\n\nobtain greenspace sites from OS\n\nfilter out not relevant categories (allottments, golf courses, bowling greens)\nretain entrances on the edge of the sites\nassociate\n\n~~ obtain “missing” areas from Corine or OSM~~ idea for future improvement\nget sites’ area\nfilter entrances within time threshold (15min) per each origin (OA)\nconsider unique values for parks (can be reached in )\ngenerate metric per each OA as Sum [reachable site size]\n\n2. Build time travel matrix (TTM)\nWe can build a TTM from two data sources: the roads network and a time table for public transport. We can obtain the first by donwloading OpenStreetMap (OSM) data for the area of interest. Time table for public transport in GTFS format is available in England from UK2GTFS. We use GTFS data because more of compatibility with the ttm calculation package.\n\nget time table for public transport &gt; GTFS data\nget roads network &gt; OSM data\ngenerate network graph &gt; r5 engine ([Conveyal(https://github.com/conveyal/r5)]) &gt; r5py package\ngenerate ttm for 4 different modes &gt; ‘transit’, ‘bike’, ‘car’, ‘walking’\ntime is fixed atm &gt; idea for future improvement: edit for taking into account different day of the year/week, time of the day\n\n3. Run accessibility analysis\n\nget land use data (opportunities detailed above in _*Destinations_)\njobs &gt; run tracc package on the ttm per each transport mode:\n\ncompute impedance function based on a 15 minute cost (cumulative)\nsetting up the accessibility object IE joining the destination data to the travel time data\nmeasuring potential accessibility to jobs as cumulative sum of opportunities at destination per each origin\n\ngreenspace &gt; convert the tracc functions to work only on reachable sites’ area, on the ttm per each transport mode:\n\ncompute impedance function based on a 15 minute cost (cumulative)\njoin the destination data to the travel time data\nper each OA (origin), filtering only entrances with time travel within the threshold\nper each OA, filter entrances (only one per park)\nper each OA, assign metric as sum[parks’ area]\n\n\n\n\n4.1.2 Air quality index\nWe develop an air quality index as a composite of \\(PM_{2.5}\\), \\(PM_{10}\\), \\(NO_{2}\\) and \\(SO_{2}\\) particle values derived from the UK AIR project run by DEFRA with data available as a 1km grid (https://uk-air.defra.gov.uk/data/pcm-data). The composite index follows the methodology of European Environmental Agency, reflecting the relative health risk associated to the exposure to particle intensities.\n\nThe bands are based on the relative risks associated to short-term exposure to PM2.5, O3 and NO2, as defined by the World Health Organization in its report on the Health Risks of Air Pollution in Europe project (HRAPIE project report).\nThe relative risk of exposure to PM2.5 is taken as basis for driving the index, specifically the increase in the risk of mortality per 10 µg/m3 increase in the daily mean concentration of PM2.5.\nAssuming linearity across the relative risks functions for O3 and NO2, we calculate the concentrations of these pollutants that pose an equivalent relative risk to a 10 µg/m3 increase in the daily mean of PM2.5.\nFor PM10 concentrations, a constant ratio between PM10 and PM2.5 of 1:2 is assumed, in line with the World Health Organization´s air quality guidelines for Europe.\nFor SO2, the bands reflect the limit values set under the EU Air Quality Directive.\n\nThe relationship between \\(PM_{2.5}\\) : \\(PM_{10}\\) : \\(NO_{2}\\) : \\(O_{3}\\) : \\(SO_{2}\\) is then equal to 1 : 2 : 4 : 5 : 10. The combined index can then be computed as \\[Q_{air} = \\frac{PM_{2.5}}{1} + \\frac{PM_{10}}{2} + \\frac{NO_{2}}{4} + \\frac{O_{3}}{5} + \\frac{SO_{2}}{10}\\]\nExcept for the \\(O_{3}\\), UK AIR reports all as a concentration in \\(\\mu g \\cdot m^{-3}\\). \\(O_{3}\\) is reported as a number of days above a threshold of 120 \\(\\mu g \\cdot m^{-3}\\) and cannot be used in this formula but even EEA omits data when unavailable so we shall be able to create the index based on 4 remaning measurements.\nIt shall also be considered that UK AIR is not representing a direct measurements but a model.\nThe data from the 1km grid are spatially interpolated to Output Area geometry. The index itself is computed on the grid.\n\n\n4.1.3 House price\nAn optimal way of working with house prices in the modelling exercise like ours is to use price per sqm. However, those are not generally available at the level of Output Area or similar. Luckily, we can retrieve such data from the “A new attribute-linked residential property price dataset for England and Wales 2011-2019” project by Chi et al. available from 10.5255/UKDA-SN-854240. That contains individual house prices, total floor area and a resulting price per sqm resulting from a combination of Land Registry Price Paid Data and Domestic Energy Performance Certificates.\nWe use the data from years 2018-2019 and compute the mean price per sqm per output area. Given the data is not fully up-to-date, the modelling results will be presented as a percentual increase or decrease compared to the baseline value rather than in absolute values."
  },
  {
    "objectID": "book/method.html#explanatory-variables",
    "href": "book/method.html#explanatory-variables",
    "title": "4  Methodology",
    "section": "4.2 Explanatory variables",
    "text": "4.2 Explanatory variables\nWe use a set of explanatory variables describing the environment that can be changed to model different scenarios and have, at the same time, explanatory power to predict our four indicators.\n\n4.2.1 ONS population estimates\nWe use the ONS population estimates on the OA level for mid-2020. The other data used in the project are generally reflecting the period 2018-2020, so we want to ensure we describe the compatible point in time. The dataset is retrieved from the ONS. Given it is reported on Output Area level, it can be simply joined.\n\n\n4.2.2 Workplace population by industry\nWorkplace population is coming from the Census 2011 and is reported on Workplace Zone geometries. We use the data preprocessed in the Urban Grammar project that aggregated industries into following groups:\n\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\n\nThe data is then interpolated from Workplace Zones to Output Areas.\n\n\n4.2.3 CORINE Land Cover classification\nWe use CORINE Land Cover Classification for 2018 distributed as polygons of contiguous areas belonging to the same classs. We use the data extracted for the Great Britain within the Urban Grammar project and interpolate the data onto Output Areas capturing a proportion of each OA covered by each class. We further filter out fully or nearly invariant classes and use only:\n\nDiscontinuous urban fabric\nContinuous urban fabric\nNon-irrigated arable land\nIndustrial or commercial units\nGreen urban areas\nPastures\nSport and leisure facilities\n\n\n\n4.2.4 Urban morphometrics\nUrban morphometrics offers a way of describing physical built environment (buildings, streets) in a set of measurements capturing different aspects of morphological elements. We directly use the set measured within the Urban Grammar project, presented as individual characters (prior contextualisation) and interpolate their values from the original geometry (enclosed tessellation cells) to output areas. That way, we already get a contextual version using the project-specific aggregation to OA. This gives us 59 morphometric variables.\n\n\n4.2.5 Variable pruning\nGiven some of the explanatory variable were collected for a different purpose than modelling and with a different geographical extent in mind, it may happen that within our limited area of interest (Tyne and Wear), some of the variable are collinear. Those may negatively affect the performance of the predictive models and it is better to limit their number to minimum. We, therefore, measure Pearson’s correlation index and Spearman Rank correlation index between all pairs of explanatory variables and from each pair with the absolute value of the index above 0.8 retain only one, ideally the one that is more interpretable.\nThe final set contains 59 explanatory variables and the only morphometric were pruned down.\n\npopulation estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\nLand cover [Continuous urban fabric]\nLand cover [Non-irrigated arable land]\nLand cover [Industrial or commercial units]\nLand cover [Green urban areas]\nLand cover [Pastures]\nLand cover [Sport and leisure facilities]\narea of building\ncourtyard area of building\ncircular compactness of building\ncorners of building\nsquareness of building\nequivalent rectangular index of building\ncentroid - corner mean distance of building\ncentroid - corner distance deviation of building\norientation of building\narea of ETC\ncircular compactness of ETC\nequivalent rectangular index of ETC\ncovered area ratio of ETC\ncell alignment of building\nalignment of neighbouring buildings\nmean distance between neighbouring buildings\nperimeter-weighted neighbours of ETC\nmean inter-building distance\nwidth of street profile\nwidth deviation of street profile\nopenness of street profile\nlength of street segment\nlinearity of street segment\nmean segment length within 3 steps\nnode degree of junction\nlocal proportion of 3-way intersections of street network\nlocal proportion of 4-way intersections of street network\nlocal proportion of cul-de-sacs of street network\nlocal closeness of street network\nlocal cul-de-sac length of street network\nsquare clustering of street network\nlocal degree weighted node density of street network\nstreet alignment of building\narea covered by edge-attached ETCs\nbuildings per meter of street segment\nreached ETCs by neighbouring segments\nreached ETCs by tessellation contiguity\narea of enclosure\ncircular compactness of enclosure\nequivalent rectangular index of enclosure\norientation of enclosure\nperimeter-weighted neighbours of enclosure\narea-weighted ETCs of enclosure"
  },
  {
    "objectID": "book/method.html#modelling-regression-analysis",
    "href": "book/method.html#modelling-regression-analysis",
    "title": "4  Methodology",
    "section": "4.3 Modelling / regression analysis",
    "text": "4.3 Modelling / regression analysis\nWhile the accessibility indicators are a result of a deterministic model based on a fixed travel-time matrix, hence any scenario modelling can re-use it and change only the raw values at the destinations, we need to develop predictive models to be able to see the change of air quality and house price.\nBoth models are based on the same architecture, a Histogram-based Gradient Boosting Regression Tree as implemented in the scikit-learn Python package, and a set of 59 explanatory variables listed above. To ensure we are able to properly captrue the spatial nature of the indicators, we add another 59 variables on top that are a result of a spatial lag of the original set. The spatial lag is measured as a mean value of the variable within a set neighbourhood, which definition is determined empirically. We test the neighbourhoods based on contiguity (from order of contiguity 1 to 5 inclusive of lower-order neighbors), Euclidean distance (500m, 1000m, 2000m) and their unions. Each option is then part of the grid search aimed at the selection of the best model parameters and the best defition of the spatial lag in relation to the model performance metrics (R2, MSE, ME). Model parameters that are being assessed are learning rate, maximum number of iterations and maximum number of bins. For more details, see the implementation in the Air Quality notebook and the House Price notebook.\nFurthermore, we have experimented with the geographical extent of the training data. While the original models were based only on the data from the study area (Tyne and Wear), we have tested models trained on other geographical subsets:\n\nwhole England\nEngland excluding Greater London, known to be an outlier within the country that may not help in predictive quality within Tyne and Wear\nTyne and Wear plus all OAs belonging to “Urbanity” classes from the rest of England\n\nThis was to add additional robustness to the model by training on a wider set of data.\n\n4.3.1 Resulting model specifications\nThe resulting models, selected based on their performance, are based on the following specifications:\nAir Quality\n\nSpatial extent: Tyne and Wear plus all OAs belonging to “Urbanity” classes from the rest of England\nSpatial lag: A union of 5 orders of Queen contiguity and 2000m distance band\nModel parameters: learning_rate=0.2, max_bins=64, max_iter=1000\n\nThe model performance (R2) is ~0.8.\nHouse Price\n\nSpatial extent: England excluding Greater London\nSpatial lag: 5 orders of Queen contiguity\nModel parameters: learning_rate=0.1, max_bins=128, max_iter=1000\n\nThe model performance (R2) is ~0.5."
  },
  {
    "objectID": "book/method.html#building-scenarios",
    "href": "book/method.html#building-scenarios",
    "title": "4  Methodology",
    "section": "4.4 Building scenarios",
    "text": "4.4 Building scenarios\nThe mechanism for scenario building is based on four macro variables, which combination driving the sampling mechanism deriving the 59 variables used in the modelling. The four variables are Level of urbanity, which is a proxy for a signature type of spatial signatures as defined in the Urban Grammar project, Use of buildings controlling the ratio between residential and commercial or industrial use, Job types, controlling whether job structure in each output area is more white-collar or blue-collar, and Green space, reflecting the amount of formal green space (i.e. parks).\nScenarios are modelled by specifying the macro variables on output areas where the change is assumed. Any of the four variables can be specified in combination with any other. The algorithm then samples the data from either baseline capturing the existing state or from the known distribution of values per signature type based on the country-wide data.\nThe first step in the sampling procedure is the selection of a signature type. If it is not changed, subsequent steps modify the baseline. Otherwise, we sample the values for a set signature type reflecting its common characterisation as observed across Great Britain. For example, if we specify the Local urbanity signature type for an output area that had Dense urban neighbourhoods assigned, we look at how Local urbanity usually looks like and sample all the required values from a narrow normal distribution around the median of the nation-wide distribution. Further macro variables are adjusting these values. Use will change the variables of population and workplace population, job types will change the distribution of jobs to job type categories, and green space will allocate new parks onto an output area (adjusting other values like population accordingly).\nDevelopment of each scenario is then composed of a few simple steps:\n\nSelect output areas that are supposed to change\nAssing target level of urbanity for each\nAssing another macro variable for each\nSample the values as input for the model\nRun the models to assess the effect of tested changes"
  },
  {
    "objectID": "book/scenarios.html#scenario-1-low-density-residential-development",
    "href": "book/scenarios.html#scenario-1-low-density-residential-development",
    "title": "5  Scenarios",
    "section": "5.1 Scenario 1: Low-density residential development",
    "text": "5.1 Scenario 1: Low-density residential development\nThis scenario models the situation where land in the green belt is released for development. The area is taken over by a large developer used to build residential areas around the country. The new neighbourhood is a combination of low-rise detached and semi-detached housing with only minimal additional land use. The primarily residential neighbourhood does not generate a significant amount of jobs, inducing higher traffic to industrial zones and Newcastle city centre. The development is located west of the city around Callerton.\nThe new development is modelled as a combination of open sprawl and disconnected suburbia signature types, combined with an estimation of the allocation of new population and a small number of jobs in retail and education.\nThe model results in approximately:\n\n63 000 new residents\n9 250 new jobs (14% of residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-2-mid-density-mixed-neighbourhood",
    "href": "book/scenarios.html#scenario-2-mid-density-mixed-neighbourhood",
    "title": "5  Scenarios",
    "section": "5.2 Scenario 2: Mid-density mixed neighbourhood",
    "text": "5.2 Scenario 2: Mid-density mixed neighbourhood\nThis scenario models the development in the green belt under the idea of a 15-minute neighbourhood that is dense, therefore taking up less space and mixed in terms of use. Such a neighbourhood contains not only residential housing but also a few places for new retail, commercial, and other uses. As such, it should be more self-sufficient than the low-density Scenario 1, inducing less traffic from the neighbourhood to other areas in the city. The development is assumed to be in the same area west of Callerton as in Scenario 1. The form is composed more of row houses and multi-story tenement buildings forming the centre of the new neighbourhood.\nIt is modelled as a combination of accessible suburbia, connected residential neighbourhoods and dense residential neighbourhoods signature types, with an approximation of new population and job allocation. Land cover is changed accordingly to discontinuous urban fabric and continuous urban fabric, but on a smaller area than in Scenario 1, leaving space dedicated to large urban parks.\nThe model results in approximately:\n\n54 000 new residents\n10 500 new jobs (19 % of residents)\n2 300k sq.m. of new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-3-densification-of-inner-city",
    "href": "book/scenarios.html#scenario-3-densification-of-inner-city",
    "title": "5  Scenarios",
    "section": "5.3 Scenario 3: Densification of inner city",
    "text": "5.3 Scenario 3: Densification of inner city\nThis densification scenario models a high-density development in the already developed areas, following the gradual infill and rebuilding existing buildings into higher ones with more mixed-use. It is a long-term strategy aimed at preserving green spaces (especially the green belt) and creating 15-minute neighbourhoods in the existing city by adding new layers of functionality and new inhabitants to places that are already built. The scenario affects most of the city, with higher densification levels around local centres and main streets and lower levels in suburban residential areas.\nIt is modelled as a change of signature types based on their hierarchy to higher order ones and related estimations of new population and job allocation. Land cover changes from discontinuous urban fabric to continuous urban fabric.\nThe model results in approximately:\n\n25 000 new residents\n24 000 new jobs (96% of residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-4-brownfields-to-dense-neighbourhoods",
    "href": "book/scenarios.html#scenario-4-brownfields-to-dense-neighbourhoods",
    "title": "5  Scenarios",
    "section": "5.4 Scenario 4: Brownfields to dense neighbourhoods",
    "text": "5.4 Scenario 4: Brownfields to dense neighbourhoods\nExisting brownfield land is redeveloped into high-density neighbourhoods with mixed-use, providing housing, services, and commercial units in an attempt to densify the inner city without affecting existing areas. Compared to scenario 3, this strategy is less invasive but has a lower scale. However, both scenarios can be potentially combined, as shown in Scenario 6.\nIt is modelled as a change of signature types on brownfield land to dense urban neighbourhoods and local urbanity, plus relevant changes in land cover, population and job allocation.\nThe model results in approximately:\n\n55 700 new residents\n-1 000 new jobs (0% of residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-5-brownfields-into-parks",
    "href": "book/scenarios.html#scenario-5-brownfields-into-parks",
    "title": "5  Scenarios",
    "section": "5.5 Scenario 5: Brownfields into parks",
    "text": "5.5 Scenario 5: Brownfields into parks\nContrary to Scenario 4, this scenario assumes that all the brownfield land is turned into urban parks with no development. While it does not help to solve the issue of the capacity of a city, it may be viewed favourably by the local population and can balance potential densification as outlined in Scenario 3. Both Scenarios 3 and 5 can be combined, as shown in Scenario 7.\nIt will be modelled as a change of signature types on brownfield land to park/warehouse land plus relevant land cover changes and removal of any population and job allocation.\nThe model results in approximately:\n\n0 new residents\n-20 000 new jobs\n6 230k sq.m. of new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-6-urbanisation-to-the-edge",
    "href": "book/scenarios.html#scenario-6-urbanisation-to-the-edge",
    "title": "5  Scenarios",
    "section": "5.6 Scenario 6: Urbanisation to the edge",
    "text": "5.6 Scenario 6: Urbanisation to the edge\nThis scenario models the city following the densification strategies outlined in both Scenarios 3, where we target higher density to already-dense central areas, and Scenario 4, we model the development of dense neighbourhoods in the brownfield and industrial areas around the River Tyne. As such, the scenario combines changes from Scenarios 3 and 4.\nThe model results in approximately:\n\n66 000 new residents\n30 000 new jobs (45% of residents)\nno new parks"
  },
  {
    "objectID": "book/scenarios.html#scenario-7-urbanisation-with-greenery",
    "href": "book/scenarios.html#scenario-7-urbanisation-with-greenery",
    "title": "5  Scenarios",
    "section": "5.7 Scenario 7: Urbanisation with greenery",
    "text": "5.7 Scenario 7: Urbanisation with greenery\nThis scenario directs changes to two locations in two different directions. First, it assumes the densification of an already-dense city centre as outlined in Scenario 3, adding further population, jobs, and services to the area. Second, it combines this densification with the creation of new large parks around the River Tyne, where current brownfields and industrial areas are. As such, the scenario is, in principle, a combination of changes from Scenarios 3 and 5.\nThe model results in approximately:\n\n1000 new residents\n30 000 new jobs\n6 230k sq.m. of new parks"
  },
  {
    "objectID": "book/scenarios.html#results",
    "href": "book/scenarios.html#results",
    "title": "5  Scenarios",
    "section": "5.8 Results",
    "text": "5.8 Results\nYou can see the results of these scenarios in the interactive web application."
  },
  {
    "objectID": "book/notes.html#february",
    "href": "book/notes.html#february",
    "title": "6  Notes",
    "section": "6.1 February",
    "text": "6.1 February\n\n6.1.1 Greens space sites\nA first accessibility computation using Ordnance Service (OS) open data was rejected, as not in line with expected results.\nThe reason for this are:\n\nthe data misses some vast green sites outside of town\nthe chosen metric was not proper [IE count the n of access points accessible within 15 min from origins]\n\nDealing with the two issues separately:\n\nThe good thing of the OS data is that they are accurate (resolution, categories) AND they contain also the access points (which for everyone’s sake we are gonna refer to as entrances). NOTE: we need entrances to green space as the destinations for our time travel matrices. A good candidate to make up for not covered areas is the landuse layer from OpenStreetMap (OSM).\n[from OSM landuse layer we consider the tags/fields: cemetery, forest, nature reserve, park, recreation ground]\nOSM data has very much good cover of the existing green sites, but: lacks a proper classification, doesn’t have entrances to sites, doesn’t distinguish between public/private/military spaces, therefore the use of OSM would bring to over-estimate green spaces. Example: in OSM we have some side-road areas inside Metrocentre classified as forest.\nIn order to understand parks where people actually can/does pass through, a cross-check was carried out manually looking for parks with existing trails (passing through them) using https://hiking.waymarkedtrails.org/ AND Google Street View to check entrances to parks/greens/ etc [IE some entrances actually show a sign that make it clear if it’s a public access or not]\nList of parks/green spaces which we would want to consider in the green belt/suburbs of Tyne and Wear (and their status, IE open/public etc):\n\n\n\npark name\nstatus\nlocation\n\n\n\n\nChopwell Woodland\nopen\nSW\n\n\nMilkweelburn Wood\nopen\nSW\n\n\nWatergated Park\nopen\n\n\n\nSilverhill Wood / Ravesworth Estate\nclosed\n\n\n\nBanesley Lane woodland\nopen\n\n\n\nHedley Hall Woods\nopen\n\n\n\nSpen Banks / Sherburn Green Woods\nopen\n\n\n\nClinty Wood\n?\n\n\n\nGibside national Trust\nclosed ($$)\n\n\n\n\n\nWe want to generate a metric that take into account the proximity to a green space AND the size of the site you are accessing to. It doesn’t matter how many access points are accessible, as long as one gives you access to the site within 15 min (walk, bike, other means)\nMetric: once generated the time travel matrix (OAs centroids to GS entrances) retain one access point per park, IE consider only once each park that is reachable at least through one access point in 15 min. Then sum up all the areas of the reachable parks.\nIn order to do so, we need to clean the data from the eccessive entrances: many sites are included within other ones (for example a playground or tennis court can be enclosed in a public park). These enclosed sites also generate an overestimate of the accessible area, because they are overlapping with the main major park - which is the only category/feature/polygon we would like to consider.\nFirst of all we clean the OS data from not-wanted categories, IE areas which are green but not openly accessible by the public. These include: allottments, golf courses, bowling greens.\nWe then can do this consideration: most of the times it’s the 2 categories Playing Field and Public Park Or Garden that contain other categories within their boundaries. We could play with this. Other Sport Facilities also contains and is contained, but mostly is contained.\nThe hierarchy of the layers seems to be roughly this: Public Park Or Garden contains Playing Field contains Other Sport Facilities contains rest of categories.\nAfter several trials with spatial joins and other geospatial operations, have managed to obtain a good results by separating the above categories into single layers and performing operations among them. See notebook greenspace_datacleaning.ipynb for details.\n\n\n6.1.2 Open issues/questions on indicators building\nAccessibility\naccessibility computation:\n\nrouting engine: 5r\n\nin python:\n\ntogether with tracc or pysal/access\n~~use conveyal with mongoDB ~~(not tried yet)\n\nin R: wrapped within r5r (not tried yet)\n\nttm generation:\n\nwe only consider origins within the county, what about extra zonal areas?\ncan generate one single table with multimodal trip &gt; can create a comprehensive table using scripting\nhow does it work with scenarios generation - especially for OSM\nusing random date and time in database: 2023,1,19,8,30;\n\ncould generate several network graphs and get median value of them all (see Pereira about this)\noptimal date? (Journey Time Statistics use a Tuesday in October)\n\nthe ttm with origins OAs (centroids) to destinations LSOAs (centroids) for transit (leg_mode walk) takes 39 min!\nOAs as origins … while we could “force” the points to the nearest road node (see Statistics method mentioned above)\n\nGTFS data:\n\ngood source for UK or do we need to generate it ourselves from\ncan we edit the GTFS file, in case we want add a route / stop?\n\nmeasuring acc to jobs:\n\ncan we find more detailed data of where people work?\nand of where people live?\n\n15 min neighborhoods\n\napproximating where people live, IE can we improve the location from actual approximation to population weighted centroids?\n\n\nGTFS data handling\nChecking the possibility to view and edit GTFS data with [gtfs_kit](https://gitlab.com/mrcagney/gtfs_kit) in Python. All the trials are in python/gtfs_kit_trials.ipynb. Note: created env_gtfs-kit.yml environment to test the package.\nworking with the files in raw/GTFS:\n\nitm_all_gtfs.zip:\n\nGTFS file downloaded from https://data.bus-data.dft.gov.uk/timetable/download/gtfs-file/all/\nthis file was accessed though the website https://www.transit.land/feeds/f-busdftgov~uk\nIt contains all the UK buses only? CHECKING\n\nitm_north_east.gtfs.zip:\n\nshould be a subset of the one above for North East England\ndownloaded from same source, where I have managed to get subset (don’t remember how)\n\n\nNE.zip:\n\nGTFS generated and downloaded from UK2GTFS https://github.com/ITSLeeds/UK2GTFS, see releases\nthis file has no shape.txt but it’s for North East England\n\n\nHave done some trials in Qgis to view the routes (starting from the file shape.txt), we want to understand if only buses are in this GTFS data or also for example the light rail line (Yellow, owned and managed by Nexus) is included.\nIt might be a disadvantage to use the whole UK data, but it might be more complete than the local one (NE) generated in UK2GTFS.\nWe would like also to be able to edit the GTFS data, adding a line/stop.\n:bangbang: :loud_sound: update :loud_sound: :bangbang:\ndecided that the best source to use (compromise of coverage, file size, reliability) is the north_east version of bus-data.dft.gov\nin fact it looks loike the metro line (yellow) is included\nTwo screenshots from sources used to cross-check the data: 1. Dustin’s bus spotting website (bus spotting) \n\npersonal map on Felt with added transport network layer \n\n\n\n6.1.3 Jobs location\nWe have number of jobs as a datum down to LSOA level, it comes from nomis\n\nAn employer survey of the number of jobs held by employees broken down by full/part-time and detailed industry (5 digit SIC2007). The survey records a job at the location of an employees workplace. Available from country down to lower level super output area and Scottish datazone.\n\nFor destinations, when calculating the travel time matrix (TTM) to jobs locations, we can use then the population weighted centroids (PWC) of LSOAs areas. This assumption though affects the distances we calculate, in particular when considering vast industrial/business estates, where very few population is present - in this case in fact the PWC centroid will be probably (or could be) located in a marginal point of the polygon (IE LSOA area), resulting in invalid results.\nSee as an example the image below (created in Qgis) showing the Team Valley trading estate in Gateshead location\n Red dots = OA centroids (our origins)pink lines = LSOAs boundaries yellow dots = LSOAs centroids blue dots = WPS centroids\nLooking for alternatives, I have found a geography for the Census that takes into account the working population.\nWorkplace Zones (WPZ)\n\nWorkplace Zones are a small-area geography designed to contain a consistent number of workers allowing workplace statistics to be released at a more granular level.\n\nThe data available at this geographic level are here and they include the Workplace population (table WP101EW)\n\nThis dataset provides 2011 Census estimates of the workplace population in England and Wales by residence type (household or communal resident), by sex and by age. The estimates are as at census day, 27 March 2011.\n\nWe have decided to use the number of workers as a proxy for jobs count (STRONG ASSUMPTION). We can then use as destinations the WPZ centroids, and as n. of jobs (opportunities) the working population for the WPZ. A stronger variable could be generated using the location from the WPZ (destination), and as opportunities a interpolation from the LSOA jobs count to the WPZ areas. Note that the relation between Output Areas and WPZ is that “in England and Wales they [WPS] are designed to align with Middle Layer Super Output Areas (MSOAs)”.\nConclusions: In any case we need to make an approximation: using LSOA we give up on location, using WPZ we give up on actual (estimated?) job count.\nThe ideal situation would be to have the number of jobs and their location / employment centres. A way to deal with the lack of data on this, could be to generate it using Free Company Data Product\n\ndownloadable data snapshot containing basic company data of live companies on the register\n\nFrom this data set we can get number of companies registered at postcode level (see data fields here) which is quite detailed, and then find a way to aggregate/interpolate them to identify “job centres”. We have postcodes centroids from geoportal-gov.\nAs a future improvement, keep in mind the idea to generate more precise job locations, either by using the House company data, or other sources (and ideas)."
  },
  {
    "objectID": "book/notes.html#january",
    "href": "book/notes.html#january",
    "title": "6  Notes",
    "section": "6.2 January",
    "text": "6.2 January\nJanuary starts basically as a continuation of the work started in December.\nWarming up Image created in Qgis showing median house prices (year ending March 2022) and green spaces in Tyne and Wear county (current case study):  Note:\n\nhouseprices data are associated to the LSOA boundaries shp via the python code in data_prep.ipynb\ngreen spaces are from the OS open data base see description here\n\n\n6.2.1 Accessibility accessibility accessibility\n\nAccessibility is understood as the ease with which a person can reach places and opportunities from a given location and it results from the interaction between the transport system, land-use patterns, and the constraints of individuals  (Pereira et al. 2019, Distributional effects of transport policies on inequalities in access to opportunities in Rio de Janeiro, Journal of Transport and Land Use 12(1): 741-764)\n\nLet’s start from green spaces (easier?) to create methodology, then pass on to jobs…\n\nresolution (LSOA…?)\nfind relation to land use\ngenerate indicators:\n\nrelevant variables\nrelevant scenarios\n\n\n\nIDEAS in random order\n\naccessibility definition??\nIdentify the shortest path between two locations using the OSM transport network:\n\nroutino application for finding a route between two points using OSM\nvalhalla open source routing engine and accompanying libraries for use with OSM\nOpenTripPlanner open-source multimodal trip planning software system\nPlan4better Geo Open Accessibility GOAT tool\nOSMnx Python package\nbuild from OSM and implementing Dijkstra myself (??? @Dustin suggestion)\nR5 - Conveyal:\n\nin R: r5r\nin Python: r5py\n\n\n… check Propensity to Cycle Tool by Robin? [PTC]\ndefine the paths we want to see https://en.wikipedia.org/wiki/Shortest_path_problem#Algorithms\n\nfrom where to where?\nhow to define people’s home… centroids buildings??\n\n\n\n2 ways of seeing accessibility:\n\nbetween “areas” (IE “from this LSOA I can access N schools/jobs/… located in other LSOAS”) calculated using origin/destination matrix for centroids (pop weighted likely) and cumulative sum of opportunities\nfrom the amenity (school/job/hospital…)  generate a N-min “isochrone” (depending on mode) and see how it overlaps with population distribution (at LSOA level? the more detailed the merrier)\n\napparently the distinction is between passive and active accessibility see\n\nNote  When you have available the 2 datasets:\n\nland_use_data - file with columns like population, jobs, school per LSOA (or whichever geography unit)\ntravel_matrix - origin destination matrix with travel cost column (can use time rather than distance, as a cost proxy) per LSOAs (from_id, to_id, travel_time)\n\nyou can run and play around with accessibility, a package in R from Rafa Pereira from ipeagit\n\ncalculate active and passive accessibility levels using multiple accessibility metrics, such as cumulative opportunities (using either a travel cost cutoff or a travel cost interval), minimum travel cost to closest N number of activities, gravitational measures and different floating catchment area methods.\n\n\nor run tracc or pysal/access pr rpy5 on the TTM\n\neven better: from the above (written in R), reference to Python stuff here:\n\n\ntracc: Transport accessibility measures in Python\naccess: Spatial Access for PySAL\n\naceso: a lightweight Python package for measuring spatial accessibility\n\n\n\n… see application and comments directly in accessib.ipynb\n\nIn order to run these functions we need to create/have the following input files:\n\ntransit time data (origin-destination matrix with time) between origins and destinations\nsupply / opportunity:\n\n\njob counts per area can download data at LSOA level (one file per LAD, or full county) from Nomis BRES\n\n\nother type of opportunity (ex: n of groceries, n of schools, etc… extension of greenspace? n of access points?)\n\n\n\n as a proxy for future complicated stuff and to try something for now, let’s generate the number of access points (to greenspaces) per LSOA… what would be the fastest way:\n\nQGIS analysis (sure a function exists for this, but this solution is valid only temporarily, better the next one)\nin Python with Geopandas (or similar stuff) I assume it’s doable\n\nA good explanation of the process is in the r5py documentation\n\nTrials\nDifferent trials for accessibility calculation are accessible as notebook files within the python folder, generally named as accessib_****_.ipynb\n1. OTP\nCalculations for OTP in the script [accessib_OTP.ipynb](./../python/accessib_OTP.ipynb)\nStarted by the tutorial at https://github.com/marcusyoung/otp-tutorial\n\nYoung, M. (2021). OpenTripPlanner - creating and querying your own multi-modal route planner.\n\nExamples of 15-90 minisochrones from (54.96835, -1.60778) generated using OTP for walk/transit mode, at 9am on 01-19-2-23)\n\ntime travel matrix \\(\\rightarrow\\) using OTP OpenTripPlanner with M Young instructions\n\nfind GTFS data\ndownload OSM data\nbuild graph.obj\n\nsupply data (opportunities / land use / …)\nrun tracc for calculating accessibility to jobs… see basic_accessibility_example_TRACC.ipynb\nrun also for green spaces, as in “minimum travel time to reach n destinations” (see above link)\n\nFeedback The above procedure makes it unclear to understand the results PLUS editing the parameters by own requirements (in particular in creating the TTM), so trying to re-do the same using r5py\n2. r5py + tracc\n\ntime travel matrix –&gt; more editable?? shorter to generate, no need of a graph building\n\nfind GTFS data\ndownload OSM data\nbuild graph.obj? network\n\nsupply data (opportunities / land use / …)\ncalculating accessibility to jobs using tracc\nrun also for green spaces, as in “minimum travel time to reach n destinations” using tracc\n\nNote that the accessibility calculation is not supported in the r5py (see here)\n\nAt the time of this writing, only the computation of travel time matrices has been fully implemented. Over time, r5py will be expanded to incorporate other functionalities from R5.\n\nbut you can run this r5 functionality (and everything that Conveyal does, IE “Conveyal Analysis”) using MongoDB locally: github page\n-STILL NEED TO CHECK THIS OPTION-\n\nTO_DO: (once run the above stuff, first trial with jobs accessibilty)  generalise the procedure above in order to create indicators for accessibility:\n\ninput variables format (table fields, etc)\nunderstand how to edit parameters of time travel matrix and generate more of them\nunderstand how travel time matrix work, as atm it seems to only generate transit mode matrix\nscenarios?"
  },
  {
    "objectID": "book/notes.html#december",
    "href": "book/notes.html#december",
    "title": "6  Notes",
    "section": "6.3 December",
    "text": "6.3 December\nHands on code finally?\n\nLet’s start from the indicator of House prices, as it is the only one indicator we have atm that is actual values data, not deterministically defined (see table here).\nAlso greenspace is a possible candidate to start the analysis, for determining a method for accessibility calculation. In this case consider Valhalla.\n\nThe 5 LADs in Tine and Wear (labelled by LAD20NM and LAD20CD), highlighted in gray the major urban areas, bordered in blue the 145 MSOAs\n\nPutting in doubt the use of LUR for what we want to do as we are gonna have aggregated outputs\n\nThis regression-based approach estimates the ambient pollutant concentrations at un-sampled points of interest by considering the relationship between ambient concentrations and several predictor variables selected from the surrounding environment\n\n\nA raster graphic image of the area is generated and intersected with area-level population data to formulate the exposure distribution.\n\nExample of a raster as output of a LUR analysis:\n\nQUESTION: WHY do we want aggregated (MSOAs) outputs? I think it’s because we want to be able to compare among scenarios + (quoting Dani) because SPC data is at MSOAs scale\n\nInputs from call with Federico (14 Dec):\n\njobs posting tracker (ONS ???)\n\nfrom Indeed Hiring Lab (employment website for job listings)\nIn the United Kingdom, they provide:\n\nregional_postings_gb.csv This file contains the % change in seasonally-adjusted postings since February 1, 2020 for total job postings in each region in the UK.\ncity_postings_gb.csv This file contains the % change in seasonally-adjusted postings since February 1, 2020 for total job postings in each city in the UK.\n\n\nBRES (Business Registry and Employment Survey) data\n\nAn employer survey of the number of jobs held, broken down by industry\nestimates at detailed geographical and industrial levels (from country down to LSOA)\n\n\n\nExploratory phase The idea is to plot/visualise/put together a few pieces of information and see how they interact with each other, get ideas on what to do with them.\nSteps:\n\nunderstanding Python VS excel (for house prices data)\nunderstanding Python VS shp (for greenspace data + admin boundaries)\nplot house prices VS greenspace locations\nunderstanding json (valhalla) VS maps/python etc… know nothing\n\n\n6.3.1 House prices indicator\nData processing workflow\n\nSee available datasets for House Price Statistics for Small Areas (HPSSAs) from here: houseprices_available-data_HPSSA.xlsx\nThe tables come in .xls format\nchoose either MSOA or LSOA level:\n\nHPSSA Dataset 2 - Median price paid by MSOA.xls file (September 2022 version); note that this dataset contains also breakdown by house type (detached, semidetached, terraced) and by newly/existing dwellings\nHPSSA Dataset 46 - Median house prices by LSOA.xls file\n\nneed to estrapolate just the tab with the relevant info … \\(\\rightarrow\\) automate this??  “table 1a (all house types)”” = Median price paid by MSOA, England and Wales, year ending Dec 1995 to year ending Mar 2022\nfiltering just the relevant grographies for Tyne and Wear county: \\(\\rightarrow\\) automate this??\n\nMSOAs: list in MSOAs_Tyne-Wear.txt MSOA20CD = MSOA code MSOA20NM = MSOA name\nLSOAs: list in TyneWear_LADs_list.csv filter by Local authority: code LAD20CD = Local authority code name LAD20NM = Local authority name\n\nchoose time … data published quarterly (4 times a year) with all editions back to 1995\n\n\n\n\n6.3.2 Accessibility (… Greenspaces)\n\nremove “private” spaces from the analysis (IE golf clubs)?\ncheck “overlapping” features like pitches/leisure centres/etc within parks\nrouting \\(\\rightarrow\\) Valhalla isochrones\ncensus data\nrelevant layers: public venues (schools, hospitals), green/blue spaces,"
  },
  {
    "objectID": "book/notes.html#agenda---november-29-tue---in-office",
    "href": "book/notes.html#agenda---november-29-tue---in-office",
    "title": "6  Notes",
    "section": "6.4 Agenda - November, 29 [Tue] - In office",
    "text": "6.4 Agenda - November, 29 [Tue] - In office\n\nreview of the project (objective, idea…)\ndiscussing the data\n\nsee table in data page\nsee sources below:\n\naccessibility (how to calculate, sources)\nGHG emission (resolution)\nhouse prices (the least concern?)\n\n\ndiscussing methodology\n\n2 processes to generate indicators:\n\nLUR\n_____________ (accessibility)\n\noutput\n\nname"
  },
  {
    "objectID": "book/notes.html#november",
    "href": "book/notes.html#november",
    "title": "6  Notes",
    "section": "6.5 November",
    "text": "6.5 November\nNeed to fine-tune things after last talk with Dani (end of October).\nInstead of trying to reproduce carbon.place results, we want to produce a Land Use regression model \\(\\rightarrow\\) LUR\nMAJOR UPDATE - We can’t use a regression model for indicators such as accessibility, where the dataset is derived/generated by us and not coming from “actual” measurement (like is the case for GHG emissions or house prices, for instance). In this case we will need to find another procedure to generate a modelisation. We will then have 2 different procedures to generate the indicators, depending on the type of indicator. Within these two types, we aim at generating anyways a comprehensive methodology accross indicators to assess/analyse the different scenarios.A certain scenario can in fact affect different indicators, for example removing a green area to add a working compound can impact positively jobs accessibility, lowering greenspace accessibility.\n\n\n6.5.1 Project name\nA good name for the project? (acronym*)\nwhat does “demonstrator” stand for, why should we stick to that term?\n\nL(O)UD = Land (:egg:) Use Demonstrator :mega:\ndemoland 🏙\n🥚 and 🐥 … 🍳\n\nLAID = LAnd Use Indicators Demonstrator 🪹\nNEST (?) 🪺\n\nLOUSE = Land (o) USe Explanator  🪲 (‘louse’ is singular for lice)\nDODO 🦤 … just because it’s a nice name\n\n\n\n6.5.2 Variables creation\nWe are searching data for:\n\nthe indicators\nthe variables (land use features) to predict the indicators\n\nIn a first phase we are concentrating on 4 of the indicators (the numbering refers to the Intro list here): 1.a/b. Pollution/emissions 1.c. Green space accessibility 2.b. Jobs accessibility 2.c. House prices Below, some notes for each indicator on the data searching process\n\n1.a/b. Pollution/emissions\nIndicator: GHG emissions\nPredictive variables: no. of trips, distance travelled, mode of transport\n\ndata for GHG emissions (total, IE comprehensive of mobility, housing, etc) is available from gov.uk only at LAD level\n[not relevant] estimated Per Capita Consumption-Based Greenhouse Gas Emissions for UK Lower and Middle Layer Super Output Areas, 2016. Data CollectionKilian, Lena and Owen, Anne and Newing, Andy and Ivanova, Diana (2021).UK Data Service. 10.5255/UKDA-SN-854888\n\n\n1.c. Green space accessibility\nIndicator: Green space accessibility\nPredictive variables: number of access points, areal extention of greenspaces, distance from greenspace (access point)\nDefinition:\n\nproportion of an urban population living within a certain distance from a green space boundary\n\n Van Den Bosch et al., 2016\n\nUGS = Urban Green Space\nsee Issue on GitHub for relevant literature on indicators, definitions, methodology to generate UGS maps\naccessibility is not actual data (like emissions for instance) but it’s itself calculated as an index, so what is the meaning in modeling it?\nhow we calculate the distance to the nearest green space… or the entrance gate (tricky)\nEEA Urban Atlas for UGS data (landcover map) data available here  Download per each metro areas here Method described in WHO Europe 2016 report\n\nEuropean Urban Atlas Class 1.4.1 (vector data code 14100):Green Urban Areas (European Commission, 2011) Minimum mapping resolution 0.25 ha, Minimum width: 10 m Included: • Public green areas for predominantly recreational use such as gardens, zoos, parks, castle parks. • Suburban natural areas that have become and are managed as urban parks. • Forests or green areas extending from the surroundings into urban areas are mapped as green urban areas when at least two sides are bordered by urban areas and structures, and traces of recreational use are visible. Not included: • Private gardens within housing areas • Cemeteries • Buildings within parks, such as castles or museums • Patches of natural vegetation or agricultural areas enclosed by built‐up areas without being managed as green urban areas\n\nNote that several green areas in Newcastle fall under the category “Pasture” (see for example Nuns Moor)\n\nLands that are permanently used (at least 5 years) for fodder production. Includes natural or sown herbaceous species, unimproved or lightly improved meadows and grazed or mechanically harvested meadows. Regular agriculture impact influences the natural development of natural herbaceous species composition.\n\nand allotments fall into “Sports and leisure facilities”\n\n\n\nUK SOURCES\n\nOrdnance Survey Open greenspace map with technical notes here  \\(\\rightarrow \\rightarrow\\) open data!  Advantages:\n\nmore accurate\ncovers England, Scotland, Wales\nhas access points!!\nneed for data cleaning, as many sport facilities/playgrounds are drawn with a separate polygon within the big park they are part of (overlapping features in the shp)\nthe file includes cemeteries AND allottments\n\nNatural England Accessible Natural Green Space Standards in Towns and Cities: A Review and Toolkit for their Implementation (ENRR526)\n\nreview and toolkit\n\nPublic Health England report\n\n… data??\n\nFields in trust Green Space Index\n\ninteractive map\nthey launched GSI in 2019\nthey use OS Open Greenspace Map (above) and ArcGIS\nnot available as open data\n\nAHAH Access to Healthy Assets & Hazards (Dani suggested)\n• A multi-dimensional index developed by the CDRC for Great Britain at LSOA level, contains also access to greenspace (passive) - already calculated\n• … different versions:\nVERSION 3 (2022)\nVERSION 2 (2017) \nVERSION 1 (2016)\n• the current version (3) only gives the ‘passive’ greenspace, in terms of NDVI values (from Sentinel)\n\n\n\nScreenshot from the metadata file (Version 3)\n\n\n where the values range between … and … (that’s the index)\nthough, by the description the “passive” greenspace should be a measure of surface (km2):\n\n\n\nScreenshot from the Version 2 Short Technical Report\n\n\n… see “active” and “passive” greenspace definition below\n• the latest pubblication claims to use OSM data: Daras K., Green M. A., Davies A., Barr B., Singleton A. 2019. Open data on health-related neighbourhood features in Great Britain. Scientific data 6 (1), 107. DOI: 10.1038/s41597-019-0114-6\n\nFrom all the available types of ‘green’ spaces in the OSM data, we selected only areas tagged as public accessible with the following area types: cemetery, common, dog park, scrub, fell, forest, garden, greenfield, golf course, grass, grassland, heath, meadow, nature reserve, orchard, park, pitch, recreation ground, village green, vineyard and wood  … The green space indicator has been defined as an area measure of access to green space available to each postcode that intersect with a 900 meters buffer zone\n\nbut, from this 2019 publication, it looks like they are using the OS dataset (and giving both active and passive access):  Daras, K., Green, MA., Davies, A., Singleton, A., Barr, B. 2019. Access to Healthy Assets and Hazards (AHAH) - Updated version 2017. figshare. Short Technical Report. DOI: 10.6084/m9.figshare.8295842.v1\n\nOpen data from OS on Green spaces was used for preparing two variables related to the distance from the nearest green space (active) and the total green space areas available to each postcode in a range of a 900-meter buffer (passive) before creating LSOA level averages\n\nNote on Active and Passive Green Spaces\n\n‘active’ is based on the distance people have to travel to their nearest greenspace access point conducive to physical activity […] The measure is built up from analysis at the postcode level. Distances from each postcode centroid along road networks to the nearest greenspace access point are calculated. Postcodes are matched to LSOAs and an average is taken to get an LSOA score (mean value).\nThe second greenspace indicator labelled as ‘passive’ is based on the proportion of greenspace within a 900 meter buffer (~15 mins walk) from where people live\n\nthe (assumed?) first version though uses OSM (like described above): Green, M. A., Daras, K., Davies, A., Barr, B. & Singleton, A. 2018. Developing an openly accessible multi-dimensional small area index of ‘Access to Healthy Assets and Hazards’ for Great Britain,  but is aware of OS, from the Appendix:\n\nThere were few open source alternatives (or data that could be shared openly within the terms of their licences) containing similar data during the construction of the indicators and our index. Since then, the Ordnance Survey (OS) have released an open resource of locations of green space (OS Green Space Layer; https://getoutside.ordnancesurvey.co.uk/greenspaces/) . We are currently undertaking a full comparison of OSM and OS to explore the strengths and weaknesses of each data source with the aim of improving our green space metrics, however such an evaluation is beyond the scope of this paper.\n\n• They use Routino open source software https://www.routino.org to identify the shortest path between two locations using the OSM transport network\n\nmeasured the network distance between the population-weighted centroid of each postcode in the National Statistics Postcode Lookup (NSPL) and the coordinates of the nearest service (e.g. a population-weighted centroid of postcode for off-license)\n\ncode here\nConclusions:\n\nnot enough METADATA to be able to understand what to use (missing field names)\nnot clear which data source they use in which version (NDVI, OS, OSM?)\nonly passive accessibility available in last version\n\nWRITE TO THE TEAM detailed list of questions\n\n\n\n2.b. Jobs accessibility\nIndicator: accessibility to jobs (measured/calculated)\nPredictive variables: working population, n. of available jobs\n\naccessibility is not actual data (like emissions for instance) but it’s itself calculated as an index, so what is the meaning in modeling it?\nQUANT map - how is it calculated - how to download it … format, etc\nAccessibility Destination Datasets see paragraph below\n\n\n2.c. House prices\nIndicator: house price (avg / zonal effect / dimension / amenities ???)\nPredictive variables: dwellings availability, services, accessibility to PT\n\nwhat scenario on house prices?\ngov.uk data most recent source: House price statistics for small areas in England and Wales: year ending March 2022\n\nDifferences to other house price statistics:\n\nThere are two sets of official statistics for house prices. In addition to these HPSSAs, the Office for National Statistics (ONS) also produces the UK House Price Index (UK HPI). The HPSSAs measure the number of property transactions and the price paid for properties sold in a given period, while the UK HPI measures the changing value of properties in the housing market. You can find out more about the differences and uses of these outputs in our House price statistics for small areas Quality and Methodology Information (QMI).\n\n\navailable at MSOA level:\n\n\n\n\n\n\n\n\n\nDataset No.\nDataset Name\nPeriodicity\n\n\n\n\nHPSSA dataset 1\nNumber of sales of residential properties for middle layer super output areas\nQuarterly\n\n\nHPSSA dataset 2\nMedian price paid for middle layer super output areas\nQuarterly\n\n\nHPSSA dataset 3\nMean house prices for middle layer super output areas\nQuarterly\n\n\nHPSSA dataset 4\nLower quartile house prices for middle layer super output areas\nAnnually\n\n\nHPSSA dataset 5\nTenth percentile house prices for middle layer super output areas\nQuarterly\n\n\n\n\n6.5.2.1 Accessibility data\nUPDATE\nIt seems that from 2014 the same type of data is released as Journeytime Statistics\nwith the list of tables available here\nNEED TO CHECK if they give also number of people, that seems missing in the new data format. For example the variable:\n\n\n\nAll20_PT/walk\nEMPLO049\nUsers within 20 minutes by PT/walk\n\n\n\ndoes not appear in the new tables, though, we have the total population (Empl_pop) AND the percentage of population per each mode of transport + time, for example (for table 0501):\n\n\n\n\n\n\n\n\nField\nAlternate name\nDescription\n\n\n\n\n100EmpPT15pct\nEmp106\n% users within 15 minutes of employment centres with 100 to 499 jobs available by PT/walk\n\n\n100EmpPT30pct\nEmp107\n% users within 30 minutes of employment centres with 100 to 499 jobs available by PT/walk\n\n\n100EmpPT45pct\nEmp108\n% users within 45 minutes of employment centres with 100 to 499 jobs available by PT/walk\n\n\n100EmpPT60pct\nEmp109\n% users within 60 minutes of employment centres with 100 to 499 jobs available by PT/walk\n\n\n\n… from which we could infer the variable above? \n\nSome useful and valuable datasets on accessibility by several means of transport was (apparently?) available from DFT until 2012, in csv format. After some strenuous research managed to find the archive for it here.\nAccessibility Destination Datasets\n\nthey measure accessibility to key services like:\n\nfood stores\neducation\nhealth care\ntown centres\nemployment centres\n\npublished for England at national, regional, LAD and LSOA level.\nhave they been substited by Journey time statistics data?\n\nNote data at LSOA level is also available in gov.uk in xls format\nwhere also 2013 is included!\n\nExcel datasets containing raw destination data for calculating Accessibility statistics. This gives the locations of the different services used within these calculations: Primary schools, Secondary Schools, Further Education, Hospitals, GPs, Town Centres, Employment Centres\n\nthese makes up to 8 tables of accessibility:\n\nscreenshot (from gov.uk webpage) of some of the the available tables\nAccessibility Statistics Guidance 2014\n\n\n\n6.5.3 Theoretical framework\nIn a general (non-linear/linear) regression model we have a given dependent variable which we try to explain by several independent/explanatory variables.  In our case for a LUR model we have, considering for example GHG emissions from mobility or housing:\n\n\n\n\n\n\n\n\nDependent variable\nactual data for GHG emissions\n\n\n\nExplanatory variables\nlanduse variables\n(ex. n. of trips, distance,  mode of transport…)\n\n\n\n\nFormulas here \n\n\n\n6.5.4 Some literature on LUR\nQuestions\n\nLUR is generally used for air quality prediction  …FIND examples of non-air pollutants applications?\nLUR potential predictor variables extraction … relevant for us? (limited number of variables) \\(\\rightarrow\\) ideally we’d like to perform this as well\nLUR output is raster VS vectorial \\(\\rightarrow\\) we’ll need to aggregate the data at some spatial level (MSOAs for instance), introducing some level of approximation\nnot clear how the spatial connotation enters the model, just because we use data defined at (some) geographic level? YES\n\n\n\nhttps://en.wikipedia.org/wiki/Land_use_regression_model\n\nA raster graphic image of the area is generated and intersected with area-level population data to formulate the exposure distribution.\n\nHoek et al., 2008 “A review of land-use regression models to assess spatial variation of outdoor air pollution”  They review 25 land-use regression studies.\n\nApplication of the land-use regression approach for air pollution mapping was introduced in the SAVIAH (Small Area Variations In Air quality and Health) study (Briggs et al., 1997). The technique was initially termed regression mapping\n\n\nLand-use regression combines monitoring of air pollution at typically 20–100 locations, spread over the study area, and development of stochastic models using predictor variables usually obtained through geographic information systems (GIS)\n\n\nmain components of LUR: monitoring data, geographic predictors and model development and validation\n\nBertazzon et al., 2015 “Accounting for spatial effects in land use regression for urban air pollution modeling”\n\n\nLUR models are described by standard regression equations. Over the past decade, land use regression (LUR) modeling has emerged as a preferred method for assessing exposure to spatially heterogeneous pollutants, including NO2 (Health Effects Institute, 2010). Despite their advantages, LUR models rely on spatial data; therefore, they are subject to the spatial effects associated with the properties of these data.\n\n\nMorley and Gulliber, 2018“A land use regression variable generation, modelling and prediction tool for air pollution exposure assessment”\n\nRLUR software to develop LUR model in R; Open-source code available in GitHub; GUI with visualisation tools\n\nMa et al. 2020 “PyLUR: efficient software for land use regression modelling the spatial distribution of air pollutants using GDAL/OGR library in Python”\n\n\nAlthough conceptually quite simple, its successful implementation requires detailed knowledge of the area, expertise in GIS, statistics, and programming skills, which makes this modelling approach relatively inaccessible to novice users.\n\n\nPyLUR out-performs RLUR for modelling in the Bradford and Auckland case studies examined. Furthermore, PyLUR is much more efficient in data processing and it has a capability to handle detailed GIS input data.\n\n\nThe principles of LUR modelling can be summarized in five steps. First, air pollution monitoring and GIS data are collected within the scope of the study area. Secondly, different kinds of potential predictor variables for each site are generated using GIS buffering or other geospatial analysis methods. Thirdly, multiple regression analysis is carried out to develop one regression equation establishing the relationship between the observed air pollutant concentrations and significant predictor variables selected from a pool of all potential predictor variables. Fourthly, model performance is evaluated using holdout or cross-validation. **Finally*, once the model is successfully validated, it can be applied to predict the concentration at un-sampled points of interest or generate an air pollutant concentration map of the whole study area (Morley and Gulliver, 2018).\n\n\nMolter and Lindley, 2021 “Developing land use regression models for environmental science research using the XLUR tool – More than a one-trick pony”\n\n\nA Python toolbox for ArcGIS Pro that enables the development and application of land use regression models\n\n… relevant for literature (it’s developed for ArcGIS, so no use)\n\n\n6.5.5 LUR in Python\n\ncheck out this blog How to Build a Regression Model in Python\nGeographic Data Science with Python, book Chap. 11, Spatial Regression\nPaper PyLUR: Efficient software for land use regression modeling the spatial distribution of air pollutants using GDAL/OGR library in Python  Note the code is available online on ResearchGate here (find “linked data” tab)\n\n\n\n6.5.6 Facts on GHG emissions\nGovernmental report UK local authority and regional greenhouse gas emissions national statistics, 2005 to 2020"
  },
  {
    "objectID": "book/notes.html#october",
    "href": "book/notes.html#october",
    "title": "6  Notes",
    "section": "6.6 October",
    "text": "6.6 October\nInitial trials\nWorking on a. mobility based emissions \\(\\rightarrow\\) IE transport data\n&\nBuilding base scenario\nRoadmap:\n\nTry to dl/open/visualise SPC data \\(\\rightarrow\\) Note: study case Metropolitan county of Tyne and Wear\nInvestigate potentially relevant data from UK Data Services data \\(\\rightarrow\\) any other relevant dataset?\nHow to connect/integrate TUS to SPC dataset – why?\nvisualise the integrated dataset \\(\\rightarrow\\) base scenario\nthink “future scenarios”\n\n The 5 LADs in the Metropolitan County of Tyne and Wear, highlighted are also the 4 major town areas\n\nKeep in mind when you’ll “generalise” the procedure\nIE pass from the study case to a national case\n\nhow to select MSOAs for Origin and Destination from the dataset (take off abroad destinations as well)\n\n\n\n\n6.6.1 Defining relations\nWe can see the Indicators \\(I_{i}\\), with \\(i = \\{1, ..., n\\}\\), as a function of different land uses \\(l_{j}\\) where \\(j = \\{1, ..., k\\}\\) and \\(I_{i}\\) can be expressed as generic mathematical relation (that we express with the symbol “\\(\\star\\)”“) between several \\(l_{j}\\) weighted by \\(k\\) generic weights/parameters \\(w_{j}\\), as follows:\n\\[I_{i}(l) = w_1^il_{1} \\star  w_2^il_{2} \\dots \\star w_k^il_{k}\\]\nDifferent indicators can depend on same landuse variables, and per each indicator the scenarios can be expressed by acting on the paramters \\(w_{j}\\).\nAs an example, using the definition above the indicator Mobility based emissions (a.) could be expressed for commuting trips per mode of transport and per each MSOA as:\n\n\\[\n\\begin{align}\n[\\textit{emissions/msoa/year}] & =  w_1[trips/day] \\star\\\\\n& \\star  w_2[distance/trip] \\star\\\\\n& \\star  w_3[n\\_individuals] \\star\\\\\n& \\star  w_4[\\textit{n of commuting days/year}] \\star \\\\\n& \\star w_5[\\textit{CO}_2 \\textit{k}]\n\\end{align}\n\\]\nwhere:\n\n\\([trips/day]\\) is a fixed value (average per person from DFT data … *)\n\\([distance/trip]\\) is calculated as shortest distance between the MSOAs’ centroids\n\\([n\\_individuals]\\) is given by the UK Census 2011 commuting flow\n\\([\\textit{n of commuting days/year}]\\) is a fixed value (average per person from DFT data … *)\n\\([CO_2 k]\\) is the carbon dioxide equivalent emission factor per mode of transport\n\n\n\n6.6.2 Working questions:\n\nwhich year to take into consideration for the analysis? (Census, conversion factors, surveys…)\nwhen I calculate the distance home-place of work, how do I consider the intraMSOA trips (IE within the same MSOA), as I am only taking into account the centroids of the MSOAs?\nhow we count total number of people (age, people working)\ndo we give the results per capita (then see point above) or total per MSOA\nhow many hours for the working from home people (not counting for now, as not using transport)\n\n\n\n\n6.6.3 Approximations\n\nonly OD within MSOAs of this county\nconsidering as distance only the pop weighted centroids for each MSOA\nLondon has specific values for CO2 conversion factors (ex for cabs, buses, underground) but only using general values (correct later if needed for London case)\n…\n\n\nCommuting is really just one part of the total travels by car per per person per year:\n source: national travel survey\n… it sounds then reductive to consider only commuting trips for emissions!\n\n\n\n6.6.4 Python code\nNote: first trials in R, as I am more confident with the language, and trying to reproduce the code from carbun calculator for prep_travel_to_work.R,\nwill then translate to Python\n\nRemember to add in READ_ME a walkthrough.\nUsing env.yml \\(\\dots\\) use conda or poetry?\n\n\n\n6.6.5 Preliminary results\nOrigin Destination flows of individuals for commuting to work (2011 UK Census data), the Metropolitan County of Tyne and Wear, separated by mode of transport \nPreliminary results for GHG emissions from cars, total per MSOA per year in Tyne and Wear"
  },
  {
    "objectID": "book/notes.html#september",
    "href": "book/notes.html#september",
    "title": "6  Notes",
    "section": "6.7 September",
    "text": "6.7 September\nExploratory phase\nThe idea is to focus on a few indicators (one or two) in order generate a methodology to apply to different variables (IE all the rest of indicators)\nConcentrating on Indicators for Net-zero a and b:\n\n\nmobility based emissions … integration with SPC \\(/\\) AB-street?\n\n\nhousing based emissions\n\n\n\n6.7.1 Data availability\n\n\n(aggregated at LAD level) UK GHG emissions national statistics, estimates per Local Authority Data for 2005 - 2020 \\(\\rightarrow\\) this datum is a sum of emissions from 2005-2020\n\n\n\n6.7.1.1 a. mobility based emissions\n\nwe can get from gov.uk CO2e (carbon dioxide equivalent) emissions factors for several occupations and travel modes, including transport, freight, etc link;&lt;br&gt; We get the emission factors per mode of transport from the tab “Business travel-land” and “homeworking” (for working from home individuals)\nOD flows from UK census … only to work and home? check source \\(\\rightarrow\\) YES \\(\\Downarrow\\)\nget travel to school flow data from Propensity to Cycle Tool PCT?\nLocation of usual residence and place of work by method of travel to work (…origin destination flows) table WU03UK](https://wicid.ukdataservice.ac.uk/cider/wicid/downloads.php) at LAD level\nNote: can download all the tables for “Location of usual residence and place of work by method of travel to work” at different levels (LAD to LSOA, though the lattest is not Open data) here https://wicid.ukdataservice.ac.uk/cider/about/data_int.php?type=2 &lt;br&gt; safeguarded data!! git sttuslet’s find MSOA data: &lt;br&gt; nomisweb\n\n\nWU03BEW(with ‘outside UK’ detailed) safeguarded\nWU03EW (with ‘outside UK’ collapsed) open\n\n\n\n\n6.7.1.2 b. housing based emissions\nSub-national electricity and gas consumption data, overview link\n\nelectricity consumption:\n\n\none table per year (20105 to 2020) of total consumption at region and LAD levels link\none table per year (2010 to 2020) of total consumption at LSOA or MSOA level link\nelectricity and gas consumption (kWh) at postcode level (England, Scotland, Wales), one table per year, 2013,2015-2020 Experimental\nelectricity consumption data for 2005 to 2020 as a single table, with years stacked one above the other stacked data\n\n\nanalogous links for gas consumption\n\n\n\n\n6.7.1.3 People behavior\nPeople behavior and time use from The UK Time Diary Study 2014 - 2015 - UK Time Use Survey carried by NatCen and available from UK Data Service\nwhat we can get from this source: time people spend in different activities, mode of transport (yes?)\n\n\n\n6.7.2 Did someone already do this?\ncarbon.place\nProject from Malcom Morgan (Univ of Leeds) …\n\nThe PBCC estimates the average carbon footprint per person for each LSOA in England\n\nNOTE:\ncool project, GitHub code available but not reproducible (?)\nopened issue here https://github.com/creds2/CarbonCalculator/issues/6"
  },
  {
    "objectID": "book/notes.html#open-general-questions",
    "href": "book/notes.html#open-general-questions",
    "title": "6  Notes",
    "section": "6.8 Open general questions",
    "text": "6.8 Open general questions\nAnd plausible answer\n\nimportance of localised emissions and need for air-flow modelling … not relevant for GHG\nwhat is the Research Question \\(\\rightarrow\\) What are the “best” values in the parametrisation of the land use variables? Could we train the computer to predict these values?\nResolution LSOAS/ MSOAS ? feedback from Dani regarding SS: SS gives conversion to LSOAs \\(\\Downarrow\\)\nhow to generate scenarios - ideas: change parameters’ range/values, run ABstreet simulations…?\nintegrate with SPC - need to integrate more Census variables? feasible?\nUse SPC (individuals’ level data) as we aggregate at MSOA level… what variables are meaningful when aggregated?\ncan we actually build a methodology, given the different range of indicators and data?\nhow turn on/off scenarios in the visualisation- different layers in the map\ntype of output - map, web … MapBox MapLibre GL, Leaflet, …\n\n\n\n6.8.1 Output / map generation\nNote: could generate a website via MapLibre, checked the options\n\n\n\nSuggested data sources:\n\nSPC - Synthetic Population Catalyst website\nSS - Spatial Signature website\nboth\n\nIndicators are grouped into 4 general themes to reflect the goals of the National Land Development Programme (NLDP) in relation to the environment, economy, infrastructure and society:\n\n\n6.8.2 Net-zero\n\nMobility-based emissions (e.g., trips x distance x mode —&gt; CO2)\nHousing-based emissions (e.g., CO2 emissions from building energy consumption)\nGreen space (area, access, fragmentation)\n\n\n\n6.8.3 Economy\n\nDistance to nearest job (by industry)\nJob volume within accessible distance\nAvailability of (appropriate) housing stock\n\n\n\n6.8.4 Infrastructure stock (vacancy)\n\nBrownfield areas\nVacant lots and properties\nTransport networks and connectivity\n\n\n\n6.8.5 Society and Health\n\nPhysical health\nMental health\nHealthy living and lifestyles\n\nThe page Data Sources will be updated once the sources are determined and actually used in the analysis."
  },
  {
    "objectID": "book/notes.html#scenarios-building",
    "href": "book/notes.html#scenarios-building",
    "title": "6  Notes",
    "section": "6.9 Scenarios building",
    "text": "6.9 Scenarios building\nThe existing baseline will be modified through indicative scenarios of relevance, underlining the ability of the modelling system to illustrate the trade-offs between competing objectives through evaluation of the key indicators.\nScenarios will be generated as:\n\nType I what if? scenarios: the consequence of a range of concrete actions (e.g. construct a new tram line);\nType II what could be? scenarios: to explore more aspirational targets (e.g. reduce emissions by 10%)\n\nAlthough the final set of scenarios will be co-produced with the partners, an illustration is provided below:\n\nInfrastructure scenarios:\n\nS1A – Add new jobs through significant inward investment (e.g., construction of Government offices/public buildings) - Type I\nS1B – reorient to new mobility targets through modal shift (increased pedestrian and public transport) - Type II\nS1C – Increase access to jobs (e.g., more roads/transit/cycle lanes) - Type I or II\n\nEnvironmental scenarios:\n\nS2A – create a new urban park or nature reserve - Type I\nS2B – design a future urban space with net zero emissions - Type II\n\nHousing stock:\n\nRetrofit existing neighbourhoods to increase density/decrease sprawl - Type I or II"
  },
  {
    "objectID": "book/notes.html#data-sources",
    "href": "book/notes.html#data-sources",
    "title": "6  Notes",
    "section": "6.10 Data Sources",
    "text": "6.10 Data Sources\n\n6.10.1 Comprehensive tables\nTable 1 -  4 indicators and data we use to quantify them:\n\n\n\nactual (X) VS estimated (O)\nIndicator\nDefinition\nData source\nLink\n\n\n\n\nO\nMobility emissions\n\ngov uk data estimates at LAD level (only)\n\n\n\nX\nAir pollution hazard\n\nLocal Authority + European data\n\n\n\nO\nGreenspace accessibility\n\nOS greenspace layer\nlink\n\n\nO\nJobs accessibility\nn. jobs within certain time interval by transport mode\nto be created\n\n\n\nX\nHouse prices\nMedian house prices for administrative geographies\nONS\n2022 data\n\n\n\n\nTable 2 -  Indicators (columns) and plausible explanatory/building variables (rows - landuse variables)\n\n\n\n\n\n\n\n\n\n\n\nVariables\nMobility emissions\nGreenspace acc\nJobs acc\nHouse prices\nSources\n\n\n\n\nn. of trips\nX\n\n\n\nOD data (Nomis)\n\n\ndistance\nX\n\n\n\nOD data (Nomis)\n\n\nmode transport\nX\n\n\n\nOD data (Nomis)\n\n\ndistance to access point\n\nX\n\nX\nfrom OS layer\n\n\nareal extension of gs\n\nX\n\nX\nfrom OS layer\n\n\nn. available jobs\n\n\nX\nX\nNomis BRES\n\n\ndistance to employment center\n\n\nX\ngenerated\n\n\n\naccess to public transport\n\n\nX\ngenerated\n\n\n\n\n… distinguish landuse as explanatory variable from landuse as indicator…\n\n\n6.10.2 List of sources\nData hunting still in progress, to be filled up when data use is certain. See notes to follow the development.\n\n6.10.2.1 Time Use Survey\nTime Use Survey for UK is available from UK Data Service website at http://dx.doi.org/10.5255/UKDA-SN-8128-1"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#variables-definition-and-data-import",
    "href": "code/01_baseline/01_accessibility.html#variables-definition-and-data-import",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.1 0. Variables definition and data import",
    "text": "A.1 0. Variables definition and data import\n\n# definitions\nimport sys\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport datetime as dt\nimport tracc\nfrom r5py import TransportNetwork, TravelTimeMatrixComputer, TransitMode, LegMode\nfrom datetime import datetime, date, timedelta\nimport matplotlib.pyplot as plt\nfrom itertools import product  # needed for generating all combinations of O-D pairs\n\nsys.argv.append([\"--max-memory\", \"8G\"])\n\n\ndata_folder = \"/Users/azanchetta/OneDrive - The Alan Turing Institute/demoland_data\"\n\n\n# regional level files: (require previous editing)\noas_centroids_file = (\n    f\"{data_folder}/processed/OA_centroids_TyneWear.gpkg\"  # used for population origin\n)\noas_file = f\"{data_folder}/processed/authorities/OA_TyneWear.gpkg\"  # needed for visualisation purposes\nregion_lads_file = f\"{data_folder}/processed/authorities/LADs_tynewear.shp\"  # needed in order to filter greenspace data within the regional boundaries\nworkingplacezones_centroids_file = f\"{data_folder}/processed/authorities/WPZ_centroids_tynewear.gpkg\"  # needed for destinations centroids coordinates\n# greenspace_sites_file = f\"{data_folder}/processed/accessibility/greenspace-sites_tynewear.gpkg\" # needed for calcualting opportunities at greenspaces (area)\n# greenspace_entrances_file = f\"{data_folder}/processed/accessibility/accessTOgs_tynewear.gpkg\" # needed for destinations centroids coordinates\ngreenspace_file = (\n    f\"{data_folder}/processed/accessibility/greenspace_tynewear_edited.gpkg\"\n)\njobs_file = f\"{data_folder}/processed/accessibility/wpz_tynewear_occupation_edited.csv\"\n\n# national level files\n# greenspace_file = f\"{data_folder}/raw/accessibility/OS Open Greenspace (GPKG) GB/data/opgrsp_gb.gpkg\"\nosm_data_file = f\"{data_folder}/raw/accessibility/tyne-and-wear-latest.osm.pbf\"\ngtfs_data_file = f\"{data_folder}/raw/accessibility/itm_north_east_gtfs.zip\"\n\n\n# import\n\n# origins (IE output areas, OAs)\noas_centroids = gpd.read_file(oas_centroids_file, layer=\"OA_centroids_TyneWear\")\noas_centroids[\"id\"] = oas_centroids[\n    \"OA11CD\"\n]  # Origin dataset must contain an 'id' column for r5py\noas_centroids.head()\n\n# destination data\n# green space sites' entrances\ngs_entrances = gpd.read_file(greenspace_file, layer=\"access_points\")\n\ngs_entrances.head()  # Destination dataset already contains an 'id' column\n# WPZ centroids\nwpz_centroids = gpd.read_file(\n    workingplacezones_centroids_file, layer=\"WPZ_centroids_tynewear\"\n)\nwpz_centroids.head()\nwpz_centroids[\"id\"] = wpz_centroids[\n    \"wz11cd\"\n]  # Destination dataset must contain an 'id' column for r5py\n\ngs_sites = gpd.read_file(greenspace_file, layer=\"sites\")\n\n# network data\n# uploaded in the sequent operation\n\n# opportunities / land use data\njobs_per_wpz_df = pd.read_csv(\n    jobs_file\n)  # working place zones, population (as a proxy for n of jobs)\n# note: opportunities column is called \"pop\"\n\n\ngs_entrances.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nA.1.1 CRS conversion\n\n# Converting the original files' crs to GWS84, which is compatible with GTFS and OSM data\noas_centroids_wgs84 = oas_centroids.to_crs(\"epsg:4326\")\ngs_entrances = gs_entrances.to_crs(\"epsg:4326\")\n# gs_sites = gs_sites.to_crs(\"epsg:4326\") # let's leave the layer in epsg:27700, as we need the prj for calculating the areas\nwpz_centroids = wpz_centroids.to_crs(\"epsg:4326\")\n\n\n\nA.1.2 Origins and destinations\n\noas_centroids.head()\n\n\n\n\n\n\n\n\nOBJECTID\nOA11CD\nGlobalID\ngeometry\nid\n\n\n\n\n0\n126926\nE00041377\nc03c9813-26f3-41f9-85e5-d4cdf3742ca0\nPOINT (425583.000 562952.000)\nE00041377\n\n\n1\n126927\nE00041435\n16e6607e-0b59-4f6f-8ec6-06a7396a70a5\nPOINT (427216.699 555732.531)\nE00041435\n\n\n2\n126928\nE00041745\n4b5fa995-b251-4ee7-9a97-aef0a2598fe3\nPOINT (427897.004 559557.605)\nE00041745\n\n\n3\n126929\nE00041432\n6e660884-3917-4e46-a693-bad0821318cb\nPOINT (427856.367 555759.595)\nE00041432\n\n\n4\n126930\nE00041742\n0bfb7f06-a910-4fa2-8db1-e79d319ba232\nPOINT (427932.556 559770.754)\nE00041742\n\n\n\n\n\n\n\n\nwpz_centroids.head()\n\n\n\n\n\n\n\n\nOBJECTID\nwz11cd\nGlobalID\ngeometry\nid\n\n\n\n\n0\n2\nE33000251\n{AF2BD35C-B624-4E2D-9C78-F26DF4FCABCE}\nPOINT (-1.41992 54.91839)\nE33000251\n\n\n1\n3\nE33000799\n{8CB93749-3349-462C-93C7-B6E321CC765C}\nPOINT (-1.61606 54.97382)\nE33000799\n\n\n2\n4\nE33000257\n{03204BF6-50A6-4AD1-855F-C7BBE6D8137B}\nPOINT (-1.53272 54.90010)\nE33000257\n\n\n3\n5\nE33000079\n{53333BDF-9792-4370-94AB-BE7853FA2ACA}\nPOINT (-1.62268 55.01104)\nE33000079\n\n\n4\n8\nE33000174\n{35114C58-FAA7-4E83-9724-ACED166052D5}\nPOINT (-1.50942 55.02269)\nE33000174\n\n\n\n\n\n\n\n\ngs_entrances.head()\n\n\n\n\n\n\n\n\nid\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n4\nid1BED7A99-E143-48C3-90CE-B7227E820454\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55988 55.03359)\n\n\n\n\n\n\n\n\n# origins:\n#   OAs\n# destinations:\n#   gs: entrances + OAs centroids\n#   jobs: wpz centroids + OAs centroids\n# total destination: OAs centroids + wpz centroids + gs entrances\n\norigins = oas_centroids_wgs84\n\n# destinations common fields: 'id', 'geometry'\n# simply concatenate the dataframes...\n# need to keep the info on greenspace site's name to link with the entrances later on\n\ndestinations = pd.concat(\n    [\n        oas_centroids_wgs84[[\"id\", \"geometry\"]],\n        wpz_centroids[[\"id\", \"geometry\"]],\n        gs_entrances[[\"id\", \"geometry\", \"refToGreenspaceSite\"]],\n    ]\n).reset_index(drop=True)\n\n\n\nA.1.3 Opportunities\n\n# jobs: n of employees per WPZ\n# greenspace: area of site\n\n\n# add column with opportunity ... one for all?"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#travel-time-matrix-computation",
    "href": "code/01_baseline/01_accessibility.html#travel-time-matrix-computation",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.2 1. Travel time matrix computation",
    "text": "A.2 1. Travel time matrix computation\n\nA.2.1 Generate the transport network\nCompute the network starting from OSM and GTFS data\n\n# load in transport network\ntransport_network = TransportNetwork(osm_data_file, [gtfs_data_file])\n\n\n\nA.2.2 Create an empty matrix that contains all origins and destinations to be used later on\nThis table will be filled up once we calculate the ttm\n\n# # # only for testing purposes:\n# k = 1000\n# # selecting first n rows of dataframe for origins and destinations\n# # origins = oas_centroids.loc[:k, :]\n# # destinations = wpz_centroids.loc[:n, :]\n# # selecting random rows, so to make sure we have both wpz AND gs_entrances in the selection of destinations\n# origins = origins.sample(n=k)\n# destinations = destinations.sample(n=k)\n\n\n# generate dataframe with all from_id and all to_id pairs\n# (empty for now, to be filled up later on)\nprod = product(origins[\"id\"].unique(), destinations[\"id\"].unique())\nempty_ttm = pd.DataFrame(prod)\nempty_ttm.columns = [\"from_id\", \"to_id\"]\nempty_ttm.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\n\n\n\n\n0\nE00041377\nE00041377\n\n\n1\nE00041377\nE00041435\n\n\n2\nE00041377\nE00041745\n\n\n3\nE00041377\nE00041432\n\n\n4\nE00041377\nE00041742\n\n\n\n\n\n\n\n\n\nA.2.3 Travel time matrix\nThe following piece of code is split in 2: - first part is definition of variables that will be inputted as parameters in the ttm computation - second part is the loop to generate ttm for several transport modes\n\n# defining variables\ndate_time = \"2023,01,19,9,30\"  # CHOOSE BEST DATE/TIME\n# max_time = dt.timedelta(seconds=900) # SET TO 15 MIN\nwalking_speed = 4.8\ncycling_speed = 16\n# dataframe to match legmode and transitmode objects (to be inputted in the ttm computer):\nmodes_lut = pd.DataFrame(\n    [\n        [\"transit\", TransitMode.TRANSIT, LegMode.WALK],\n        [\"car\", \"\", LegMode.CAR],\n        [\"bicycle\", \"\", LegMode.BICYCLE],\n        [\"walk\", \"\", LegMode.WALK],\n    ],\n    columns=(\"Mode\", \"Transit_mode\", \"Leg_mode\"),\n)\n\n\n# function to generate custom list of transit+transport mode for the parameter transport_modes in TravelTimeMatrixComputer\ndef list_making(s, z):\n    return [s] + [z]\n\n\nttm_complete = empty_ttm\n\n# loop to compute a ttm for all the modes and generate one single ttm table in output\nfor row in modes_lut.itertuples():\n    start_time = dt.datetime.now()\n    mode = row.Mode\n    transit_mode = row.Transit_mode\n    leg_mode = row.Leg_mode\n    transport_mode = list_making(\n        transit_mode, leg_mode\n    )  # creating list of objects for transport_modes parameter\n\n    print(\n        \"The current mode is:\",\n        mode,\n        \", transit is:\",\n        transit_mode,\n        \", transport var is:\",\n        transport_mode,\n    )\n    ttm_computer = TravelTimeMatrixComputer(\n        transport_network,\n        origins=origins,\n        destinations=destinations,\n        departure=dt.datetime.strptime(date_time, \"%Y,%m,%d,%H,%M\"),\n        # max_time = max_time,\n        speed_walking=walking_speed,\n        speed_cycling=cycling_speed,\n        transport_modes=transport_mode,\n    )\n\n    ttm = ttm_computer.compute_travel_times()\n    ttm = ttm.rename(\n        columns={\"travel_time\": f\"time_{mode}\"}\n    )  # renaming 'travel_time' column (automatically generated) to 'time_{mode of transport}'\n    ttm.isna().sum()  # checking for empty values, to see if the ttm actually calculated something\n    #  merging the empty table generated before (with all possible origins and destinations) with the ttm, per each mode adding a travel time column\n    ttm_complete = ttm_complete.merge(\n        ttm, how=\"outer\", left_on=[\"from_id\", \"to_id\"], right_on=[\"from_id\", \"to_id\"]\n    )\n\n    print(\"finished calculating ttm for mode\", mode)\n    end_time = datetime.now()\n    print(\"Duration for\", mode, \": {}\".format(end_time - start_time))\n\nThe current mode is: transit , transit is: TransitMode.TRANSIT , transport var is: [&lt;TransitMode.TRANSIT: &lt;java object 'com.conveyal.r5.api.util.TransitModes'&gt;&gt;, &lt;LegMode.WALK: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\n\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/r5py/r5/regional_task.py:224: RuntimeWarning: Departure time 2023-01-19 09:30:00 is outside of the time range covered by currently loaded GTFS data sets.\n  warnings.warn(\n\n\nfinished calculating ttm for mode transit\nDuration for transit : 0:07:34.098400\nThe current mode is: car , transit is:  , transport var is: ['', &lt;LegMode.CAR: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\nfinished calculating ttm for mode car\nDuration for car : 0:21:01.904903\nThe current mode is: bicycle , transit is:  , transport var is: ['', &lt;LegMode.BICYCLE: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\nfinished calculating ttm for mode bicycle\nDuration for bicycle : 0:16:26.882727\nThe current mode is: walk , transit is:  , transport var is: ['', &lt;LegMode.WALK: &lt;java object 'com.conveyal.r5.api.util.LegMode'&gt;&gt;]\nfinished calculating ttm for mode walk\nDuration for walk : 0:03:23.352848\n\n\n\nttm_complete.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\n\n\n\n\n\n\n\n\n# # saving ttm in output\n# ttm_complete.to_parquet(f\"{data_folder}/processed/accessibility/ttm_complete.parquet\")"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#accessibility-calculation",
    "href": "code/01_baseline/01_accessibility.html#accessibility-calculation",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.3 2. Accessibility calculation",
    "text": "A.3 2. Accessibility calculation\nUsing jamaps/tracc package"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#accessibility-to-jobs",
    "href": "code/01_baseline/01_accessibility.html#accessibility-to-jobs",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.4 Accessibility to jobs",
    "text": "A.4 Accessibility to jobs\n\nttm_jobs = ttm_complete.copy(\n    deep=True\n)  # saving a copy of the matrix (the following operations will add columns to it, but we want to keep the original one also)\n\n# generate tracc cost object\nttm_jobs_tracc = tracc.costs(ttm_jobs)\n\nmodes_list = [\"transit\", \"car\", \"bicycle\", \"walk\"]\n\n# empty dataframe to be filled up in the next for loop\nacc_pot_jobs = origins[[\"id\"]]\n\nfor m in modes_list:\n    # generate variable names to be used in the tracc function below\n    cost_name = \"time_\" + m\n    travel_costs_ids = [\"from_id\", \"to_id\"]\n    supplyID = \"wpz11cd\"\n    impedence_param = 15  # value for impedence function, to be changed as needed\n    impedence_param_string = str(impedence_param)\n    cost_output = \"cum_\" + impedence_param_string + \"_\" + m\n    acc_column_name = \"pot_cum_acc_\" + impedence_param_string + \"_\" + m\n    opportunity = \"pop\"\n    # Computing impedance function based on a 15 minute travel time threshold.\n    ttm_jobs_tracc.impedence_calc(\n        cost_column=cost_name,\n        impedence_func=\"cumulative\",\n        impedence_func_params=impedence_param,  # to calculate n of jobs in n min threshold\n        output_col_name=cost_output,\n        prune_output=False,\n    )\n\n    # Setting up the accessibility object. This includes joining the destination data to the travel time data\n    acc_jobs = tracc.accessibility(\n        travelcosts_df=ttm_jobs_tracc.data,\n        supply_df=jobs_per_wpz_df,\n        travelcosts_ids=travel_costs_ids,\n        supply_ids=supplyID,\n    )\n    acc_jobs.data.head()\n\n    # Measuring potential accessibility to jobs, using a 15 minute cumulative impedance function\n    # acc_pot_jobs = acc_jobs.potential(\n    #         opportunity = \"pop\",\n    #         impedence = cost_output,\n    #         output_col_name= \"pot_acc_\" + cost_output\n    #         )\n    # the above function generate overwrite the column at every loop\n    # so we reproduce the same function (from tracc documentation) per each mode:\n    acc_jobs.data[acc_column_name] = (\n        acc_jobs.data[opportunity] * acc_jobs.data[cost_output]\n    )\n    group_sum_bymode_acc = acc_jobs.data.groupby(acc_jobs.data[travel_costs_ids[0]])[\n        [acc_column_name]\n    ].sum()\n    acc_pot_jobs = acc_pot_jobs.merge(\n        group_sum_bymode_acc, how=\"outer\", left_on=\"id\", right_on=\"from_id\"\n    )\n\n\nacc_jobs.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\ncum_15_transit\ncum_15_car\ncum_15_bicycle\ncum_15_walk\nwpz11cd\npop\npot_cum_acc_15_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\n1\n1\n1\n1\nNaN\nNaN\nNaN\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\n0\n1\n0\n0\nNaN\nNaN\nNaN\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\n0\n1\n0\n0\nNaN\nNaN\nNaN\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\n0\n0\n0\n0\nNaN\nNaN\nNaN\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\n0\n1\n0\n0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nacc_pot_jobs.head()\n\n\n\n\n\n\n\n\nid\npot_cum_acc_15_transit\npot_cum_acc_15_car\npot_cum_acc_15_bicycle\npot_cum_acc_15_walk\n\n\n\n\n0\nE00041377\n32139.0\n318618.0\n107910.0\n11817.0\n\n\n1\nE00041435\n4839.0\n164613.0\n8649.0\n3814.0\n\n\n2\nE00041745\n865.0\n208472.0\n9597.0\n865.0\n\n\n3\nE00041432\n2086.0\n67634.0\n11214.0\n2086.0\n\n\n4\nE00041742\n865.0\n170808.0\n10267.0\n615.0\n\n\n\n\n\n\n\n\n# saving output to external file"
  },
  {
    "objectID": "code/01_baseline/01_accessibility.html#accessibility-to-greenspace",
    "href": "code/01_baseline/01_accessibility.html#accessibility-to-greenspace",
    "title": "Appendix A — Measuring accessibility",
    "section": "A.5 Accessibility to greenspace",
    "text": "A.5 Accessibility to greenspace\n\n# edit greenspace layers\n# change the 'id' column name, as it's the same in both layers and generates issues later on\ngs_entrances.columns  # ['id', 'accessType', 'refToGreenspaceSite', 'geometry']\ngs_entrances.rename(columns={\"id\": \"id_entrance\"}, inplace=True)\ngs_sites.columns  # ['id', 'function', 'geometry']\ngs_sites.rename(columns={\"id\": \"id_site\"}, inplace=True)\n\n# calculates sites' area:\ngs_sites[\"area_m2\"] = gs_sites[\"geometry\"].area\n\n\ngs_entrances.head()\ngs_sites.head()\ngs_sites.explore(column=\"area_m2\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngs_entrances.head()\n\n\n\n\n\n\n\n\nid_entrance\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n4\nid1BED7A99-E143-48C3-90CE-B7227E820454\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55988 55.03359)\n\n\n\n\n\n\n\n\ngs_sites.head()\n\n\n\n\n\n\n\n\nid_site\nfunction\ngeometry\narea_m2\n\n\n\n\n0\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nPlay Space\nMULTIPOLYGON (((440767.260 552692.600, 440777....\n1560.70565\n\n\n1\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440761.280 552942.510, 440753....\n1966.87245\n\n\n2\nidE56DE6D8-C9BD-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440968.500 552987.220, 440983....\n8135.95125\n\n\n3\nidE56DE6D8-8F64-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((439560.480 560021.050, 439578....\n2868.07275\n\n\n4\nidE56DE6D8-8F65-13A9-E053-AAEFA00A0D0E\nCemetery\nMULTIPOLYGON (((439858.700 560473.170, 439817....\n153540.65735\n\n\n\n\n\n\n\n\n# associate park area to entrances\ngs_entrances_with_parkarea = pd.merge(\n    gs_entrances[[\"id_entrance\", \"refToGreenspaceSite\"]],\n    gs_sites[[\"id_site\", \"function\", \"area_m2\"]],\n    left_on=\"refToGreenspaceSite\",\n    right_on=\"id_site\",\n    how=\"right\",\n)\n\n\ngs_entrances_with_parkarea.head()\n\n\n\n\n\n\n\n\nid_entrance\nrefToGreenspaceSite\nid_site\nfunction\narea_m2\n\n\n\n\n0\nidCAC0A6B3-0FDB-446D-8E36-700AF2CC1256\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nPlay Space\n1560.70565\n\n\n1\nidCE043231-4C15-4265-A370-2D70261224C7\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n2\nid379B3089-2FF5-4BD3-B695-9B7DA915FB02\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n3\nid7AE0057A-2F40-43F3-970E-A517BBC99804\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n4\nidE5DAAC5C-29B5-49A0-BB46-62C78F46BA6C\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\n1966.87245\n\n\n\n\n\n\n\n\nttm_complete.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\n\n\n\n\n\n\n\n\nttm_greenspace = (\n    ttm_complete.copy()\n)  # saving a copy of the matrix (the following operations will add columns to it, but we want to keep the original one also)\n\n\nttm_gs_with_area = pd.merge(\n    ttm_greenspace,\n    gs_entrances_with_parkarea[[\"id_entrance\", \"refToGreenspaceSite\", \"area_m2\"]],\n    left_on=\"to_id\",\n    right_on=\"id_entrance\",\n    how=\"left\",\n)\n# generate tracc cost object\nttm_gs_tracc = tracc.costs(ttm_gs_with_area)\n\nmodes_list = [\"transit\", \"car\", \"bicycle\", \"walk\"]\n\n# empty dataframes to be filled up in the next for loop\nacc_pot_gs = origins[[\"id\"]]\ngs_acc = []\n\nfor m in modes_list:\n    # generate variable names to be used in the tracc function below\n    cost_name = \"time_\" + m\n    travel_costs_ids = [\"from_id\", \"to_id\"]\n    impedence_param = 15  # value for impedence function, to be changed as needed\n    impedence_param_string = str(impedence_param)\n    # name of the column\n    cost_output = (\n        \"cum_\" + impedence_param_string + \"_\" + m\n    )  # naming depends on impedence function threshold\n    area_column_name = \"area_\" + impedence_param_string + \"_\" + m\n    acc_column_name = (\n        \"pot_cum_acc_\" + impedence_param_string + \"_\" + m\n    )  # naming depends on impedence function threshold\n    opportunity = \"pop\"\n    # Computing impedence function based on a 15 minute travel time threshold.\n    ttm_gs_tracc.impedence_calc(\n        cost_column=cost_name,\n        impedence_func=\"cumulative\",\n        impedence_func_params=impedence_param,  # to calculate opportunities in X min threshold\n        output_col_name=cost_output,\n        prune_output=False,\n    )\n    ttm_gs_df = ttm_gs_tracc.data\n    print(ttm_gs_df.columns)\n    # Setting up the accessibility object. This includes joining the destination data to the travel time data\n    # this needed to be done differently for greenspace, as opportunity is sites's area cumulative sum\n    # A. Filtering only rows with time travel within the threshold\n    print(\"cost output is\", cost_output)\n    print(\"area column name is\", area_column_name)\n    # tracc_15min = ttm_gs_tracc.data[ttm_gs_tracc.data.loc[:,cost_output]==1] # this doesn't work because of the different lenghts of the columns generated per mode\n    ttm_gs_tracc.data[area_column_name] = (\n        ttm_gs_tracc.data[\"area_m2\"] * ttm_gs_tracc.data[cost_output]\n    )\n    ttm_gs_df = ttm_gs_tracc.data\n\n    # B. Filter entrances (only one per park)\n    oneaccess_perpark = ttm_gs_df.sort_values(cost_name).drop_duplicates(\n        [\"from_id\", \"refToGreenspaceSite\"]\n    )\n    oneaccess_perpark.head()\n    # C. Assign metric as sum[parks' area]\n    # generate df with one row per OA centroid ('from_id') and sum of sites' areas - per each mode\n    gs_metric_per_mode = oneaccess_perpark.groupby([\"from_id\"])[\n        area_column_name\n    ].sum()  # .reset_index()\n    gs_acc.append(gs_metric_per_mode)\ngs_acc = pd.concat(gs_acc, axis=1)\n\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit'],\n      dtype='object')\ncost output is cum_15_transit\narea column name is area_15_transit\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit', 'area_15_transit', 'cum_15_car'],\n      dtype='object')\ncost output is cum_15_car\narea column name is area_15_car\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit', 'area_15_transit', 'cum_15_car', 'area_15_car',\n       'cum_15_bicycle'],\n      dtype='object')\ncost output is cum_15_bicycle\narea column name is area_15_bicycle\nIndex(['from_id', 'to_id', 'time_transit', 'time_car', 'time_bicycle',\n       'time_walk', 'id_entrance', 'refToGreenspaceSite', 'area_m2',\n       'cum_15_transit', 'area_15_transit', 'cum_15_car', 'area_15_car',\n       'cum_15_bicycle', 'area_15_bicycle', 'cum_15_walk'],\n      dtype='object')\ncost output is cum_15_walk\narea column name is area_15_walk\n\n\n\nttm_gs_tracc.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntime_transit\ntime_car\ntime_bicycle\ntime_walk\nid_entrance\nrefToGreenspaceSite\narea_m2\ncum_15_transit\narea_15_transit\ncum_15_car\narea_15_car\ncum_15_bicycle\narea_15_bicycle\ncum_15_walk\narea_15_walk\n\n\n\n\n0\nE00041377\nE00041377\n0.0\n0\n0.0\n0.0\nNaN\nNaN\nNaN\n1\nNaN\n1\nNaN\n1\nNaN\n1\nNaN\n\n\n1\nE00041377\nE00041435\n31.0\n12\n37.0\n99.0\nNaN\nNaN\nNaN\n0\nNaN\n1\nNaN\n0\nNaN\n0\nNaN\n\n\n2\nE00041377\nE00041745\n32.0\n11\n25.0\n63.0\nNaN\nNaN\nNaN\n0\nNaN\n1\nNaN\n0\nNaN\n0\nNaN\n\n\n3\nE00041377\nE00041432\n43.0\n16\n39.0\n107.0\nNaN\nNaN\nNaN\n0\nNaN\n0\nNaN\n0\nNaN\n0\nNaN\n\n\n4\nE00041377\nE00041742\n33.0\n12\n24.0\n60.0\nNaN\nNaN\nNaN\n0\nNaN\n1\nNaN\n0\nNaN\n0\nNaN\n\n\n\n\n\n\n\nExporting results in output\n\nacc_pot_jobs.to_csv(\n    f\"{data_folder}/processed/accessibility/acc_jobs_allmodes_15min_tynewear.csv\"\n)\ngs_acc.to_csv(\n    f\"{data_folder}/processed/accessibility/acc_greenspace_allmodes_15min_tynewear.csv\"\n)\n\nPlotting results\n\noas_boundaries = gpd.read_file(oas_file, layer=\"OA_TyneWear\")\noas_boundaries_wgs84 = oas_boundaries.to_crs(\"epsg:4326\")\n\n\noas_boundaries_jobs = oas_boundaries_wgs84.merge(\n    acc_pot_jobs, left_on=\"geo_code\", right_on=\"id\", how=\"right\"\n)\n\n\noas_boundaries_jobs.plot(\n    \"pot_cum_acc_15_transit\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_jobs.explore(\n    column=\"pot_cum_acc_15_car\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_jobs.explore(\n    column=\"pot_cum_acc_15_transit\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_metric = oas_boundaries_wgs84.merge(\n    gs_acc, left_on=\"geo_code\", right_on=\"from_id\", how=\"right\"\n)\n\n\noas_boundaries_metric.explore(\n    column=\"area_15_transit\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\noas_boundaries_metric.explore(\n    column=\"area_15_car\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#variables-definition-and-data-import",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#variables-definition-and-data-import",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.1 Variables definition and data import",
    "text": "B.1 Variables definition and data import\n\nimport sys\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport datetime as dt\nimport tracc\nfrom r5py import TransportNetwork, TravelTimeMatrixComputer, TransitMode, LegMode\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\n\nsys.argv.append([\"--max-memory\", \"8G\"])\n\n\ndata_folder = \"/Users/azanchetta/OneDrive - The Alan Turing Institute/demoland_data\"\n\n\n# Ordnance Survey (OS) Greenspace data\n# (using Tyne and Wear data for now, generated previously in QGis)\n# greenspace_file = f\"{data_folder}/raw/accessibility/OS Open Greenspace (GPKG) GB/data/opgrsp_gb.gpkg\"\ngreenspace_sites_file = (\n    f\"{data_folder}/processed/accessibility/greenspace-sites_tynewear.gpkg\"\n)\naccesspoints_file = f\"{data_folder}/processed/accessibility/accessTOgs_tynewear.gpkg\"\n\n# OSM landuse data (Tyne and Wear data)\nosm_landuse_file = f\"{data_folder}/raw/OSM_tynewear/tyne-and-wear-latest-free.shp/gis_osm_landuse_a_free_1.shp\"\n\n# if needed for mapping purposes (?)\nregion_lads_file = f\"{data_folder}/processed/authorities/LADs_tynewear.shp\"  # needed in order to filter greenspace data within the regional boundaries\n\n\nData import\n\n\ngreenspace_sites = gpd.read_file(\n    greenspace_sites_file, layer=\"grenspace-sites_tynewear\"\n)\n\ngreenspace_sites.head()\n\naccesspoints = gpd.read_file(accesspoints_file, layer=\"pointsaccessTOgs_tynewear\")\naccesspoints.head()\n\n# for mapping:\nregion_lads = gpd.read_file(region_lads_file)\nregion_lads.head()\n\n\n\n\n\n\n\n\nOBJECTID\nLAD20CD\nLAD20NM\nLAD20NMW\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Are\nShape__Len\nlabel\ngeometry\n\n\n\n\n0\n265\nE08000021\nNewcastle upon Tyne\nNone\n422287\n569662\n-1.65297\n55.02101\n1.134619e+08\n65202.925674\nNewcastle upon Tyne\\nE08000021\nPOLYGON ((422592.399 576160.095, 422618.297 57...\n\n\n1\n266\nE08000022\nNorth Tyneside\nNone\n431471\n570602\n-1.50923\n55.02896\n8.231373e+07\n65337.781081\nNorth Tyneside\\nE08000022\nMULTIPOLYGON (((435203.599 575441.701, 435209....\n\n\n2\n267\nE08000023\nSouth Tyneside\nNone\n435514\n564057\n-1.44679\n54.96988\n6.442842e+07\n51370.230506\nSouth Tyneside\\nE08000023\nPOLYGON ((438030.200 568413.300, 438021.350 56...\n\n\n3\n268\nE08000024\nSunderland\nNone\n436470\n551524\n-1.43344\n54.85719\n1.374412e+08\n99737.411804\nSunderland\\nE08000024\nMULTIPOLYGON (((441259.800 557854.000, 441252....\n\n\n4\n281\nE08000037\nGateshead\nNone\n420168\n559658\n-1.68680\n54.93120\n1.423691e+08\n90476.826397\nGateshead\\nE08000037\nPOLYGON ((415042.801 565083.296, 415104.202 56...\n\n\n\n\n\n\n\n\n# selecting from greenspace layer only the relevant layers\ngreenspace_sites = greenspace_sites[[\"id\", \"function\", \"geometry\"]]\ngreenspace_sites.head()\n\n\n\n\n\n\n\n\nid\nfunction\ngeometry\n\n\n\n\n0\nidE56DE6D8-CB5D-13A9-E053-AAEFA00A0D0E\nAllotments Or Community Growing Spaces\nMULTIPOLYGON (((440933.560 552196.810, 440929....\n\n\n1\nidE56DE6D8-CB1F-13A9-E053-AAEFA00A0D0E\nAllotments Or Community Growing Spaces\nMULTIPOLYGON (((441101.270 552520.550, 441095....\n\n\n2\nidE56DE6BE-8E16-13A9-E053-AAEFA00A0D0E\nPlaying Field\nMULTIPOLYGON (((440984.530 552520.890, 440985....\n\n\n3\nidE56DE6C8-8E48-13A9-E053-AAEFA00A0D0E\nPlay Space\nMULTIPOLYGON (((440985.360 552520.750, 441000....\n\n\n4\nidE56DE6C6-4CFA-13A9-E053-AAEFA00A0D0E\nOther Sports Facility\nMULTIPOLYGON (((440986.650 552565.050, 440989....\n\n\n\n\n\n\n\n\ngreenspace_sites.explore(column=\"function\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# viewing 'function' categories to work on later\ngreenspace_sites[\"function\"].unique()\n\narray(['Allotments Or Community Growing Spaces', 'Playing Field',\n       'Play Space', 'Other Sports Facility', 'Public Park Or Garden',\n       'Religious Grounds', 'Bowling Green', 'Cemetery', 'Tennis Court',\n       'Golf Course'], dtype=object)\n\n\n\nWorking on OS data before understanding how to integrate OSM data\nIE points 1, 2, 5, 6 from Workflow above"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#filter-categories-from-os-data",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#filter-categories-from-os-data",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.2 1. filter categories from OS data",
    "text": "B.2 1. filter categories from OS data\nwe consider only open and not paid-for spaces\n\ngreenspace_sites_select = greenspace_sites.query(\n    \"function!='Allotments Or Community Growing Spaces' & function!='Golf Course' & function!='Bowling Green'\"\n)\n\n\ngreenspace_sites_select.explore(column=\"function\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#clean-os-data",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#clean-os-data",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.3 2. Clean OS data",
    "text": "B.3 2. Clean OS data\nA. dissolve the overlapping polygons\n\npublicpark = greenspace_sites_select.query(\"function=='Public Park Or Garden'\")\nplayingfield = greenspace_sites_select.query(\"function=='Playing Field'\")\nothersport = greenspace_sites_select.query(\"function=='Other Sports Facility'\")\ntherest = greenspace_sites_select.query(\n    \"function!='Playing Field' & function!='Public Park Or Garden' & function!='Other Sports Facility'\"\n)\n\n\n# find 'therest' not included in the upper categories\n# we use sjoin to performe a spatial filter of 'therest' polygons contained in upper categories\njoin11 = gpd.sjoin(therest, othersport, op=\"within\", how=\"inner\")\njoin12 = gpd.sjoin(therest, playingfield, op=\"within\", how=\"inner\")\njoin13 = gpd.sjoin(therest, publicpark, op=\"within\", how=\"inner\")\n# join13.columns = join13.columns.str.replace('index_', 'join3')\njoin13.head()\n# generate list of the IDs of 'therest' contained in upper categories, in order to eliminate the corresponding polygons from the layer\nlist_for_diff11 = join11[\"id_left\"].drop_duplicates().to_list()\n# diff1 =therest.query('id'.isin('list_for_diff1')) # doesn't work\n# diff1 = therest[therest['id'].isin('list_for_diff1')] # doesn't work\ndiff11 = therest[\n    ~therest.id.isin(list_for_diff11)\n]  # 1st difference layer # note the negation character ~ to take the polygons NOT included\n\nlist_for_diff12 = join12[\"id_left\"].drop_duplicates().to_list()\ndiff12 = diff11[~diff11.id.isin(list_for_diff12)]  # 2nd difference layer\n\nlist_for_diff13 = join13[\"id_left\"].drop_duplicates().to_list()\ndiff13 = diff12[\n    ~diff12.id.isin(list_for_diff13)\n]  # 3rd difference layer, this is for 'therest' categories\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# we repeat the same operation for subsequent categories:\n# find 'othersport' not included in the upper categories\njoin21 = gpd.sjoin(othersport, playingfield, op=\"within\", how=\"inner\")\njoin22 = gpd.sjoin(othersport, publicpark, op=\"within\", how=\"inner\")\n\nlist_for_diff21 = join21[\"id_left\"].drop_duplicates().to_list()\ndiff21 = othersport[~othersport.id.isin(list_for_diff21)]\n\nlist_for_diff22 = join22[\"id_left\"].drop_duplicates().to_list()\ndiff22 = diff21[~diff21.id.isin(list_for_diff22)]  # 'othersport' difference\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# find 'playing fields' not included in the upper categories (and viceversa?)\njoin31 = gpd.sjoin(playingfield, publicpark, op=\"within\", how=\"inner\")\njoin32 = gpd.sjoin(\n    publicpark, playingfield, op=\"within\", how=\"inner\"\n)  ## check it is not empty ... it is empty, we do not use this join\n\nlist_for_diff31 = join31[\"id_left\"].drop_duplicates().to_list()\ndiff31 = playingfield[\n    ~playingfield.id.isin(list_for_diff31)\n]  # 'playingfield' difference\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n# put together all the differences layers: (and should bring out to the desired output)\ntogether1 = pd.concat([diff13, diff22]).pipe(\n    gpd.GeoDataFrame\n)  # 'therest' + 'othersport' differences\ntogether1.head()\ntogether2 = pd.concat([together1, diff31]).pipe(\n    gpd.GeoDataFrame\n)  # last gdf + 'playingfield' difference\ntogether_again = pd.concat([together2, publicpark]).pipe(\n    gpd.GeoDataFrame\n)  # last gdf + all the public parks)\n\ntogether_again.head()\n\n\n\n\n\n\n\n\nid\nfunction\ngeometry\n\n\n\n\n5\nidE56DE6D8-CA9A-13A9-E053-AAEFA00A0D0E\nPlay Space\nMULTIPOLYGON (((440767.260 552692.600, 440777....\n\n\n7\nidE56DE6D8-C9DE-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440761.280 552942.510, 440753....\n\n\n8\nidE56DE6D8-C9BD-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((440968.500 552987.220, 440983....\n\n\n16\nidE56DE6D8-8F64-13A9-E053-AAEFA00A0D0E\nReligious Grounds\nMULTIPOLYGON (((439560.480 560021.050, 439578....\n\n\n17\nidE56DE6D8-8F65-13A9-E053-AAEFA00A0D0E\nCemetery\nMULTIPOLYGON (((439858.700 560473.170, 439817....\n\n\n\n\n\n\n\n\ntogether_again = together_again.set_crs(\"epsg:27700\")\n\n\n# re-setting the crs as we lost in the last operation\ntogether_again.explore(\n    column=\"function\"\n)  # any reason why it doesn't show the whole gdf\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nB. Filter the entrances (only the ones on the edges)\nWe can, as a first measure, filter out from the entrances the ones to greenspaces that are not in the data set anymore.\nThe entrances that are left should only be located along the edges of the relevant polygons.\n\nlist_gs_id = together_again[\"id\"].to_list()\n\n\naccesspoints.head()\n\n\n\n\n\n\n\n\nid\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid548D0EAC-E6BE-4DFA-B90C-DB631A75309B\nPedestrian\nidE56DE841-2BC6-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55981 55.03343)\n\n\n4\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n\n\n\n\n\n\naccesspoints_edge = accesspoints[accesspoints.refToGreenspaceSite.isin(list_gs_id)]\n\n\naccesspoints_edge.explore()\n\n\n# checking crs\ntogether_again.crs  # epsg:27700\naccesspoints_edge.crs  # epsg:4326 (wgs84)\naccesspoints_edge_OSGB36 = accesspoints_edge.to_crs(\"epsg:27700\")\n\n\n# saving the two edited into geopackage. first the sites\ncomplete_gpkg_filename = (\n    f\"{data_folder}/processed/accessibility/greenspace_tynewear_edited.gpkg\"\n)\ntogether_again.to_file(complete_gpkg_filename, layer=\"sites\")\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  pd.Int64Index,\n\n\n\n# adding the entrances to the same gpkg\naccesspoints_edge_OSGB36.to_file(complete_gpkg_filename, layer=\"access_points\")\n\n/usr/local/anaconda3/envs/demoland_r5/lib/python3.9/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  pd.Int64Index,"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#get-areas-size",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#get-areas-size",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.4 5 . get areas size",
    "text": "B.4 5 . get areas size\n\ntogether_again[\"area_m2\"] = together_again[\"geometry\"].area\n\n\ntogether_again.explore(column=\"area_m2\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10)"
  },
  {
    "objectID": "code/01_baseline/01b_greeenspace_datacleaning.html#run-accessibility-analysis",
    "href": "code/01_baseline/01b_greeenspace_datacleaning.html#run-accessibility-analysis",
    "title": "Appendix B — Green space data cleaning",
    "section": "B.5 6. run accessibility analysis",
    "text": "B.5 6. run accessibility analysis\n\n# define variables path\noas_centroids_file = (\n    f\"{data_folder}/processed/OA_centroids_TyneWear.gpkg\"  # used for population origin\n)\noas_file = f\"{data_folder}/processed/authorities/OA_TyneWear.gpkg\"  # needed for visualisation purposes\nosm_data_file = f\"{data_folder}/raw/accessibility/tyne-and-wear-latest.osm.pbf\"\ngtfs_data_file = f\"{data_folder}/raw/accessibility/itm_north_east_gtfs.zip\"\n\n\n# importing needed data\n# origins (IE output areas, OAs)\norigin_centroids = gpd.read_file(oas_centroids_file, layer=\"OA_centroids_TyneWear\")\norigin_centroids[\"id\"] = origin_centroids[\n    \"OA11CD\"\n]  # Origin dataset must contain an 'id' column\norigin_centroids.head()\noas_boundaries = gpd.read_file(oas_file, layer=\"OA_TyneWear\")\n\n\noas_boundaries.crs\n\n&lt;Projected CRS: EPSG:27700&gt;\nName: OSGB 1936 / British National Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: UK - Britain and UKCS 49°46'N to 61°01'N, 7°33'W to 3°33'E\n- bounds: (-9.2, 49.75, 2.88, 61.14)\nCoordinate Operation:\n- name: British National Grid\n- method: Transverse Mercator\nDatum: OSGB 1936\n- Ellipsoid: Airy 1830\n- Prime Meridian: Greenwich\n\n\n\n# checking crs are compatible with OSM and GTFS data\norigin_centroids.crs\naccesspoints_edge.crs  # wgs84\norigin_centroids_wgs84 = origin_centroids.to_crs(\"epsg:4326\")\norigin_centroids_wgs84.crs\noas_boundaries_wgs84 = oas_boundaries.to_crs(\"epsg:4326\")\n\n\n# generate transport network\ntransport_network = TransportNetwork(osm_data_file, [gtfs_data_file])\n\n\n# generate ttm (by foot only for now)\nttm_walking_OAtoGS = TravelTimeMatrixComputer(\n    transport_network,\n    origins=origin_centroids_wgs84,\n    destinations=accesspoints_edge,\n    max_time=dt.timedelta(seconds=900),  # restricting travel to 15min\n    speed_walking=4.8,\n    transport_modes=[LegMode.WALK],\n)\nttm_walking_OAtoGS = ttm_walking_OAtoGS.compute_travel_times()\nttm_walking_OAtoGS.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\n\n\n\n\n0\nE00041377\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nNaN\n\n\n1\nE00041377\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nNaN\n\n\n2\nE00041377\nid0E14522B-427F-47C1-B043-BC3847ABE673\nNaN\n\n\n3\nE00041377\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nNaN\n\n\n4\nE00041377\nid1BED7A99-E143-48C3-90CE-B7227E820454\nNaN\n\n\n\n\n\n\n\n\n# accessibility calculation\ndf_tracc = tracc.costs(ttm_walking_OAtoGS)\ndf_tracc.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\n\n\n\n\n0\nE00041377\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nNaN\n\n\n1\nE00041377\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nNaN\n\n\n2\nE00041377\nid0E14522B-427F-47C1-B043-BC3847ABE673\nNaN\n\n\n3\nE00041377\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nNaN\n\n\n4\nE00041377\nid1BED7A99-E143-48C3-90CE-B7227E820454\nNaN\n\n\n\n\n\n\n\n\nmax_time = ttm_walking_OAtoGS.groupby(\"from_id\")[\"travel_time\"].max()\nmax_time.max()\n\n24.0\n\n\n\n# Computing impedance function based on a 15 minute travel time threshold.\ndf_tracc.impedence_calc(\n    cost_column=\"travel_time\",\n    impedence_func=\"cumulative\",\n    impedence_func_params=15,  # minutes cap\n    output_col_name=\"cum_15\",\n    prune_output=False,\n)\ndf_tracc.data.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\n\n\n\n\n0\nE00041377\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nNaN\n0\n\n\n1\nE00041377\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nNaN\n0\n\n\n2\nE00041377\nid0E14522B-427F-47C1-B043-BC3847ABE673\nNaN\n0\n\n\n3\nE00041377\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nNaN\n0\n\n\n4\nE00041377\nid1BED7A99-E143-48C3-90CE-B7227E820454\nNaN\n0\n\n\n\n\n\n\n\n\ndf_tracc_15min = df_tracc.data[df_tracc.data.loc[:, \"cum_15\"] == 1]\ndf_tracc_15min.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\n\n\n\n\n1360\nE00041377\nidA22A0B91-5C86-435E-9903-1504CF541EBE\n10.0\n1\n\n\n1361\nE00041377\nid46A79A0F-CC21-453B-8D1E-718602AC52B2\n11.0\n1\n\n\n1362\nE00041377\nid5CCFD37A-C45C-4812-B1A0-90EDA6A28407\n10.0\n1\n\n\n1363\nE00041377\nidF12A35EF-34D2-4233-8DA7-0C804753070F\n11.0\n1\n\n\n1364\nE00041377\nid09D9EBD9-CDAC-443F-9381-E95FF2C69732\n12.0\n1\n\n\n\n\n\n\n\nA. filter entrances (one per park)\n\naccesspoints.head()\n\n\n\n\n\n\n\n\nid\naccessType\nrefToGreenspaceSite\ngeometry\n\n\n\n\n0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55733 55.03322)\n\n\n1\nid951F323D-8E88-4A5B-B9A4-37E0D69DD870\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56184 55.03333)\n\n\n2\nid0E14522B-427F-47C1-B043-BC3847ABE673\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.56197 55.03340)\n\n\n3\nid548D0EAC-E6BE-4DFA-B90C-DB631A75309B\nPedestrian\nidE56DE841-2BC6-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55981 55.03343)\n\n\n4\nid0FECA8F4-6053-4147-A11D-62B01EC6C135\nPedestrian\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\nPOINT (-1.55989 55.03344)\n\n\n\n\n\n\n\n\n# associate park ID to entrances\naccesspoints_withPark = pd.merge(\n    df_tracc_15min,\n    accesspoints[[\"id\", \"refToGreenspaceSite\"]],\n    left_on=\"to_id\",\n    right_on=\"id\",\n    how=\"right\",\n)\n\n\naccesspoints_withPark.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\nid\nrefToGreenspaceSite\n\n\n\n\n0\nE00043145\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n2.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n1\nE00043142\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n7.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n2\nE00043143\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n4.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n3\nE00043140\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n10.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n4\nE00043141\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\n3.0\n1.0\nidD93E3AB6-BDCE-483D-B3CF-4242FA90A0B7\nidE56DE6C0-48DC-13A9-E053-AAEFA00A0D0E\n\n\n\n\n\n\n\n\nlen(accesspoints_withPark.from_id.unique())  # 3726\nlen(accesspoints_withPark.refToGreenspaceSite.unique())  # 1171\n# not relevant, but just to get an idea of the dimensions\n\n1853\n\n\n\noneaccess_perpark = accesspoints_withPark.sort_values(\"travel_time\").drop_duplicates(\n    [\"from_id\", \"refToGreenspaceSite\"]\n)\noneaccess_perpark.head()\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\ncum_15\nid\nrefToGreenspaceSite\n\n\n\n\n50823\nE00170026\nid02365D7F-C250-4444-89F1-DBE551510281\n0.0\n1.0\nid02365D7F-C250-4444-89F1-DBE551510281\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n\n\n36894\nE00042112\nid2232DADE-7F34-44A9-A68A-A251D6F4401D\n0.0\n1.0\nid2232DADE-7F34-44A9-A68A-A251D6F4401D\nidE56DE840-A14F-13A9-E053-AAEFA00A0D0E\n\n\n66716\nE00041433\nidA0076802-9488-4034-A4FF-70873196025F\n0.0\n1.0\nidA0076802-9488-4034-A4FF-70873196025F\nidE56DE6C6-B4B3-13A9-E053-AAEFA00A0D0E\n\n\n83464\nE00043860\nidD07325AC-C4A1-4684-AF75-2FB2BB03EE98\n0.0\n1.0\nidD07325AC-C4A1-4684-AF75-2FB2BB03EE98\nidE56DE849-1E1A-13A9-E053-AAEFA00A0D0E\n\n\n90749\nE00043604\nidEFA07B19-6B76-419D-91B9-47D61F7C6709\n0.0\n1.0\nidEFA07B19-6B76-419D-91B9-47D61F7C6709\nidE56DE6D7-1FB7-13A9-E053-AAEFA00A0D0E\n\n\n\n\n\n\n\n\n# oneaccess_perpark = accesspoints_withPark.groupby('from_id')['refToGreenspaceSite'].agg(['unique'])\n# oneaccess_perpark.head()\n\nB. assign metric as sum[park size]\n\noneaccess_perpark.describe()\noneaccess_perpark.columns\n\nIndex(['from_id', 'to_id', 'travel_time', 'cum_15', 'id',\n       'refToGreenspaceSite'],\n      dtype='object')\n\n\n\n# generate df with area per site\noneaccess_perpark_witharea = pd.merge(\n    oneaccess_perpark[[\"from_id\", \"refToGreenspaceSite\"]],\n    together_again[[\"id\", \"area_m2\"]],\n    left_on=\"refToGreenspaceSite\",\n    right_on=\"id\",\n)\noneaccess_perpark_witharea.head()\n\n\n\n\n\n\n\n\nfrom_id\nrefToGreenspaceSite\nid\narea_m2\n\n\n\n\n0\nE00170026\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n1\nE00041915\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n2\nE00041910\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n3\nE00041941\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n4\nE00170027\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\nidE56DE6C6-2F90-13A9-E053-AAEFA00A0D0E\n74.2263\n\n\n\n\n\n\n\n\n# generate df with one row pr OA and sum of sites' areas\nOAs_metric = (\n    oneaccess_perpark_witharea.groupby([\"from_id\"])[\"area_m2\"].sum().reset_index()\n)\n\n\nOAs_metric.head()\n\n\n\n\n\n\n\n\nfrom_id\narea_m2\n\n\n\n\n0\nE00041363\n526165.89095\n\n\n1\nE00041364\n521458.94910\n\n\n2\nE00041366\n311400.41615\n\n\n3\nE00041367\n505729.33200\n\n\n4\nE00041368\n325226.25195\n\n\n\n\n\n\n\n\nOAs_metric.to_csv(\"../output/acc_walking_gs15_OAtoGS_tynewear.csv\")\n\n\n# plotting results\noas_boundaries_metric = oas_boundaries_wgs84.merge(\n    OAs_metric, left_on=\"geo_code\", right_on=\"from_id\", how=\"right\"\n)\n\n\noas_boundaries_metric.explore(\n    column=\"area_m2\", cmap=\"plasma\", scheme=\"NaturalBreaks\", k=10\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "code/01_baseline/02_air_quality.html#exploration-of-air-quality-data-from-newcastle-urban-observatory",
    "href": "code/01_baseline/02_air_quality.html#exploration-of-air-quality-data-from-newcastle-urban-observatory",
    "title": "Appendix C — Air quality",
    "section": "C.1 Exploration of air quality data from Newcastle urban observatory",
    "text": "C.1 Exploration of air quality data from Newcastle urban observatory\nDownloaded from https://archive.dev.urbanobservatory.ac.uk/agg/900/ aggregated for 15 minutes blocks. We can also get coarser aggregation (hour, 6 hours, day) or even raw minute data.\n\nimport pandas as pd\nimport geopandas as gpd\nimport xarray as xr\nimport contextily as ctx\n\nSpecify paths to existing files, some of which need to be downloaded manually (see Notes).\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\n\nlsoa_list_file = (\n    f\"{data_folder}/processed/tynewear_lsoas_list.csv\"  # list of LSOAs within AoI\n)\naoi_file = f\"{data_folder}/processed/authorities/\"\n\n# national level files\nLSOA_boundaries_file = f\"{data_folder}/raw/LSOA_(Dec_2011)_Boundaries_Super_Generalised_Clipped_(BSC)_EW_V3.geojson\"  # administrative boundaries from gov.uk OS\n\nLoad the data for CO for December to see the structure\n\nco_dec22 = pd.read_csv(f\"{data_folder}/raw/air_quality/2022-12-CO-900.0.csv.zip\")\n\nGenerate actual geometry\n\nco_dec22 = gpd.GeoDataFrame(\n    co_dec22, geometry=gpd.GeoSeries.from_wkt(co_dec22[\"Location (WKT)\"], crs=4326)\n).to_crs(27700)\n\n\nco_dec22.head()\n\n\n\n\n\n\n\n\nSensor Name\nVariable\nUnits\nTimestamp\nMean Value\nMin Value\nMax Value\nMedian Value\nStandard Dev\nCount Records\nLocation (WKT)\nGround Height Above Sea Level\nSensor Height Above Ground\nBroker Name\nThird Party\nSensor Centroid Longitude\nSensor Centroid Latitude\nRaw ID\ngeometry\n\n\n\n\n0\nPER_AIRMON_MESH1972150\nCO\nugm -3\n2022-12-01 00:00:00\n417.865689\n406.879185\n491.198130\n410.974850\n20.274449\n15.0\nPOINT (-1.617121 54.974193)\n47.290001\n2.0\naq_mesh_api\nFalse\n-1.617121\n54.974193\n79208\nPOINT (424607.499 564463.931)\n\n\n1\nPER_AIRMON_MESH1907150\nCO\nugm -3\n2022-12-01 00:00:00\n630.707678\n614.071515\n678.104495\n627.741670\n17.546116\n15.0\nPOINT (-1.603448 54.965679)\n31.170000\n2.0\naq_mesh_api\nFalse\n-1.603448\n54.965679\n79150\nPOINT (425488.062 563521.339)\n\n\n2\nPER_AIRMON_MESH1968150\nCO\nugm -3\n2022-12-01 00:00:00\n1.257286\n1.196525\n1.766735\n1.217135\n0.138612\n15.0\nPOINT (-1.614848 54.97149)\n42.040001\n2.0\naq_mesh_api\nFalse\n-1.614848\n54.971490\n79204\nPOINT (424754.639 564163.933)\n\n\n3\nPER_AIRMON_MESH1969150\nCO\nugm -3\n2022-12-01 00:00:00\n343.280313\n337.130365\n364.159235\n342.825595\n6.512988\n15.0\nPOINT (-1.593537 55.039309)\n64.440002\n2.0\naq_mesh_api\nFalse\n-1.593537\n55.039309\n79205\nPOINT (426075.104 571718.592)\n\n\n4\nPER_AIRMON_MONITOR1157100\nCO\nugm -3\n2022-12-01 00:00:00\n0.179765\n0.179765\n0.179765\n0.179765\n0.000000\n1.0\nPOINT (-1.623043 54.970093)\n45.270000\n2.0\naq_mesh_api\nFalse\n-1.623043\n54.970093\n79565\nPOINT (424230.858 564005.621)\n\n\n\n\n\n\n\nWe have classic summary stats for the CO values, each per 15 minute slot. We can get those for every month back to roughly 2016.\nThe same can be done for the other metrics:\n\nCO (this one)\nNO\nNO2\nNOx\nO3\nPM 4\nPM 1\nPM 10\nPM 2.5\nParticle count\n\nGiven we are dealing with rather static scenarios, the target indicator in the modelling process needs to be an aggregation of some sort per a longer period of time (a year?).\nFurther, we may not want to model each metric individually, so there may be a scope for a creation of an index of air quality combining all these into a single metric that will be then predicted. Or we can predict them all…\n\naoi = gpd.read_file(aoi_file)\n\n\nm = co_dec22.loc[~co_dec22[\"Location (WKT)\"].duplicated()].explore(marker_type=\"marker\")\naoi.boundary.explore(m=m, color=\"black\")\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe main issue with the dataset is its coverage. As shown above, it is very limited and even if we wanted to interpolate the data we wouldn’t be able to do so south of the city. UO has other sensors there but not those for air quality."
  },
  {
    "objectID": "code/01_baseline/02_air_quality.html#eea-based-index",
    "href": "code/01_baseline/02_air_quality.html#eea-based-index",
    "title": "Appendix C — Air quality",
    "section": "C.2 EEA-based index",
    "text": "C.2 EEA-based index\nEEA specifies the relationship between concentration of different pollutants to get an index of hazard for human health (here). We can use their multipliers to create a single index based on the UK AIR data. From EEA:\n\nThe bands are based on the relative risks associated to short-term exposure to PM2.5, O3 and NO2, as defined by the World Health Organization in its report on the Health Risks of Air Pollution in Europe project (HRAPIE project report).\nThe relative risk of exposure to PM2.5 is taken as basis for driving the index, specifically the increase in the risk of mortality per 10 µg/m3 increase in the daily mean concentration of PM2.5.\nAssuming linearity across the relative risks functions for O3 and NO2, we calculate the concentrations of these pollutants that pose an equivalent relative risk to a 10 µg/m3 increase in the daily mean of PM2.5.\nFor PM10 concentrations, a constant ratio between PM10 and PM2.5 of 1:2 is assumed, in line with the World Health Organization´s air quality guidelines for Europe.\nFor SO2, the bands reflect the limit values set under the EU Air Quality Directive.\n\nThe relationship between \\(PM_{2.5}\\) : \\(PM_{10}\\) : \\(NO_{2}\\) : \\(O_{3}\\) : \\(SO_{2}\\) is then equal to 1 : 2 : 4 : 5 : 10. The combined index can then be computed as \\[Q_{air} = \\frac{PM_{2.5}}{1} + \\frac{PM_{10}}{2} + \\frac{NO_{2}}{4} + \\frac{O_{3}}{5} + \\frac{SO_{2}}{10}\\]\nExcept for the \\(O_{3}\\), UK AIR reports all as a concentration in \\(\\mu g \\cdot m^{-3}\\). \\(O_{3}\\) is reported as a number of days above a threshold of 120 \\(\\mu g \\cdot m^{-3}\\) and cannot be used in this formula but even EEA omits data when unavailable so we shall be able to create the index based on 4 remaning measurements.\nIt shall also be condsidered that UK AIR is not representing a direct measurements but a model.\n\nC.2.1 Data\nData is available from DEFRA (https://uk-air.defra.gov.uk/data/pcm-data) as CSVs in a UK grid.\n\nThe data begins on row 6 of each file and contains 4 columns: - a unique code (ukgridcode) for each 1x1km cell in the map - the x coordinates for the centroid of each grid cell - the y coordinates for the centroid of each grid cell - the values for the metric itself.\nThe coordinate system is OSGB and the coordinates represent the centre of each 1x1km cell.\nThere are some grid squares in each data set that do not have values associated with them and are labelled as ‘MISSING’\n\nWe can use pandas to read grids to Xarray for efficient array computation of the index.\n\npm10_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm102021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npm25_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm252021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nno2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapno22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nso2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapso22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npollutants_2021 = xr.merge([pm10_21, pm25_21, no2_21, so2_21])\npollutants_2021\n\nWe can compute the index based on the formula above.\n\naqi = (\n    pollutants_2021.pm252021g\n    + pollutants_2021.pm102021g / 2\n    + pollutants_2021.no22021 / 4\n    + pollutants_2021.so22021 / 10\n)\npollutants_2021 = pollutants_2021.assign(aqi=aqi)\n\nWe can filter only the area of interest before converting to a GeoDataFrame.\n\nbds = aoi.total_bounds\npollutants_2021_tyne = pollutants_2021.sel(\n    x=slice(bds[0], bds[2]), y=slice(bds[1], bds[3])\n)\n\nWe convert the array to a GeoDataFrame with polygons representing grid area. This will be needed for areal interpolation to LSOA/MSOA.\n\npollutants_2021_tyne_df = pollutants_2021_tyne.to_dataframe().reset_index()\npollutants_2021_tyne_df = gpd.GeoDataFrame(\n    pollutants_2021_tyne_df,\n    geometry=gpd.points_from_xy(\n        pollutants_2021_tyne_df.x, pollutants_2021_tyne_df.y, crs=27700\n    ).buffer(500, cap_style=3),\n)\n\nWe can check how the derived index looks.\n\npollutants_2021_tyne_df.explore(\"aqi\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\npollutants_2021_tyne_df.to_file(\n    f\"{data_folder}/processed/air_quality/air_quality_grid_2021.gpkg\"\n)"
  },
  {
    "objectID": "code/01_baseline/03_house_prices.html#process-house-prices-and-store-them-on-lsoa-level",
    "href": "code/01_baseline/03_house_prices.html#process-house-prices-and-store-them-on-lsoa-level",
    "title": "Appendix D — House price",
    "section": "D.1 Process house prices and store them on LSOA level",
    "text": "D.1 Process house prices and store them on LSOA level\nThis notebook processes a dataset containing a time-series of house price per LSOA and saves it as a GeoPackage for future use.\n\nimport datetime\n\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nSpecify paths to existing files, some of which need to be downloaded manually (see Notes).\n\nhttps://www.ons.gov.uk/peoplepopulationandcommunity/housing/datasets/medianpricepaidbylowerlayersuperoutputareahpssadataset46\nhttps://geoportal.statistics.gov.uk/datasets/2fb4e13605cb4745992390165307697e_0/explore?location=52.755877%2C-2.489798%2C7.62\n\n\nlsoa_list_file = (\n    f\"{data_folder}/processed/tynewear_lsoas_list.csv\"  # list of LSOAs within AoI\n)\n\n# national level files\nhouse_prices_lsoa_xls_file = f\"{data_folder}/raw/house_prices/hpssadataset46medianpricepaidforresidentialpropertiesbylsoa.xlsx\"  # excel spreadsheet downloaded from gov website (see Notes) with median house prices, quarterly data from Dec 1995\nLSOA_boundaries_file = f\"{data_folder}/raw/LSOA_(Dec_2011)_Boundaries_Super_Generalised_Clipped_(BSC)_EW_V3.geojson\"  # administrative boundaries from gov.uk OS\n\nRead house prices for whole country. The actual table starts with row 5 in sheet “Data”. ONS uses : to indicate missing value so we shall filter it on import.\n\nhouse_prices = pd.read_excel(\n    house_prices_lsoa_xls_file, \"Data\", header=5, na_values=\":\"\n)\n\nRead LSOA boundaries to link to data.\n\nlsoa = gpd.read_file(LSOA_boundaries_file)\n\nCheck structure before merge.\n\nlsoa.head()\n\n\n\n\n\n\n\n\nOBJECTID\nLSOA11CD\nLSOA11NM\nLSOA11NMW\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Area\nShape__Length\nGlobalID\ngeometry\n\n\n\n\n0\n1\nE01000001\nCity of London 001A\nCity of London 001A\n532129\n181625\n-0.097060\n51.51810\n157794.481079\n1685.391778\nb12173a3-5423-4672-a5eb-f152d2345f96\nPOLYGON ((-0.09474 51.52060, -0.09546 51.51544...\n\n\n1\n2\nE01000002\nCity of London 001B\nCity of London 001B\n532480\n181699\n-0.091970\n51.51868\n164882.427628\n1804.828196\n90274dc4-f785-4afb-95cd-7cc1fc9a2cad\nPOLYGON ((-0.08810 51.51941, -0.09546 51.51544...\n\n\n2\n3\nE01000003\nCity of London 001C\nCity of London 001C\n532245\n182036\n-0.095230\n51.52176\n42219.805717\n909.223277\n7e89d0ba-f186-45fb-961c-8f5ffcd03808\nPOLYGON ((-0.09453 51.52205, -0.09274 51.52139...\n\n\n3\n4\nE01000005\nCity of London 001E\nCity of London 001E\n533581\n181265\n-0.076280\n51.51452\n212682.404259\n2028.654904\na14c307a-874c-4862-828a-3b1486cc21ea\nPOLYGON ((-0.07589 51.51590, -0.07394 51.51445...\n\n\n4\n5\nE01000006\nBarking and Dagenham 016A\nBarking and Dagenham 016A\n544994\n184276\n0.089318\n51.53876\n130551.387161\n1716.896118\n65121a2d-3d2b-4935-9712-690f2993cfd2\nPOLYGON ((0.09328 51.53787, 0.09363 51.53767, ...\n\n\n\n\n\n\n\n\nhouse_prices.head()\n\n\n\n\n\n\n\n\nLocal authority code\nLocal authority name\nLSOA code\nLSOA name\nYear ending Dec 1995\nYear ending Mar 1996\nYear ending Jun 1996\nYear ending Sep 1996\nYear ending Dec 1996\nYear ending Mar 1997\n...\nYear ending Mar 2020\nYear ending Jun 2020\nYear ending Sep 2020\nYear ending Dec 2020\nYear ending Mar 2021\nYear ending Jun 2021\nYear ending Sep 2021\nYear ending Dec 2021\nYear ending Mar 2022\nYear ending Jun 2022\n\n\n\n\n0\nE06000001\nHartlepool\nE01011949\nHartlepool 009A\n34750.0\n34500.0\n30500.0\n30000.0\n29950.0\n29000.0\n...\n89000.0\n84000.0\n88000.0\n88000.0\n81500.0\n80500.0\n85500.0\n100750.0\n100000.0\n121000.0\n\n\n1\nE06000001\nHartlepool\nE01011950\nHartlepool 008A\n25000.0\n25000.0\n25300.0\n25625.0\n25000.0\n24800.0\n...\n27000.0\n26000.0\n28500.0\n30000.0\n33000.0\n47000.0\n50079.0\n50159.0\n50159.0\n49000.0\n\n\n2\nE06000001\nHartlepool\nE01011951\nHartlepool 007A\n27000.0\n27000.0\n27250.0\n28950.0\n28500.0\n28950.0\n...\n40000.0\n29425.0\n30000.0\n50000.0\n51500.0\n53000.0\n58573.5\n60000.0\n61499.5\n60000.0\n\n\n3\nE06000001\nHartlepool\nE01011952\nHartlepool 002A\n44500.0\n44500.0\n30000.0\n26675.0\n26000.0\n25500.0\n...\n70000.0\n66475.0\n85000.0\n85000.0\nNaN\n83500.0\n83000.0\n80000.0\n75500.0\n75000.0\n\n\n4\nE06000001\nHartlepool\nE01011953\nHartlepool 002B\n22000.0\n27000.0\n27000.0\n20600.0\n20000.0\n19500.0\n...\n58000.0\n60000.0\nNaN\nNaN\nNaN\nNaN\nNaN\n90000.0\nNaN\n95000.0\n\n\n\n\n5 rows × 111 columns\n\n\n\nMerge the data on LSOA codes.\n\nlsoa = lsoa.merge(house_prices, right_on=\"LSOA code\", left_on=\"LSOA11CD\", how=\"left\")\n\nRead the list of LSOA belonging to the area of interest.\n\ntyne_lsoa = pd.read_csv(lsoa_list_file, index_col=0)\n\nFilter the whole country.\n\ntyne_prices = lsoa[lsoa.LSOA11CD.isin(tyne_lsoa[\"LSOA code\"])]\n\n\ntyne_prices.head()\n\n\n\n\n\n\n\n\nOBJECTID\nLSOA11CD\nLSOA11NM\nLSOA11NMW\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Area\nShape__Length\n...\nYear ending Mar 2020\nYear ending Jun 2020\nYear ending Sep 2020\nYear ending Dec 2020\nYear ending Mar 2021\nYear ending Jun 2021\nYear ending Sep 2021\nYear ending Dec 2021\nYear ending Mar 2022\nYear ending Jun 2022\n\n\n\n\n7948\n7949\nE01008162\nGateshead 027A\nGateshead 027A\n426408\n562669\n-1.58915\n54.95797\n4.337674e+05\n2795.514484\n...\n124000.0\n107500.0\n97000.0\n97000.0\n100000.0\n137260.0\n137510.0\n107000.0\n108250.0\n99500.0\n\n\n7949\n7950\nE01008163\nGateshead 028A\nGateshead 028A\n425687\n562359\n-1.60044\n54.95522\n3.886396e+05\n2960.341729\n...\n113500.0\n105000.0\n124000.0\n138000.0\n138975.0\n138000.0\n121500.0\n107000.0\n107000.0\n115000.0\n\n\n7950\n7951\nE01008164\nGateshead 027B\nGateshead 027B\n426855\n562213\n-1.58222\n54.95385\n1.853580e+05\n2097.848575\n...\n76250.0\n87500.0\n66475.0\n62000.0\n60000.0\n56250.0\n65000.0\n84925.0\n125000.0\n140000.0\n\n\n7951\n7952\nE01008165\nGateshead 011A\nGateshead 011A\n426340\n562156\n-1.59026\n54.95336\n2.549571e+05\n2523.273552\n...\n79250.0\n81250.0\n83250.0\n105000.0\n120000.0\n123500.0\n147250.0\n119500.0\n112630.0\n125260.0\n\n\n7952\n7953\nE01008166\nGateshead 027C\nGateshead 027C\n426244\n563319\n-1.59166\n54.96382\n1.412473e+06\n4863.402280\n...\n145750.0\n149000.0\n145975.0\n136000.0\n139025.0\n138050.0\n139025.0\n150500.0\n173500.0\n177000.0\n\n\n\n\n5 rows × 123 columns\n\n\n\nSimplify columns names\n\ntyne_prices.columns = [c.replace(\"Year ending \", \"\") for c in tyne_prices.columns]\n\n\ntyne_prices.columns\n\nIndex(['OBJECTID', 'LSOA11CD', 'LSOA11NM', 'LSOA11NMW', 'BNG_E', 'BNG_N',\n       'LONG', 'LAT', 'Shape__Area', 'Shape__Length',\n       ...\n       'Mar 2020', 'Jun 2020', 'Sep 2020', 'Dec 2020', 'Mar 2021', 'Jun 2021',\n       'Sep 2021', 'Dec 2021', 'Mar 2022', 'Jun 2022'],\n      dtype='object', length=123)\n\n\nSave to GeoPackage\n\ntyne_prices.reset_index(drop=True).to_file(\n    f\"{data_folder}/house_prices_lsoa_jun22.gpkg\"\n)\n\nQuickly check the distribution. Housing prices are often better to model as log.\n\ntyne_prices[\"Jun 2022\"].plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThere is a long tail but not that critical. Let’s check logged.\n\nnp.log(tyne_prices[\"Jun 2022\"]).plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nNow we have some outliers on lower end. Will need to decide how to pass the data to the modelling process, some additional preprocessing may be needed.\n\ntyne_prices[\"Jun 2022\"].describe()\n\ncount       694.000000\nmean     160843.278818\nstd       72681.078875\nmin       26000.000000\n25%      114612.500000\n50%      139972.500000\n75%      186937.500000\nmax      705500.000000\nName: Jun 2022, dtype: float64\n\n\nThe problem with this dataset is that it is not normalised, so we have values per whole house without acknowledging its size or any other variable.\nWe can potentially get other measures - mean, lower quartile and tenth percentile price, plus counts of sales per type but at this level of aggregation, it probably won’t help a lot. The optimal would be a measure per sq.m. or access to other variables allowing us to model it but that does not seem to be available."
  },
  {
    "objectID": "code/01_baseline/03_house_prices.html#a-new-attribute-linked-residential-property-price-dataset-for-england-and-wales-2011-2019",
    "href": "code/01_baseline/03_house_prices.html#a-new-attribute-linked-residential-property-price-dataset-for-england-and-wales-2011-2019",
    "title": "Appendix D — House price",
    "section": "D.2 A new attribute-linked residential property price dataset for England and Wales 2011-2019",
    "text": "D.2 A new attribute-linked residential property price dataset for England and Wales 2011-2019\nWe can use https://reshare.ukdataservice.ac.uk/854942/ that derived a price per sqm from link between sales data and EPC.\n\n# It is 6.3GB...\nlinked_epc_path = \"https://reshare.ukdataservice.ac.uk/854942/1/tranall2011_19.csv\"\n\nFilter only Tyne and Wear for the last two years (2018-19).\n\nepc = pd.read_csv(linked_epc_path)\nepc[\"dateoftransfer\"] = pd.to_datetime(epc.dateoftransfer)\nlast2years = epc[epc.dateoftransfer &gt; datetime.datetime(2018, 1, 1)]\ntyne = last2years[last2years.county == \"TYNE AND WEAR\"]\ntyne\n\n/tmp/ipykernel_3533493/3103178836.py:1: DtypeWarning: Columns (40,60) have mixed types. Specify dtype option on import or set low_memory=False.\n  epc = pd.read_csv(\"https://reshare.ukdataservice.ac.uk/854942/1/tranall2011_19.csv\")\n\n\n\n\n\n\n\n\n\nid\ntransactionid\noa11\npostcode\nprice\ndateoftransfer\npropertytype\noldnew\nduration\npaon\n...\nMAIN_FUEL\nWIND_TURBINE_COUNT\nHEAT_LOSS_CORRIDOOR\nUNHEATED_CORRIDOR_LENGTH\nFLOOR_HEIGHT\nPHOTO_SUPPLY\nSOLAR_WATER_HEATING_FLAG\nMECHANICAL_VENTILATION\nLOCAL_AUTHORITY_LABEL\nCONSTITUENCY_LABEL\n\n\n\n\n938059\n12825134\n{8A78B2B0-1AE4-5CB0-E053-6B04A8C0F504}\nE00042737\nNE3 1QX\n162000.0\n2019-04-24\nF\nN\nL\n132\n...\nmains gas (not community)\n0\nno corridor\nNaN\n3.07\n0.0\nNaN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne North\n\n\n938062\n12825155\n{75050A85-C3A2-9A88-E053-6B04A8C02390}\nE00042719\nNE3 4RQ\n363200.0\n2018-08-02\nT\nN\nF\n2\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne Central\n\n\n938067\n12825169\n{75050A85-C385-9A88-E053-6B04A8C02390}\nE00042710\nNE3 1NN\n205000.0\n2018-07-18\nF\nN\nL\nFERNDENE COURT\n...\nmains gas (not community)\n0\nunheated corridor\n7.8\nNaN\nNaN\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne North\n\n\n938068\n12825170\n{68FEB20C-0D16-38DA-E053-6C04A8C051AE}\nE00042820\nNE4 6BA\n160000.0\n2018-03-15\nF\nN\nL\nBARRACK COURT\n...\nmains gas - this is for backwards compatibilit...\n0\nno corridor\nNaN\n2.28\n0.0\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne Central\n\n\n938082\n12825219\n{79A74E22-3FBD-1289-E053-6B04A8C01627}\nE00042388\nNE3 2HA\n176500.0\n2018-09-24\nF\nN\nL\n46\n...\nmains gas (not community)\n0\nno corridor\nNaN\nNaN\nNaN\nN\nnatural\nNewcastle upon Tyne\nNewcastle upon Tyne North\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1543044\n14846632\n{8F1B26BE-496B-53DB-E053-6C04A8C03649}\nE00041827\nNE40 3RT\n200000.0\n2019-07-16\nT\nN\nL\n15\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nGateshead\nBlaydon\n\n\n1543047\n14846641\n{965B6D91-FF99-95E4-E053-6C04A8C07729}\nE00041731\nNE10 9SA\n135000.0\n2019-10-04\nS\nN\nL\n55\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nGateshead\nGateshead\n\n\n1543048\n14846652\n{8F1B26BE-47B9-53DB-E053-6C04A8C03649}\nE00041511\nNE9 7TQ\n179950.0\n2019-07-05\nD\nN\nF\n1\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nGateshead\nGateshead\n\n\n3053559\n3043457\n{93E6821E-E6ED-40FD-E053-6B04A8C0C1DF}\nE00104805\nNE17 7TB\n310000.0\n2019-09-09\nD\nN\nF\nSCHOOL HOUSE\n...\nbulk wood pellets\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nCounty Durham\nNorth West Durham\n\n\n3068117\n3098279\n{6DA0844A-764F-30F2-E053-6B04A8C05F3B}\nE00104812\nNE17 7QA\n81000.0\n2018-03-12\nT\nN\nF\n6\n...\nmains gas (not community)\n0\nNO DATA!\nNaN\nNaN\nNaN\nN\nnatural\nCounty Durham\nNorth West Durham\n\n\n\n\n20143 rows × 105 columns\n\n\n\nLink to geometry\n\noa = gpd.read_file(\n    \"https://borders.ukdataservice.ac.uk/ukborders/easy_download/prebuilt/shape/infuse_oa_lyr_2011_clipped.zip\"\n)\ntyne_agg = (\n    tyne[[\"oa11\", \"priceper\", \"numberrooms\", \"price\", \"tfarea\"]]\n    .groupby(\"oa11\")\n    .mean()\n    .reset_index()\n)\ntyne_oa = oa.merge(tyne_agg, left_on=\"geo_code\", right_on=\"oa11\", how=\"inner\")\ntyne_oa\n\nERROR 1: PROJ: proj_create_from_database: Open of /home/martin/mambaforge/envs/ulce/share/proj failed\n\n\n\n\n\n\n\n\n\ngeo_code\ngeometry\noa11\npriceper\nnumberrooms\nprice\ntfarea\n\n\n\n\n0\nE00042786\nPOLYGON ((428997.799 566018.331, 428998.491 56...\nE00042786\n1717.546956\n5.666667\n203158.333333\n124.333333\n\n\n1\nE00042707\nPOLYGON ((424221.655 568003.052, 424221.754 56...\nE00042707\n2933.816547\n4.066667\n341234.066667\n124.475333\n\n\n2\nE00042703\nPOLYGON ((419858.836 565454.433, 419858.374 56...\nE00042703\n1608.132502\n3.777778\n126666.666667\n84.666667\n\n\n3\nE00042782\nPOLYGON ((428932.199 566299.133, 428933.629 56...\nE00042782\n1864.265091\n5.000000\n179833.333333\n99.056667\n\n\n4\nE00042789\nPOLYGON ((428853.730 565689.295, 428860.602 56...\nE00042789\n1814.122597\n3.857143\n136528.571429\n75.697143\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3510\nE00041811\nPOLYGON ((428782.519 562282.321, 428783.470 56...\nE00041811\n1660.067873\n5.000000\n147500.000000\n86.000000\n\n\n3511\nE00041818\nPOLYGON ((416764.269 564158.045, 416764.216 56...\nE00041818\n2616.513428\n4.333333\n222333.333333\n81.800000\n\n\n3512\nE00041898\nPOLYGON ((423738.129 560286.484, 423738.500 56...\nE00041898\n1538.080092\n4.250000\n124487.500000\n81.070000\n\n\n3513\nE00041819\nPOLYGON ((415843.233 564846.473, 415868.526 56...\nE00041819\n2719.109422\n3.750000\n202417.000000\n75.750000\n\n\n3514\nE00041388\nPOLYGON ((426402.927 562379.016, 426417.019 56...\nE00041388\n1463.861134\n5.333333\n153000.000000\n103.333333\n\n\n\n\n3515 rows × 7 columns\n\n\n\n\ntyne_oa.to_file(\n    f\"{data_folder}/processed/house_prices/price_per_sqm.gpkg\", engine=\"pyogrio\"\n)\n\n\ntyne_oa.plot(\"priceper\", figsize=(12, 12), legend=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ntyne_oa.priceper.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\n\nnp.log(tyne_oa.priceper).plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nWe should be comfortably able to model log of price per sqm here."
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#air-quality",
    "href": "code/02_models/01_explanatory_variables.html#air-quality",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.1 Air Quality",
    "text": "E.1 Air Quality\nAir quality index is measured on a grid and needs to be interpolated.\nLoad the grid.\n\nair = gpd.read_file(f\"{data_folder}/processed/air_quality/air_quality_grid_2021.gpkg\")\n\nCheck the projections.\n\nair.crs == oa_aoi.crs\n\nTrue\n\n\nRun areal interpolation.\n\ninterp = tobler.area_weighted.area_interpolate(air, oa_aoi, intensive_variables=[\"aqi\"])\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: aqi, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n\n\nVisual check.\n\ninterp.plot(\"aqi\", figsize=(12, 12), legend=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nAssign the interpolated values to the OA dataframe.\n\noa_aoi[\"air_quality_index\"] = interp.aqi.values"
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#house-prices",
    "href": "code/02_models/01_explanatory_variables.html#house-prices",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.2 House prices",
    "text": "E.2 House prices\nHouse prices are reported on the OA level and can be merged via attribute.\nLoad data.\n\nhouse_price = gpd.read_file(f\"{data_folder}/processed/house_prices/price_per_sqm.gpkg\")\n\nMerge based on an OA code.\n\noa_aoi = oa_aoi.merge(house_price[[\"geo_code\", \"priceper\"]], on=\"geo_code\", how=\"left\")\n\nVisual check.\n\noa_aoi.plot(np.log(oa_aoi[\"priceper\"]), figsize=(12, 12), legend=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nQuestion: We need to figure out what to do with OAs that do not have any house price data. I would probably drop them before the training of ML. Interpolation may be tricky as we don’t know the exact reasons for their missingness (may be mostly parks…). We can also try to load older data there but given the dynamics of house pricing, that may cause more trouble than good.\n\nE.2.1 Job Accessibility\nJob accessibility is measured on the OA level and can be merged.\n\njobs = pd.read_csv(\n    f\"{data_folder}/outputs/results_accessibility/acc_jobs15_OAtoWPZ_tynewear.csv\",\n    index_col=0,\n)\njobs.head()\n\n\n\n\n\n\n\n\nfrom_id\nA_pop_cum_15\n\n\n\n\n0\nE00041363\n4318\n\n\n1\nE00041364\n7604\n\n\n2\nE00041366\n845\n\n\n3\nE00041367\n3413\n\n\n4\nE00041368\n1728\n\n\n\n\n\n\n\nMerge based on an attribute.\n\noa_aoi = oa_aoi.merge(jobs, left_on=\"geo_code\", right_on=\"from_id\", how=\"left\")\n\n\n\nE.2.2 Green space accessibility\nGreen space accessibility is measured on the OA level and can be merged.\n\ngs = pd.read_csv(\n    f\"{data_folder}/outputs/results_accessibility/acc_walking_gs15_OAtoGS_tynewear.csv\",\n    index_col=0,\n)\ngs.head()\n\n\n\n\n\n\n\n\nfrom_id\narea_m2\n\n\n\n\n0\nE00041363\n526165.89095\n\n\n1\nE00041364\n527070.65980\n\n\n2\nE00041366\n265337.36045\n\n\n3\nE00041367\n505729.33200\n\n\n4\nE00041368\n325226.25195\n\n\n\n\n\n\n\nMerge based on an attribute.\n\noa_aoi = oa_aoi.merge(gs, left_on=\"geo_code\", right_on=\"from_id\", how=\"left\")"
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#explanatory-variables",
    "href": "code/02_models/01_explanatory_variables.html#explanatory-variables",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.3 Explanatory variables",
    "text": "E.3 Explanatory variables\nWe also want to have all our explanatory variables linked to the OA geometries.\n\nE.3.1 Population estimates\nONS population estimates are reported on the OA level and can be merged.\nRead the file from ONS.\n\npop = pd.read_excel(\n    pooch.retrieve(\n        url=\"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/censusoutputareaestimatesinthenortheastregionofengland/mid2020sape23dt10d/sape23dt10dmid2020coaunformattedsyoaestimatesnortheast.xlsx\",\n        known_hash=None,\n    ),\n    sheet_name=\"Mid-2020 Persons\",\n    skiprows=4,\n)\n\nAttribute join.\n\noa_aoi = oa_aoi.merge(\n    pop[[\"OA11CD\", \"All Ages\"]], left_on=\"geo_code\", right_on=\"OA11CD\", how=\"left\"\n)\n\nVisual check.\n\noa_aoi.plot(\"All Ages\", figsize=(12, 12), legend=True, scheme=\"naturalbreaks\")\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nCheck of the distribution.\n\noa_aoi[\"All Ages\"].plot.hist(bins=500)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nWe will need to manage outliers prior modeling here.\n\n\nE.3.2 Workplace population\nWorkplace population is reported on Workplace Zones and needs to be interpolated to OA. We use the preprocessed data from the Urban Grammar project.\n\nwp = gpd.read_parquet(\n    f\"{data_folder}/raw/workplace_population/workplace_by_industry_gb.pq\"\n)\n\nWe use spatial query to filter only WPZs within the area of interest.\n\naoi_ix, wp_ix = wp.sindex.query_bulk(aoi.geometry, predicate=\"intersects\")\nwp_tyne = wp.iloc[np.unique(wp_ix)]\n\nVisual check.\n\nwp_tyne.plot(\n    wp_tyne.sum(axis=1, numeric_only=True),\n    figsize=(12, 12),\n    legend=True,\n    scheme=\"naturalbreaks\",\n)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nWe interpolate the data from WPZ to OA.\n\nwp_interpolated = tobler.area_weighted.area_interpolate(\n    wp_tyne,\n    oa_aoi,\n    extensive_variables=[\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ],\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nAnd assign the result to OA dataframe.\n\noa_aoi[\n    [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ]\n] = wp_interpolated.drop(columns=\"geometry\").values\n\n\n\nE.3.3 Land cover (CORINE)\nCORINE is shipped as custom polygons. We use the data downloaded for the Urban Grammar project.\n\ncorine = gpd.read_parquet(f\"{data_folder}/raw/land_cover/corine_gb.pq\")\n\nWe filter out only those polygons intersecting the area of interest.\n\naoi_ix, corine_ix = corine.sindex.query_bulk(aoi.geometry, predicate=\"intersects\")\ncorine_tyne = corine.iloc[np.unique(corine_ix)]\n\nWe interpolate the values to OA as a proportion covered by each type.\n\ncorine_interpolated = tobler.area_weighted.area_interpolate(\n    corine_tyne, oa_aoi, categorical_variables=[\"Code_18\"]\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nVisual check.\n\ncorine_interpolated.plot(\"Code_18_112\")\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nAssign land cover class names.\n\ncorine_names = {\n    \"Code_18_124\": \"Land cover [Airports]\",\n    \"Code_18_211\": \"Land cover [Non-irrigated arable land]\",\n    \"Code_18_121\": \"Land cover [Industrial or commercial units]\",\n    \"Code_18_421\": \"Land cover [Salt marshes]\",\n    \"Code_18_522\": \"Land cover [Estuaries]\",\n    \"Code_18_142\": \"Land cover [Sport and leisure facilities]\",\n    \"Code_18_141\": \"Land cover [Green urban areas]\",\n    \"Code_18_112\": \"Land cover [Discontinuous urban fabric]\",\n    \"Code_18_231\": \"Land cover [Pastures]\",\n    \"Code_18_311\": \"Land cover [Broad-leaved forest]\",\n    \"Code_18_131\": \"Land cover [Mineral extraction sites]\",\n    \"Code_18_123\": \"Land cover [Port areas]\",\n    \"Code_18_122\": \"Land cover [Road and rail networks and associated land]\",\n    \"Code_18_512\": \"Land cover [Water bodies]\",\n    \"Code_18_243\": \"Land cover [Land principally occupied by agriculture, with significant areas of natural vegetation]\",\n    \"Code_18_313\": \"Land cover [Mixed forest]\",\n    \"Code_18_412\": \"Land cover [Peat bogs]\",\n    \"Code_18_321\": \"Land cover [Natural grasslands]\",\n    \"Code_18_322\": \"Land cover [Moors and heathland]\",\n    \"Code_18_324\": \"Land cover [Transitional woodland-shrub]\",\n    \"Code_18_111\": \"Land cover [Continuous urban fabric]\",\n    \"Code_18_423\": \"Land cover [Intertidal flats]\",\n    \"Code_18_523\": \"Land cover [Sea and ocean]\",\n    \"Code_18_312\": \"Land cover [Coniferous forest]\",\n    \"Code_18_133\": \"Land cover [Construction sites]\",\n    \"Code_18_333\": \"Land cover [Sparsely vegetated areas]\",\n    \"Code_18_332\": \"Land cover [Bare rocks]\",\n    \"Code_18_411\": \"Land cover [Inland marshes]\",\n    \"Code_18_132\": \"Land cover [Dump sites]\",\n    \"Code_18_222\": \"Land cover [Fruit trees and berry plantations]\",\n    \"Code_18_242\": \"Land cover [Complex cultivation patterns]\",\n    \"Code_18_331\": \"Land cover [Beaches, dunes, sands]\",\n    \"Code_18_511\": \"Land cover [Water courses]\",\n    \"Code_18_334\": \"Land cover [Burnt areas]\",\n    \"Code_18_244\": \"Land cover [Agro-forestry areas]\",\n    \"Code_18_521\": \"Land cover [Coastal lagoons]\",\n}\n\ncorine_interpolated.columns = corine_interpolated.columns.map(corine_names)\n\nAssign the subset of classes that may be interesting for our purposes to OA dataframe.\n\ninteresting = [\n    \"Land cover [Discontinuous urban fabric]\",\n    \"Land cover [Continuous urban fabric]\",\n    \"Land cover [Non-irrigated arable land]\",\n    \"Land cover [Industrial or commercial units]\",\n    \"Land cover [Green urban areas]\",\n    \"Land cover [Pastures]\",\n    \"Land cover [Sport and leisure facilities]\",\n]\noa_aoi[interesting] = corine_interpolated[interesting].values\n\n\n\nE.3.4 Urban morphometrics\nMorphometric data is coming directly from the Urban Grammar project.\nWe need to first identify the chunks covering the AOI.\n\nchunks = gpd.read_parquet(f\"{data_folder}/raw/urban_morpho/local_auth_chunks.pq\")\n\nVisual exploration.\n\nm = chunks[chunks.intersects(aoi.unary_union)].explore()\naoi.explore(m=m, color=\"red\")\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n  return lib.intersects(a, b, **kwargs)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe need to work with the chunk 26.\n\ncells = gpd.read_parquet(f\"{data_folder}/raw/urban_morpho/cells_26.pq\")\n\nDrop columns we don’t need.\n\nchars = cells.columns.drop(\n    [\n        \"hindex\",\n        \"tessellation\",\n        \"buildings\",\n        \"nodeID\",\n        \"edgeID_keys\",\n        \"edgeID_values\",\n        \"edgeID_primary\",\n    ]\n)\n\nInterpolate values to OA.\n\nmorhp_interpolated = tobler.area_weighted.area_interpolate(\n    cells, oa_aoi, intensive_variables=chars.tolist()\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sdbAre, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sdbPer, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sdbCoA, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCCo, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCor, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbSqu, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbERI, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbElo, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCCM, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: ssbCCD, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: stbOri, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: sicCAR, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: stbCeA, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: mtbAli, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: mtbNDi, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: stbSAl, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: lieWCe, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n\n\nAssign to the OA dataframe.\n\noa_aoi[morhp_interpolated.columns.drop(\"geometry\")] = morhp_interpolated.drop(\n    columns=\"geometry\"\n).values\n\nClean OA dataframe and rename some columns.\n\noa_aoi = oa_aoi.drop(columns=\"OA11CD\").rename(\n    columns={\n        \"priceper\": \"house_price_index\",\n        \"All Ages\": \"population_estimate\",\n        \"A_pop_cum_15\": \"jobs_accessibility_index\",\n        \"area_m2\": \"greenspace_accessibility_index\",\n    }\n)\n\nSave to Parquet file.\n\noa_aoi.to_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\n\noa_aoi.drop(columns=\"geometry\").to_csv(\n    f\"{data_folder}/processed/interpolated/all_oa.csv\", index=False\n)"
  },
  {
    "objectID": "code/02_models/01_explanatory_variables.html#maps",
    "href": "code/02_models/01_explanatory_variables.html#maps",
    "title": "Appendix E — Authoritative table of explanatory variables on the Output Area level",
    "section": "E.4 Maps",
    "text": "E.4 Maps\nWe can plot all the values as map for visualisation purposes.\nIn case we are loading the notebook again and want only plotting, we can load the OA dataframe directly here skipping most of the cells above.\n\noa_aoi = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nImport plotting libraries.\n\nimport contextily\nimport palettable.matplotlib as palmpl\nimport matplotlib.pyplot as plt\nimport matplotlib.cm\nimport mapclassify\nimport husl\nimport seaborn as sns\n\n\nfrom utils import legendgram\n\n\nE.4.0.1 Air Quality\nTo get the right extent, we create a box to ensure contextily loads the proper tiles.\n\nfrom shapely.geometry import box\n\nbds = oa_aoi.total_bounds\nextent = gpd.GeoSeries(\n    [box((bds[0] - 7000), bds[1], bds[2] + 7000, bds[3])], crs=oa_aoi.crs\n).to_crs(3857)\n\nPlot the data.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    \"air_quality_index\", scheme=\"equalinterval\", k=20, ax=ax, alpha=0.9, cmap=\"magma_r\"\n)\nbins = mapclassify.EqualInterval(oa_aoi[\"air_quality_index\"].values, k=20).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"air_quality_index\"],\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/air_quality_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\n\nE.4.0.2 House price\nFilling NAs just for plotting purposes.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    oa_aoi[\"house_price_index\"].fillna(method=\"pad\"),\n    scheme=\"naturalbreaks\",\n    k=10,\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nbins = mapclassify.NaturalBreaks(oa_aoi[\"house_price_index\"].dropna().values, k=10).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"house_price_index\"].dropna(),\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        oa_aoi[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/house_price_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/house_price_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\n\nE.4.0.3 Jobs accessibility\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    oa_aoi[\"jobs_accessibility_index\"].fillna(method=\"pad\"),\n    scheme=\"naturalbreaks\",\n    k=10,\n    ax=ax,\n    alpha=0.9,\n    cmap=\"plasma\",\n)\nbins = mapclassify.NaturalBreaks(\n    oa_aoi[\"jobs_accessibility_index\"].dropna().values, k=10\n).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"jobs_accessibility_index\"].dropna(),\n    bins,\n    pal=palmpl.Plasma_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    # clip = (10,20), # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/jobs_accessibility_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/jobs_accessibility_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    oa_aoi[\"greenspace_accessibility_index\"].fillna(method=\"pad\"),\n    scheme=\"naturalbreaks\",\n    k=10,\n    ax=ax,\n    alpha=0.9,\n    cmap=\"YlGn\",\n)\nbins = mapclassify.NaturalBreaks(\n    oa_aoi[\"greenspace_accessibility_index\"].dropna().values, k=10\n).bins\nlegendgram(\n    f,\n    ax,\n    oa_aoi[\"greenspace_accessibility_index\"].dropna(),\n    bins,\n    pal=matplotlib.cm.get_cmap(\"YlGn\"),\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    # clip = (10,20), # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/greenspace_accessibility_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/greenspace_accessibility_index.svg\",\n    dpi=144,\n    bbox_inches=\"tight\",\n)\n\n\n\n\n\n\nE.4.0.4 Explanatory variables\nSome explanatory variables are better to be plotted as densities.\n\nto_density = [\n    \"A, B, D, E. Agriculture, energy and water\",\n    \"C. Manufacturing\",\n    \"F. Construction\",\n    \"G, I. Distribution, hotels and restaurants\",\n    \"H, J. Transport and communication\",\n    \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n    \"O,P,Q. Public administration, education and health\",\n    \"R, S, T, U. Other\",\n    \"population_estimate\",\n]\n\nLoop over explanatory variables, assign a random hue, generate a colormap and plot.\n\nfor char in oa_aoi.columns.drop(\n    [\n        \"geometry\",\n        \"geo_code\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n    ]\n):\n    if char in to_density:\n        values = oa_aoi[char] / oa_aoi.area\n    else:\n        values = oa_aoi[char]\n\n    gdf = oa_aoi.to_crs(3857)\n    hue = np.random.randint(0, 360, 1)[0]\n    k = 10  # number of classes\n    clip = None  # clip=(0, 100)  # in case of long tail\n    mc = mapclassify.NaturalBreaks(values, k=k).bins  # classification scheme\n    bins = 50  # resolution of histogram\n\n    # plotting\n    color = (hue, 75, 35)\n    c = husl.husl_to_hex(*color)\n    cmap = sns.light_palette(color, input=\"husl\", as_cmap=True, n_colors=k)\n\n    ax = gdf.plot(\n        values,\n        cmap=cmap,\n        figsize=(18, 12),\n        scheme=\"UserDefined\",\n        k=k,\n        classification_kwds=dict(bins=mc),\n    )\n    extent.plot(ax=ax, alpha=0)\n    ax.set_axis_off()\n\n    # add legend\n    hax = legendgram(\n        plt.gcf(),  # grab the figure, we need it\n        ax,  # the axis to add the legend\n        values,  # the attribute to map\n        mc,\n        cmap,  # the palette to use\n        legend_size=(0.35, 0.15),\n        loc=4,\n        #    tick_params={'color':c, 'labelcolor':c,'labelsize':10},\n        clip=clip,\n        bins=bins,\n    )  # when usign legendgram, dev version from mpl_cmap branch should be used\n    contextily.add_basemap(\n        ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n    )\n    contextily.add_basemap(\n        ax=ax,\n        source=contextily.providers.Stamen.TonerLines,\n        alpha=0.4,\n        attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n    )\n    plt.savefig(\n        f\"{data_folder}/outputs/figures/svg/ex_var_{char}.svg\",\n        dpi=144,\n        bbox_inches=\"tight\",\n    )\n    plt.close()\n\n\nsignatures = pd.read_parquet(\"../blackbox/data/oa_key.parquet\")\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\noa_aoi.to_crs(3857).plot(\n    color=oa_aoi.merge(\n        signatures, how=\"left\", left_on=\"geo_code\", right_index=True\n    ).primary_type.map(ugg.get_signature_colors()),\n    ax=ax,\n    alpha=0.9,\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.CartoDB.PositronNoLabels.build_url(scale_factor=\"@2x\"),\n    attribution=\"\",\n)\n# contextily.add_basemap(ax=ax, source=contextily.providers.Stamen.TonerLines, alpha=.4, attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\")\n# plt.savefig(f\"{data_folder}/outputs/figures/greenspace_accessibility_index.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\n    f\"{data_folder}/outputs/figures/svg/signatures.svg\", dpi=144, bbox_inches=\"tight\"\n)"
  },
  {
    "objectID": "code/02_models/01b_latent_sentinel.html",
    "href": "code/02_models/01b_latent_sentinel.html",
    "title": "Appendix F — Sentinel 2 latent representation",
    "section": "",
    "text": "Process the data from the https://doi.org/10.1016/j.compenvurbsys.2022.101802 paper to be used in modelling.\n\nimport geopandas as gpd\nimport tobler\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_69158/2891901632.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas as gpd\n\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\noa = gpd.read_parquet(\n    f\"{data_folder}/processed/interpolated/all_oa.parquet\",\n    columns=[\"geo_code\", \"geometry\"],\n)\n\n\npostcodes = gpd.read_parquet(\n    f\"{data_folder}/raw/sentinel_latent/latent_smoothed.parquet\"\n)\n\nCreate geometry of original samples.\n\npostcodes.geometry = postcodes.buffer(80, cap_style=3)\n\nJoin with the OA in the area of interest.\n\nxmin, ymin, xmax, ymax = oa.total_bounds\npostcodes_aoi = postcodes.cx[xmin:xmax, ymin:ymax]\n\n\npostcodes_aoi = postcodes_aoi.sjoin(oa, how=\"inner\")\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/geopandas/geodataframe.py:2061: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\nUse `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n\nLeft CRS: EPSG:27700\nRight CRS: EPSG:27700\n\n  return geopandas.sjoin(left_df=self, right_df=df, *args, **kwargs)\n\n\nCreate a mean latent vector per OA.\n\nlatent_oa = (\n    postcodes_aoi.drop(columns=[\"geo_code\", \"geometry\"]).groupby(\"index_right\").mean()\n)\n\n\nlatent_oa\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\nindex_right\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n-0.682062\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.451931\n-0.211763\n0.387425\n0.928424\n...\n-1.021918\n-1.021918\n0.440444\n1.743971\n0.320842\n1.474930\n1.151639\n-0.132747\n1.498061\n0.871063\n\n\n1\n-0.663786\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.349480\n-0.311341\n0.504530\n0.902771\n...\n-1.021918\n-1.021918\n0.577340\n1.883114\n0.293000\n1.414315\n1.145569\n-0.093327\n1.454991\n0.880657\n\n\n2\n-0.657851\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.414436\n-0.291362\n0.512188\n0.857220\n...\n-1.021918\n-1.021918\n0.562472\n1.878608\n0.301625\n1.483689\n1.187823\n-0.086438\n1.513144\n0.918716\n\n\n3\n-0.666984\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.427887\n-0.262477\n0.397210\n0.949204\n...\n-1.021918\n-1.021918\n0.464777\n1.763278\n0.334995\n1.434697\n1.128427\n-0.116929\n1.466723\n0.861476\n\n\n4\n-0.679580\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.461694\n-0.156182\n0.458418\n0.925219\n...\n-1.021918\n-1.021918\n0.490356\n1.797309\n0.374750\n1.505300\n1.117723\n-0.139840\n1.487528\n0.887797\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3790\n-0.646544\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.448320\n-0.247494\n0.457923\n0.925764\n...\n-1.021918\n-1.021918\n0.596297\n1.882795\n0.314640\n1.466138\n1.180296\n-0.069748\n1.481510\n0.899854\n\n\n3791\n-0.641375\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.533849\n-0.143489\n0.441434\n0.966423\n...\n-1.021918\n-1.021918\n0.556728\n1.898314\n0.299499\n1.554656\n1.178569\n-0.087546\n1.507623\n0.944229\n\n\n3792\n-0.642435\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.508919\n-0.193876\n0.475717\n0.867998\n...\n-1.021918\n-1.021918\n0.583808\n1.872691\n0.381800\n1.518266\n1.206397\n-0.212747\n1.641260\n0.999635\n\n\n3793\n-0.626392\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n-1.021918\n0.570091\n-0.141022\n0.387859\n0.993209\n...\n-1.021918\n-1.021918\n0.558174\n1.881344\n0.309562\n1.597636\n1.203249\n-0.086112\n1.549585\n0.941391\n\n\n3794\n-0.630401\n-1.021918\n-1.021918\n-1.021909\n-1.021918\n-1.021918\n0.507333\n-0.214628\n0.472556\n0.940042\n...\n-1.021918\n-1.021918\n0.606770\n1.923926\n0.276992\n1.529824\n1.194195\n-0.077461\n1.518514\n0.937542\n\n\n\n\n3795 rows × 64 columns\n\n\n\n\nlatent_oa = latent_oa.set_geometry(oa.geometry)\n\n\nlatent_oa.to_parquet(f\"{data_folder}/processed/sentinel/latent_oa.parquet\")"
  },
  {
    "objectID": "code/02_models/02_collinearity_filter.html",
    "href": "code/02_models/02_collinearity_filter.html",
    "title": "Appendix G — Filter collinear explanatory variables",
    "section": "",
    "text": "Some explanatory variables, primarily those coming from morphometrics, may be collinear within our limited area of interest. Those shall be removed prior modelling.\nThis notebook identifies correlations between variables and drops those that are correlated and less interpetable.\n\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport numpy as np\n\nSpecify a path to the data folder.\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"geo_code\",\n        \"geometry\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n    ]\n)\n\nMeasure Pearson’s and Spearman’s Rank correllations.\n\npearson = exvars.corr().abs()\nspearman = exvars.corr(\"spearman\").abs()\n\nFill the upper triangle to keep each pair only once.\n\npearson *= np.tri(*pearson.shape)\n\nRemove self.\n\nnp.fill_diagonal(pearson.values, 0)\n\nUnstack to get pairs.\n\npearson_pairs = pearson.unstack()\npearson_pairs[pearson_pairs &gt; 0.8]\n\nsdbAre  sdbPer    0.804675\nsdbPer  ssbCor    0.831300\nssbCCo  ssbElo    0.881746\nstbOri  stcOri    0.861596\nsdcLAL  sdcAre    0.897443\n        mtbNDi    0.925586\n        mdcAre    0.894625\n        sddAre    0.805920\n        ltcAre    0.821140\nsdcAre  mtbNDi    0.905940\n        mdcAre    0.946279\n        sddAre    0.875737\n        mdsAre    0.806880\nmtbNDi  mdcAre    0.922637\n        sddAre    0.841932\n        ltcAre    0.844478\nmdcAre  sddAre    0.886339\n        sdsAre    0.848497\n        mdsAre    0.853500\n        ltcAre    0.922004\nltcWRE  lcnClo    0.852001\nltbIBD  ltcAre    0.867520\nsdsSPW  sdsSPO    0.840288\nsdsLen  mtdMDi    0.937603\n        sdsAre    0.859295\nlcdMes  linPDE    0.862986\nmtdMDi  sddAre    0.805111\n        sdsAre    0.849652\n        mdsAre    0.811618\nlddNDe  linWID    0.944004\nsddAre  sdsAre    0.925253\n        mdsAre    0.889015\n        ltcAre    0.813191\nsdsAre  mdsAre    0.955687\n        ldsAre    0.883652\n        ltcAre    0.829096\nmisCel  lisCel    0.869592\nmdsAre  ldsAre    0.956400\n        ltcAre    0.837981\nldeAre  ldePer    0.872630\nldePer  lseCWA    0.964556\ndtype: float64\n\n\nThe same with Spearman\n\nspearman *= np.tri(*spearman.shape)\nnp.fill_diagonal(spearman.values, 0)\nspearman_pairs = spearman.unstack()\nspearman_pairs[spearman_pairs &gt; 0.8]\n\nG, I. Distribution, hotels and restaurants  K, L, M, N. Financial, real estate, professional and administrative activities    0.819823\n                                            R, S, T, U. Other                                                                 0.811958\nsdbAre                                      sdbPer                                                                            0.961472\n                                            ssbCCM                                                                            0.889109\nsdbPer                                      ssbCCM                                                                            0.951864\nssbCCo                                      ssbElo                                                                            0.871556\nssbCor                                      ssbCCD                                                                            0.901580\nstbOri                                      stcOri                                                                            0.858329\nsdcLAL                                      sdcAre                                                                            0.960952\n                                            mtcWNe                                                                            0.918613\n                                            mdcAre                                                                            0.928993\n                                            ltcRea                                                                            0.832384\n                                            ltcAre                                                                            0.830742\nsdcAre                                      mtcWNe                                                                            0.873712\n                                            mdcAre                                                                            0.957952\n                                            ltcRea                                                                            0.822674\n                                            ltcAre                                                                            0.841574\nmtcWNe                                      mdcAre                                                                            0.845915\nmdcAre                                      ltbIBD                                                                            0.822638\n                                            ltcRea                                                                            0.828665\n                                            ltcAre                                                                            0.933400\nltcWRE                                      lcnClo                                                                            0.815142\nltbIBD                                      ltcAre                                                                            0.862608\nsdsSPW                                      sdsSPO                                                                            0.821366\nsdsLen                                      mtdMDi                                                                            0.925780\nlcdMes                                      linPDE                                                                            0.890219\nlinPDE                                      ldsCDL                                                                            0.810251\nmtdMDi                                      sddAre                                                                            0.823618\nlddNDe                                      linWID                                                                            0.893646\nsddAre                                      sdsAre                                                                            0.905564\n                                            mdsAre                                                                            0.859780\nsdsAre                                      mdsAre                                                                            0.946564\n                                            ldsAre                                                                            0.809841\nmisCel                                      lisCel                                                                            0.909656\nmdsAre                                      ldsAre                                                                            0.916757\nldeAre                                      ldePer                                                                            0.971434\n                                            lseCWA                                                                            0.899233\nldePer                                      lseCWA                                                                            0.965517\nlseERI                                      lseCWA                                                                            0.812116\ndtype: float64\n\n\nCombine highly correlated pairs from both. We want to eliminate those with both indices above .8.\n\nhigh_both = (\n    pearson_pairs[pearson_pairs &gt; 0.8]\n    .to_frame(\"pearson\")\n    .assign(spearman=spearman_pairs[spearman_pairs &gt; 0.8])\n    .dropna()\n)\nhigh_both\n\n\n\n\n\n\n\n\n\npearson\nspearman\n\n\n\n\nsdbAre\nsdbPer\n0.804675\n0.961472\n\n\nssbCCo\nssbElo\n0.881746\n0.871556\n\n\nstbOri\nstcOri\n0.861596\n0.858329\n\n\nsdcLAL\nsdcAre\n0.897443\n0.960952\n\n\nmdcAre\n0.894625\n0.928993\n\n\nltcAre\n0.821140\n0.830742\n\n\nsdcAre\nmdcAre\n0.946279\n0.957952\n\n\nmdcAre\nltcAre\n0.922004\n0.933400\n\n\nltcWRE\nlcnClo\n0.852001\n0.815142\n\n\nltbIBD\nltcAre\n0.867520\n0.862608\n\n\nsdsSPW\nsdsSPO\n0.840288\n0.821366\n\n\nsdsLen\nmtdMDi\n0.937603\n0.925780\n\n\nlcdMes\nlinPDE\n0.862986\n0.890219\n\n\nmtdMDi\nsddAre\n0.805111\n0.823618\n\n\nlddNDe\nlinWID\n0.944004\n0.893646\n\n\nsddAre\nsdsAre\n0.925253\n0.905564\n\n\nmdsAre\n0.889015\n0.859780\n\n\nsdsAre\nmdsAre\n0.955687\n0.946564\n\n\nldsAre\n0.883652\n0.809841\n\n\nmisCel\nlisCel\n0.869592\n0.909656\n\n\nmdsAre\nldsAre\n0.956400\n0.916757\n\n\nldeAre\nldePer\n0.872630\n0.971434\n\n\nldePer\nlseCWA\n0.964556\n0.965517\n\n\n\n\n\n\n\nDefine variables to be dropped.\n\nto_drop = [\n    \"sdbPer\",\n    \"ssbElo\",\n    \"stcOri\",\n    \"sdcLAL\",\n    \"mdcAre\",\n    \"ltcAre\",\n    \"ltcWRE\",\n    \"mtdMDi\",\n    \"lcdMes\",\n    \"lddNDe\",\n    \"sddAre\",\n    \"mdsAre\",\n    \"ldsAre\",\n    \"lisCel\",\n    \"ldePer\",\n    \"lseCWA\",\n]\n\nCheck the result.\n\npearson_check = exvars.drop(columns=to_drop).corr().abs()\nspearman_check = exvars.drop(columns=to_drop).corr(\"spearman\").abs()\n\npearson_check *= np.tri(*pearson_check.shape)\nnp.fill_diagonal(pearson_check.values, 0)\npearson_check_pairs = pearson_check.unstack()\n\nspearman_check *= np.tri(*spearman_check.shape)\nnp.fill_diagonal(spearman_check.values, 0)\nspearman_check_pairs = spearman_check.unstack()\n\nhigh_both_check = (\n    pearson_check_pairs[pearson_check_pairs &gt; 0.8]\n    .to_frame(\"pearson\")\n    .assign(spearman=spearman_check_pairs[spearman_check_pairs &gt; 0.8])\n    .dropna()\n)\n\n\nhigh_both_check\n\n\n\n\n\n\n\n\n\npearson\nspearman\n\n\n\n\nsdsSPW\nsdsSPO\n0.840288\n0.821366\n\n\n\n\n\n\n\nThe street profile width - street profile openness pair is kept as there is not necessarily a logical (only empirical) relation between the two.\nCheck remaining high-correlation pairs if we consider only single index.\n\npearson_check_pairs[pearson_check_pairs &gt; 0.8]\n\nsdcAre  mtbNDi    0.905940\nsdsSPW  sdsSPO    0.840288\nsdsLen  sdsAre    0.859295\ndtype: float64\n\n\n\nspearman_check_pairs[spearman_check_pairs &gt; 0.8]\n\nG, I. Distribution, hotels and restaurants  K, L, M, N. Financial, real estate, professional and administrative activities    0.819823\n                                            R, S, T, U. Other                                                                 0.811958\nsdbAre                                      ssbCCM                                                                            0.889109\nssbCor                                      ssbCCD                                                                            0.901580\nsdcAre                                      mtcWNe                                                                            0.873712\n                                            ltcRea                                                                            0.822674\nsdsSPW                                      sdsSPO                                                                            0.821366\nlinPDE                                      ldsCDL                                                                            0.810251\ndtype: float64\n\n\nWe assume that these can stay in the dataset.\nDrop the collinear variables from original data.\n\ndata = data.drop(columns=to_drop)\n\nSave to file.\n\ndata.to_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nGet a table of all.\nKey to names of morphometric characters:\n\nkey = {\n    \"sdbAre\": \"area of building\",\n    \"sdbPer\": \"perimeter of building\",\n    \"sdbCoA\": \"courtyard area of building\",\n    \"ssbCCo\": \"circular compactness of building\",\n    \"ssbCor\": \"corners of building\",\n    \"ssbSqu\": \"squareness of building\",\n    \"ssbERI\": \"equivalent rectangular index of building\",\n    \"ssbElo\": \"elongation of building\",\n    \"ssbCCM\": \"centroid - corner mean distance of building\",\n    \"ssbCCD\": \"centroid - corner distance deviation of building\",\n    \"stbOri\": \"orientation of building\",\n    \"sdcLAL\": \"longest axis length of ETC\",\n    \"sdcAre\": \"area of ETC\",\n    \"sscCCo\": \"circular compactness of ETC\",\n    \"sscERI\": \"equivalent rectangular index of ETC\",\n    \"stcOri\": \"orientation of ETC\",\n    \"sicCAR\": \"covered area ratio of ETC\",\n    \"stbCeA\": \"cell alignment of building\",\n    \"mtbAli\": \"alignment of neighbouring buildings\",\n    \"mtbNDi\": \"mean distance between neighbouring buildings\",\n    \"mtcWNe\": \"perimeter-weighted neighbours of ETC\",\n    \"mdcAre\": \"area covered by neighbouring cells\",\n    \"ltcWRE\": \"weighted reached enclosures of ETC\",\n    \"ltbIBD\": \"mean inter-building distance\",\n    \"sdsSPW\": \"width of street profile\",\n    \"sdsSWD\": \"width deviation of street profile\",\n    \"sdsSPO\": \"openness of street profile\",\n    \"sdsLen\": \"length of street segment\",\n    \"sssLin\": \"linearity of street segment\",\n    \"ldsMSL\": \"mean segment length within 3 steps\",\n    \"mtdDeg\": \"node degree of junction\",\n    \"lcdMes\": \"local meshedness of street network\",\n    \"linP3W\": \"local proportion of 3-way intersections of street network\",\n    \"linP4W\": \"local proportion of 4-way intersections of street network\",\n    \"linPDE\": \"local proportion of cul-de-sacs of street network\",\n    \"lcnClo\": \"local closeness of street network\",\n    \"ldsCDL\": \"local cul-de-sac length of street network\",\n    \"xcnSCl\": \"square clustering of street network\",\n    \"mtdMDi\": \"mean distance to neighbouring nodes of street network\",\n    \"lddNDe\": \"local node density of street network\",\n    \"linWID\": \"local degree weighted node density of street network\",\n    \"stbSAl\": \"street alignment of building\",\n    \"sddAre\": \"area covered by node-attached ETCs\",\n    \"sdsAre\": \"area covered by edge-attached ETCs\",\n    \"sisBpM\": \"buildings per meter of street segment\",\n    \"misCel\": \"reached ETCs by neighbouring segments\",\n    \"mdsAre\": \"reached area by neighbouring segments\",\n    \"lisCel\": \"reached ETCs by local street network\",\n    \"ldsAre\": \"reached area by local street network\",\n    \"ltcRea\": \"reached ETCs by tessellation contiguity\",\n    \"ltcAre\": \"reached area by tessellation contiguity\",\n    \"ldeAre\": \"area of enclosure\",\n    \"ldePer\": \"perimeter of enclosure\",\n    \"lseCCo\": \"circular compactness of enclosure\",\n    \"lseERI\": \"equivalent rectangular index of enclosure\",\n    \"lseCWA\": \"compactness-weighted axis of enclosure\",\n    \"lteOri\": \"orientation of enclosure\",\n    \"lteWNB\": \"perimeter-weighted neighbours of enclosure\",\n    \"lieWCe\": \"area-weighted ETCs of enclosure\",\n}\n\nList all characters with names:\n\n[key[c] if c in key else c for c in exvars.drop(columns=to_drop).columns]\n\n['population_estimate',\n 'A, B, D, E. Agriculture, energy and water',\n 'C. Manufacturing',\n 'F. Construction',\n 'G, I. Distribution, hotels and restaurants',\n 'H, J. Transport and communication',\n 'K, L, M, N. Financial, real estate, professional and administrative activities',\n 'O,P,Q. Public administration, education and health',\n 'R, S, T, U. Other',\n 'Land cover [Discontinuous urban fabric]',\n 'Land cover [Continuous urban fabric]',\n 'Land cover [Non-irrigated arable land]',\n 'Land cover [Industrial or commercial units]',\n 'Land cover [Green urban areas]',\n 'Land cover [Pastures]',\n 'Land cover [Sport and leisure facilities]',\n 'area of building',\n 'courtyard area of building',\n 'circular compactness of building',\n 'corners of building',\n 'squareness of building',\n 'equivalent rectangular index of building',\n 'centroid - corner mean distance of building',\n 'centroid - corner distance deviation of building',\n 'orientation of building',\n 'area of ETC',\n 'circular compactness of ETC',\n 'equivalent rectangular index of ETC',\n 'covered area ratio of ETC',\n 'cell alignment of building',\n 'alignment of neighbouring buildings',\n 'mean distance between neighbouring buildings',\n 'perimeter-weighted neighbours of ETC',\n 'mean inter-building distance',\n 'width of street profile',\n 'width deviation of street profile',\n 'openness of street profile',\n 'length of street segment',\n 'linearity of street segment',\n 'mean segment length within 3 steps',\n 'node degree of junction',\n 'local proportion of 3-way intersections of street network',\n 'local proportion of 4-way intersections of street network',\n 'local proportion of cul-de-sacs of street network',\n 'local closeness of street network',\n 'local cul-de-sac length of street network',\n 'square clustering of street network',\n 'local degree weighted node density of street network',\n 'street alignment of building',\n 'area covered by edge-attached ETCs',\n 'buildings per meter of street segment',\n 'reached ETCs by neighbouring segments',\n 'reached ETCs by tessellation contiguity',\n 'area of enclosure',\n 'circular compactness of enclosure',\n 'equivalent rectangular index of enclosure',\n 'orientation of enclosure',\n 'perimeter-weighted neighbours of enclosure',\n 'area-weighted ETCs of enclosure']"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#air-quality",
    "href": "code/02_models/03_england-wide-data.html#air-quality",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.1 Air Quality",
    "text": "H.1 Air Quality\nRead DEFRA data.\n\npm10_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm102021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npm25_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mappm252021g.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nno2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapno22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\nso2_21 = (\n    pd.read_csv(\n        \"https://uk-air.defra.gov.uk/datastore/pcm/mapso22021.csv\",\n        header=5,\n        na_values=[\"MISSING\"],\n    )\n    .set_index([\"x\", \"y\"])\n    .drop(columns=\"gridcode\")\n    .to_xarray()\n)\npollutants_2021 = xr.merge([pm10_21, pm25_21, no2_21, so2_21])\npollutants_2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (x: 657, y: 1170)\nCoordinates:\n  * x          (x) int64 -500 500 1500 2500 3500 ... 652500 653500 654500 655500\n  * y          (y) int64 5500 6500 7500 8500 ... 1216500 1217500 1218500 1219500\nData variables:\n    pm102021g  (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nan\n    pm252021g  (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nan\n    no22021    (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nan\n    so22021    (x, y) float64 nan nan nan nan nan nan ... nan nan nan nan nanxarray.DatasetDimensions:x: 657y: 1170Coordinates: (2)x(x)int64-500 500 1500 ... 654500 655500array([  -500,    500,   1500, ..., 653500, 654500, 655500])y(y)int645500 6500 7500 ... 1218500 1219500array([   5500,    6500,    7500, ..., 1217500, 1218500, 1219500])Data variables: (4)pm102021g(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])pm252021g(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])no22021(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])so22021(x, y)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])Indexes: (2)xPandasIndexPandasIndex(Int64Index([  -500,    500,   1500,   2500,   3500,   4500,   5500,   6500,\n              7500,   8500,\n            ...\n            646500, 647500, 648500, 649500, 650500, 651500, 652500, 653500,\n            654500, 655500],\n           dtype='int64', name='x', length=657))yPandasIndexPandasIndex(Int64Index([   5500,    6500,    7500,    8500,    9500,   10500,   11500,\n              12500,   13500,   14500,\n            ...\n            1210500, 1211500, 1212500, 1213500, 1214500, 1215500, 1216500,\n            1217500, 1218500, 1219500],\n           dtype='int64', name='y', length=1170))Attributes: (0)\n\n\nWe can compute the index based on the formula.\n\naqi = (\n    pollutants_2021.pm252021g\n    + pollutants_2021.pm102021g / 2\n    + pollutants_2021.no22021 / 4\n    + pollutants_2021.so22021 / 10\n)\npollutants_2021 = pollutants_2021.assign(aqi=aqi)\n\nWe convert the array to a GeoDataFrame with polygons representing grid area. This will be needed for areal interpolation to LSOA/MSOA.\n\npollutants_2021 = pollutants_2021.to_dataframe().reset_index()\npollutants_2021 = gpd.GeoDataFrame(\n    pollutants_2021,\n    geometry=gpd.points_from_xy(pollutants_2021.x, pollutants_2021.y, crs=27700).buffer(\n        500, cap_style=3\n    ),\n)\n\n\npollutants_2021.to_parquet(\n    f\"{data_folder}/processed/air_quality/air_quality_grid_2021_england.parquet\"\n)\n\n\npollutants_2021\n\n\n\n\n\n\n\n\nx\ny\npm102021g\npm252021g\nno22021\nso22021\naqi\ngeometry\n\n\n\n\n0\n-500\n5500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 6000.000, 0.000 5000.000, -100...\n\n\n1\n-500\n6500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 7000.000, 0.000 6000.000, -100...\n\n\n2\n-500\n7500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 8000.000, 0.000 7000.000, -100...\n\n\n3\n-500\n8500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 9000.000, 0.000 8000.000, -100...\n\n\n4\n-500\n9500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((0.000 10000.000, 0.000 9000.000, -10...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n768685\n655500\n1215500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1216000.000, 656000.000 1...\n\n\n768686\n655500\n1216500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1217000.000, 656000.000 1...\n\n\n768687\n655500\n1217500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1218000.000, 656000.000 1...\n\n\n768688\n655500\n1218500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1219000.000, 656000.000 1...\n\n\n768689\n655500\n1219500\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((656000.000 1220000.000, 656000.000 1...\n\n\n\n\n768690 rows × 8 columns\n\n\n\nInterpolate to OAs.\n\n%%time\ninterp = tobler.area_weighted.area_interpolate(\n    pollutants_2021, oa, intensive_variables=[\"aqi\"]\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/pygeos/set_operations.py:129: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/tobler/util/util.py:32: UserWarning: nan values in variable: aqi, replacing with 0\n  warn(f\"nan values in variable: {column}, replacing with 0\")\n\n\nCPU times: user 17.2 s, sys: 215 ms, total: 17.4 s\nWall time: 17.4 s\n\n\n\noa[\"air_quality\"] = interp.aqi"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#house-price",
    "href": "code/02_models/03_england-wide-data.html#house-price",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.2 House price",
    "text": "H.2 House price\nRead the same dataset as before.\n\n# It is 6.3GB...\nlinked_epc_path = \"https://reshare.ukdataservice.ac.uk/854942/1/tranall2011_19.csv\"\n\nepc = pd.read_csv(linked_epc_path)\nepc[\"dateoftransfer\"] = pd.to_datetime(epc.dateoftransfer)\nlast2years = epc[epc.dateoftransfer &gt; datetime.datetime(2018, 1, 1)]\n\nprice_per_oa = last2years[[\"oa11\", \"priceper\"]].groupby(\"oa11\").mean().reset_index()\nprice_per_oa.to_parquet(f\"{data_folder}/processed/house_prices/price_per_oa.parquet\")\n\n\nprice_per_oa = pd.read_parquet(\n    f\"{data_folder}/processed/house_prices/price_per_oa.parquet\"\n)\n\n\nprice_per_oa\n\n\n\n\n\n\n\n\noa11\npriceper\n\n\n\n\n0\nE00000003\n15722.758094\n\n\n1\nE00000005\n12587.314452\n\n\n2\nE00000007\n12640.262294\n\n\n3\nE00000010\n9535.653363\n\n\n4\nE00000012\n10341.463415\n\n\n...\n...\n...\n\n\n173553\nW00010260\n1632.955752\n\n\n173554\nW00010261\n1834.401969\n\n\n173555\nW00010262\n1511.677538\n\n\n173556\nW00010263\n1679.802956\n\n\n173557\nW00010264\n2046.069031\n\n\n\n\n173558 rows × 2 columns\n\n\n\n\noa\n\n\n\n\n\n\n\n\nOA11CD\nLAD11CD\nGlobalID\ngeometry\nair_quality\n\n\n\n\n0\nE00000001\nE09000001\nf8512cce-6727-42cf-9840-6866cdbb2deb\nPOLYGON ((532303.492 181814.110, 532213.378 18...\n27.501372\n\n\n1\nE00000003\nE09000001\n9eeeb3aa-ce92-4cea-bd70-3a0e680cddc1\nPOLYGON ((532180.131 181763.020, 532155.909 18...\n27.501372\n\n\n2\nE00000005\nE09000001\n012372dd-5e03-43c3-a915-b697d02f88e2\nPOLYGON ((532124.321 181682.675, 532127.299 18...\n27.501372\n\n\n3\nE00000007\nE09000001\nb61eb464-9c5b-4f0e-8f78-000ea938ee78\nPOLYGON ((532124.321 181682.675, 532201.292 18...\n27.545142\n\n\n4\nE00000010\nE09000001\nefc37450-7064-4f5e-bedd-433bf4de1167\nPOLYGON ((532071.302 182159.586, 532127.958 18...\n27.587650\n\n\n...\n...\n...\n...\n...\n...\n\n\n181403\nW00010261\nW06000011\nc1051d13-4fa6-48d8-8f32-5020938ce7d6\nPOLYGON ((262156.208 196600.223, 262074.703 19...\n13.828992\n\n\n181404\nW00010262\nW06000011\nea1fef0a-138d-4035-a401-48c459abbca0\nPOLYGON ((263241.217 197440.210, 263271.904 19...\n13.670047\n\n\n181405\nW00010263\nW06000011\n38819942-d3db-47e1-a85d-eec4d7a091e4\nPOLYGON ((262156.208 196600.223, 262205.269 19...\n14.125627\n\n\n181406\nW00010264\nW06000011\n61fbb0fb-10bf-4000-a669-7114865776ff\nPOLYGON ((268829.001 198038.000, 268708.179 19...\n14.219191\n\n\n181407\nW00010265\nW06000011\n999facfa-dce5-42bf-82fe-c1f04221661e\nPOLYGON ((266130.758 192630.558, 265987.755 19...\n14.927127\n\n\n\n\n181408 rows × 5 columns\n\n\n\nLink to our OA dataset.\n\noa = oa.merge(price_per_oa, left_on=\"OA11CD\", right_on=\"oa11\", how=\"left\")\noa = oa.drop(columns=\"oa11\").rename(columns={\"priceper\": \"house_price\"})\noa[[\"OA11CD\", \"geometry\", \"air_quality\", \"house_price\"]].to_parquet(\n    f\"{data_folder}/processed/oa_data_england.parquet\"\n)"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#explanatory-variables",
    "href": "code/02_models/03_england-wide-data.html#explanatory-variables",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.3 Explanatory variables",
    "text": "H.3 Explanatory variables\nWe also want to have all our explanatory variables linked to the OA geometries.\n\nH.3.1 Population estimates\nONS population estimates are reported on the OA level and can be merged.\nRead the file processed in the Urban Grammar project.\n\npop = pd.read_parquet(f\"{data_folder}/processed/population.parquet\")\n\nAttribute join.\n\noa = oa.merge(pop, left_on=\"OA11CD\", right_on=\"code\", how=\"left\")\n\n\noa = oa.drop(columns=[\"code\"])\n\n\n\nH.3.2 Workplace population\nWorkplace population is reported on Workplace Zones and needs to be interpolated to OA. We use the preprocessed data from the Urban Grammar project.\n\nwp = gpd.read_parquet(\n    f\"{data_folder}/raw/workplace_population/workplace_by_industry_gb.pq\"\n)\n\n\n%%time\nwp_interpolated = tobler.area_weighted.area_interpolate(\n    wp,\n    oa,\n    extensive_variables=[\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ],\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/pygeos/set_operations.py:129: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nCPU times: user 31.5 s, sys: 351 ms, total: 31.9 s\nWall time: 32 s\n\n\n\noa[\n    [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ]\n] = wp_interpolated.drop(columns=\"geometry\").values\n\n\noa = oa.drop(columns=[\"LAD11CD\", \"GlobalID\"])\n\n\noa.to_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")\n\n\n\nH.3.3 Land cover (CORINE)\nCORINE is shipped as custom polygons. We use the data downloaded for the Urban Grammar project.\n\ncorine = gpd.read_parquet(f\"{data_folder}/raw/land_cover/corine_gb.pq\")\n\n\n%%time\ncorine_interpolated = tobler.area_weighted.area_interpolate(\n    corine, oa, categorical_variables=[\"Code_18\"]\n)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/pygeos/set_operations.py:129: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n\n\nCPU times: user 22min 56s, sys: 5.49 s, total: 23min 1s\nWall time: 23min 4s\n\n\n\ncorine_names = {\n    \"Code_18_124\": \"Land cover [Airports]\",\n    \"Code_18_211\": \"Land cover [Non-irrigated arable land]\",\n    \"Code_18_121\": \"Land cover [Industrial or commercial units]\",\n    \"Code_18_421\": \"Land cover [Salt marshes]\",\n    \"Code_18_522\": \"Land cover [Estuaries]\",\n    \"Code_18_142\": \"Land cover [Sport and leisure facilities]\",\n    \"Code_18_141\": \"Land cover [Green urban areas]\",\n    \"Code_18_112\": \"Land cover [Discontinuous urban fabric]\",\n    \"Code_18_231\": \"Land cover [Pastures]\",\n    \"Code_18_311\": \"Land cover [Broad-leaved forest]\",\n    \"Code_18_131\": \"Land cover [Mineral extraction sites]\",\n    \"Code_18_123\": \"Land cover [Port areas]\",\n    \"Code_18_122\": \"Land cover [Road and rail networks and associated land]\",\n    \"Code_18_512\": \"Land cover [Water bodies]\",\n    \"Code_18_243\": \"Land cover [Land principally occupied by agriculture, with significant areas of natural vegetation]\",\n    \"Code_18_313\": \"Land cover [Mixed forest]\",\n    \"Code_18_412\": \"Land cover [Peat bogs]\",\n    \"Code_18_321\": \"Land cover [Natural grasslands]\",\n    \"Code_18_322\": \"Land cover [Moors and heathland]\",\n    \"Code_18_324\": \"Land cover [Transitional woodland-shrub]\",\n    \"Code_18_111\": \"Land cover [Continuous urban fabric]\",\n    \"Code_18_423\": \"Land cover [Intertidal flats]\",\n    \"Code_18_523\": \"Land cover [Sea and ocean]\",\n    \"Code_18_312\": \"Land cover [Coniferous forest]\",\n    \"Code_18_133\": \"Land cover [Construction sites]\",\n    \"Code_18_333\": \"Land cover [Sparsely vegetated areas]\",\n    \"Code_18_332\": \"Land cover [Bare rocks]\",\n    \"Code_18_411\": \"Land cover [Inland marshes]\",\n    \"Code_18_132\": \"Land cover [Dump sites]\",\n    \"Code_18_222\": \"Land cover [Fruit trees and berry plantations]\",\n    \"Code_18_242\": \"Land cover [Complex cultivation patterns]\",\n    \"Code_18_331\": \"Land cover [Beaches, dunes, sands]\",\n    \"Code_18_511\": \"Land cover [Water courses]\",\n    \"Code_18_334\": \"Land cover [Burnt areas]\",\n    \"Code_18_244\": \"Land cover [Agro-forestry areas]\",\n    \"Code_18_521\": \"Land cover [Coastal lagoons]\",\n}\n\ncorine_interpolated.columns = corine_interpolated.columns.map(corine_names)\n\nAssign the subset of classes that may be interesting for our purposes to OA dataframe.\n\ninteresting = [\n    \"Land cover [Discontinuous urban fabric]\",\n    \"Land cover [Continuous urban fabric]\",\n    \"Land cover [Non-irrigated arable land]\",\n    \"Land cover [Industrial or commercial units]\",\n    \"Land cover [Green urban areas]\",\n    \"Land cover [Pastures]\",\n    \"Land cover [Sport and leisure facilities]\",\n]\noa[interesting] = corine_interpolated[interesting].values\n\n\noa.to_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#morphometrics",
    "href": "code/02_models/03_england-wide-data.html#morphometrics",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.4 Morphometrics",
    "text": "H.4 Morphometrics\nGetting the file pre-processed based on the Urban Grammar\n\noa_morpho = pd.read_parquet(f\"{data_folder}/processed/morphometrics_oa.parquet\")\noa = oa.merge(oa_morpho, left_on=\"OA11CD\", right_index=True)\n\n\noa.to_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")"
  },
  {
    "objectID": "code/02_models/03_england-wide-data.html#cleanup",
    "href": "code/02_models/03_england-wide-data.html#cleanup",
    "title": "Appendix H — Training data sampled from the whole England",
    "section": "H.5 Cleanup",
    "text": "H.5 Cleanup\nDrop unusable rows\n\noa = gpd.read_parquet(f\"{data_folder}/processed/oa_data_england.parquet\")\n\n\nto_drop = [\n    \"sdbPer\",\n    \"ssbElo\",\n    \"stcOri\",\n    \"sdcLAL\",\n    \"mdcAre\",\n    \"ltcAre\",\n    \"ltcWRE\",\n    \"mtdMDi\",\n    \"lcdMes\",\n    \"lddNDe\",\n    \"sddAre\",\n    \"mdsAre\",\n    \"ldsAre\",\n    \"lisCel\",\n    \"ldePer\",\n    \"lseCWA\",\n]\n\noa = oa.drop(columns=to_drop)\n\n\noa_clean = oa.dropna()\n\nRemove London data as London is a massive outlier that throws everything off.\n\nlondon = gpd.read_file(f\"{data_folder}/raw/london/OA_2011_London_gen_MHW.shp\")\n\n\noa_clean = oa_clean[~oa_clean.OA11CD.isin(london.OA11CD)]\n\n\noa_clean.set_index(\"OA11CD\").to_parquet(\n    f\"{data_folder}/processed/oa_data_england.parquet\"\n)"
  },
  {
    "objectID": "code/02_models/04a_air_quality_model_exploration.html#using-spatial-lag",
    "href": "code/02_models/04a_air_quality_model_exploration.html#using-spatial-lag",
    "title": "Appendix I — Air Quality prediction model",
    "section": "I.1 Using spatial lag",
    "text": "I.1 Using spatial lag\nWe can add a lag into the mix. Let’s start with the Queen weights.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nsdsAre_lag\nsisBpM_lag\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n30280.039796\n0.064310\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n17776.067815\n0.054231\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n54958.015580\n0.058214\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n78050.340373\n0.062603\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n27462.223773\n0.067705\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n\n\n\n\n5 rows × 118 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars, data.air_quality_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 11min 1s, sys: 12min 4s, total: 23min 5s\nWall time: 14min 16s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.5538384955550043\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data.index)\nresiduals_lag = data.air_quality_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_lag)\n\n0.9191784959862161\n\n\nMSE got down from 1.34 to 0.92, so there’s a clear gain in including the lag.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nThe over and underprediction tendency still remains but the error is lower.\nThe main underpredicted area is south of the city along the highway - I belive that is due to relatively open OAs and no information on the highway in the explanatory variables.\nThe main overprediction happens in the green belt - there’s very few of these low values so the K-fold modelling exercise likely does not have enough low-value observations to learn it.\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nOverlay of original and a lagged model. We can see that the lagged one is more precise also here.\n\nax = residuals_lag.plot.hist(bins=50)\nresiduals.plot.hist(bins=50, ax=ax, color=\"r\", alpha=0.7)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThe average residuals:\n\nresiduals_lag.abs().mean(), residuals.abs().mean()\n\n(0.7127002931466176, 0.8861835077899286)\n\n\nInterestingly enough, both options have the same parameters of the best model.\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\n\nest.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\n\nI.1.1 Wider weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen3 = libpysal.weights.higher_order(queen, k=3, lower_order=True)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nqueen3.transform = \"R\"\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen3, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nsdsAre_lag\nsisBpM_lag\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n30280.039796\n0.064310\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n17776.067815\n0.054231\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n54958.015580\n0.058214\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n78050.340373\n0.062603\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n27462.223773\n0.067705\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n\n\n\n\n5 rows × 118 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars, data.air_quality_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 10min 48s, sys: 11min 32s, total: 22min 21s\nWall time: 13min 59s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.76202775278368\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data.index)\nresiduals_lag = data.air_quality_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_lag)\n\n0.5447792769804048\n\n\n\nresiduals_lag.abs().mean()\n\n0.51491983590584\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_lag,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "code/02_models/04a_air_quality_model_exploration.html#with-latent-representation-of-sentinel-2",
    "href": "code/02_models/04a_air_quality_model_exploration.html#with-latent-representation-of-sentinel-2",
    "title": "Appendix I — Air Quality prediction model",
    "section": "I.2 With latent representation of Sentinel 2",
    "text": "I.2 With latent representation of Sentinel 2\nTry including the lagged latent representation from the postcode Sentinel paper.\n\nlatent_oa = pd.read_parquet(f\"{data_folder}/processed/sentinel/latent_oa.parquet\")\n\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nexvars_latent = pd.concat([exvars, latent_oa.drop(columns=\"geometry\")], axis=1)\n\n\nfor col in exvars_latent.columns.copy():\n    exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n        queen, exvars_latent[col]\n    )\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54819/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n\n\n\nexvars_latent = exvars_latent.copy()\n\n\nparameters = {\n    \"learning_rate\": (0.01, 0.05, 0.1),\n    \"max_iter\": [500],\n    \"max_bins\": (64, 128),\n}\n\nInitiate GridSearchCV with Histogram-based Gradient Boosting Regression Tree.\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_latent = GridSearchCV(regressor, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_latent.fit(exvars_latent, data.air_quality_index)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\nCPU times: user 3min 14s, sys: 2min 35s, total: 5min 49s\nWall time: 3min 26s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_latent.best_estimator_\n\nHistGradientBoostingRegressor(max_bins=128, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(max_bins=128, max_iter=500, random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_latent.best_score_\n\n0.6205192288572696\n\n\n\ny_pred_latent = cross_val_predict(\n    est_latent.best_estimator_, exvars_latent, data.air_quality_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_latent = pd.Series(y_pred_latent, index=data.index)\nresiduals_latent = data.air_quality_index - pred_latent\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_latent)\n\n0.8272652037619179\n\n\nMSE got down from 1.34 to 0.92, so there’s a clear gain in including the lag.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_latent, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_latent).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_latent,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_latent).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_latent.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\n\nI.2.1 Only Sentinel2\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_s2 = GridSearchCV(regressor, parameters, verbose=1)\n\n\n%%time\nest_s2.fit(latent_oa.drop(columns=\"geometry\"), data.air_quality_index)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\nCPU times: user 1min 32s, sys: 2min 19s, total: 3min 52s\nWall time: 2min 36s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_s2.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_s2.best_score_\n\n0.23497429291310237\n\n\n\ny_pred_s2 = cross_val_predict(\n    est_s2.best_estimator_,\n    latent_oa.drop(columns=\"geometry\"),\n    data.air_quality_index,\n    cv=5,\n)\n\nGet the prediction and residuals.\n\npred_s2 = pd.Series(y_pred_s2, index=data.index)\nresiduals_s2 = data.air_quality_index - pred_s2\n\nCheck the error\n\nmean_squared_error(data.air_quality_index, pred_s2)\n\n1.570113449347073\n\n\nMSE got down from 1.34 to 0.92, so there’s a clear gain in including the lag.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, pred_s2, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=pred_s2).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_s2,\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_s2).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")"
  },
  {
    "objectID": "code/02_models/04b_air_quality_model_search.html",
    "href": "code/02_models/04b_air_quality_model_search.html",
    "title": "Appendix J — Air Quality prediction model search",
    "section": "",
    "text": "Loop and grid search for the optimal air quality model.\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\n\nimport contextily\nimport palettable.matplotlib as palmpl\nimport matplotlib.pyplot as plt\nimport mapclassify\nimport libpysal\n\nfrom utils import legendgram\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_14176/881383173.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas as gpd\n\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"geo_code\",\n        \"geometry\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n        \"greenspace_accessibility_index\",\n    ]\n)\n\nSpecify grid search parameters. We can limit the options based on previous exploration.\n\nparameters = {\"learning_rate\": (0.05, 0.1), \"max_iter\": [500], \"max_bins\": (64, 128)}\n\nDefine the simple weights matrices.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nweights = {\n    \"queen\": queen,\n    \"queen2\": libpysal.weights.higher_order(queen, k=2, lower_order=True),\n    \"queen3\": libpysal.weights.higher_order(queen, k=3, lower_order=True),\n    \"queen4\": libpysal.weights.higher_order(queen, k=4, lower_order=True),\n    \"queen5\": libpysal.weights.higher_order(queen, k=5, lower_order=True),\n    \"500m\": libpysal.weights.DistanceBand.from_dataframe(data, 500),\n    \"1000m\": libpysal.weights.DistanceBand.from_dataframe(data, 1000),\n    \"2000m\": libpysal.weights.DistanceBand.from_dataframe(data, 2000),\n}\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 110 disconnected components.\n There are 82 islands with ids: 47, 71, 72, 89, 263, 361, 364, 375, 376, 377, 378, 541, 642, 983, 993, 1092, 1220, 1295, 1339, 1343, 1345, 1383, 1406, 1640, 1756, 1772, 1809, 1851, 1944, 1958, 2124, 2148, 2181, 2182, 2188, 2195, 2214, 2222, 2223, 2237, 2265, 2277, 2281, 2283, 2307, 2361, 2485, 2493, 2594, 2686, 2766, 2809, 2825, 2868, 2940, 2980, 3091, 3094, 3112, 3146, 3191, 3197, 3207, 3223, 3235, 3276, 3397, 3400, 3415, 3419, 3423, 3427, 3451, 3475, 3488, 3528, 3555, 3577, 3707, 3723, 3743, 3778.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 16 disconnected components.\n There are 12 islands with ids: 89, 377, 378, 1944, 2182, 2277, 2493, 2594, 2868, 3146, 3223, 3528.\n  warnings.warn(message)\n\n\nUse Grid Search CV to find the best model for each weights option.\n\nmeta = {}\nfor name, W in weights.items():\n    W.transform = \"r\"\n    exvars = data.drop(\n        columns=[\n            \"geo_code\",\n            \"geometry\",\n            \"air_quality_index\",\n            \"house_price_index\",\n            \"jobs_accessibility_index\",\n            \"greenspace_accessibility_index\",\n        ]\n    )\n    for col in exvars.columns.copy():\n        exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(W, exvars[col])\n    regressor_lag = HistGradientBoostingRegressor(\n        random_state=0,\n    )\n    est_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n    est_lag.fit(exvars, data.air_quality_index)\n    meta[name] = {\"score\": est_lag.best_score_}\n    y_pred_lag = cross_val_predict(\n        est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n    )\n    pred_lag = pd.Series(y_pred_lag, index=data.index)\n    residuals_lag = data.air_quality_index - pred_lag\n    meta[name][\"mse\"] = mean_squared_error(data.air_quality_index, pred_lag)\n    meta[name][\"me\"] = residuals_lag.abs().mean()\n    meta[name][\"prediction\"] = pred_lag\n    meta[name][\"residuals\"] = residuals_lag\n    meta[name][\"model\"] = est_lag.best_estimator_\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 47, ' is an island (no neighbors)')\n('WARNING: ', 71, ' is an island (no neighbors)')\n('WARNING: ', 72, ' is an island (no neighbors)')\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 263, ' is an island (no neighbors)')\n('WARNING: ', 361, ' is an island (no neighbors)')\n('WARNING: ', 364, ' is an island (no neighbors)')\n('WARNING: ', 375, ' is an island (no neighbors)')\n('WARNING: ', 376, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 541, ' is an island (no neighbors)')\n('WARNING: ', 642, ' is an island (no neighbors)')\n('WARNING: ', 983, ' is an island (no neighbors)')\n('WARNING: ', 993, ' is an island (no neighbors)')\n('WARNING: ', 1092, ' is an island (no neighbors)')\n('WARNING: ', 1220, ' is an island (no neighbors)')\n('WARNING: ', 1295, ' is an island (no neighbors)')\n('WARNING: ', 1339, ' is an island (no neighbors)')\n('WARNING: ', 1343, ' is an island (no neighbors)')\n('WARNING: ', 1345, ' is an island (no neighbors)')\n('WARNING: ', 1383, ' is an island (no neighbors)')\n('WARNING: ', 1406, ' is an island (no neighbors)')\n('WARNING: ', 1640, ' is an island (no neighbors)')\n('WARNING: ', 1756, ' is an island (no neighbors)')\n('WARNING: ', 1772, ' is an island (no neighbors)')\n('WARNING: ', 1809, ' is an island (no neighbors)')\n('WARNING: ', 1851, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 1958, ' is an island (no neighbors)')\n('WARNING: ', 2124, ' is an island (no neighbors)')\n('WARNING: ', 2148, ' is an island (no neighbors)')\n('WARNING: ', 2181, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2188, ' is an island (no neighbors)')\n('WARNING: ', 2195, ' is an island (no neighbors)')\n('WARNING: ', 2214, ' is an island (no neighbors)')\n('WARNING: ', 2222, ' is an island (no neighbors)')\n('WARNING: ', 2223, ' is an island (no neighbors)')\n('WARNING: ', 2237, ' is an island (no neighbors)')\n('WARNING: ', 2265, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2281, ' is an island (no neighbors)')\n('WARNING: ', 2283, ' is an island (no neighbors)')\n('WARNING: ', 2307, ' is an island (no neighbors)')\n('WARNING: ', 2361, ' is an island (no neighbors)')\n('WARNING: ', 2485, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2686, ' is an island (no neighbors)')\n('WARNING: ', 2766, ' is an island (no neighbors)')\n('WARNING: ', 2809, ' is an island (no neighbors)')\n('WARNING: ', 2825, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 2940, ' is an island (no neighbors)')\n('WARNING: ', 2980, ' is an island (no neighbors)')\n('WARNING: ', 3091, ' is an island (no neighbors)')\n('WARNING: ', 3094, ' is an island (no neighbors)')\n('WARNING: ', 3112, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3191, ' is an island (no neighbors)')\n('WARNING: ', 3197, ' is an island (no neighbors)')\n('WARNING: ', 3207, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3235, ' is an island (no neighbors)')\n('WARNING: ', 3276, ' is an island (no neighbors)')\n('WARNING: ', 3397, ' is an island (no neighbors)')\n('WARNING: ', 3400, ' is an island (no neighbors)')\n('WARNING: ', 3415, ' is an island (no neighbors)')\n('WARNING: ', 3419, ' is an island (no neighbors)')\n('WARNING: ', 3423, ' is an island (no neighbors)')\n('WARNING: ', 3427, ' is an island (no neighbors)')\n('WARNING: ', 3451, ' is an island (no neighbors)')\n('WARNING: ', 3475, ' is an island (no neighbors)')\n('WARNING: ', 3488, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\n('WARNING: ', 3555, ' is an island (no neighbors)')\n('WARNING: ', 3577, ' is an island (no neighbors)')\n('WARNING: ', 3707, ' is an island (no neighbors)')\n('WARNING: ', 3723, ' is an island (no neighbors)')\n('WARNING: ', 3743, ' is an island (no neighbors)')\n('WARNING: ', 3778, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nAdd combined weights on top.\n\ncombined_weigts = {\n    \"queen500m\": libpysal.weights.w_union(weights[\"queen\"], weights[\"500m\"]),\n    \"queen1000m\": libpysal.weights.w_union(weights[\"queen\"], weights[\"1000m\"]),\n    \"queen2000m\": libpysal.weights.w_union(weights[\"queen\"], weights[\"2000m\"]),\n}\n\nFind models.\n\nfor name, W in combined_weigts.items():\n    W.transform = \"r\"\n    exvars = data.drop(\n        columns=[\n            \"geo_code\",\n            \"geometry\",\n            \"air_quality_index\",\n            \"house_price_index\",\n            \"jobs_accessibility_index\",\n            \"greenspace_accessibility_index\",\n        ]\n    )\n    for col in exvars.columns.copy():\n        exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(W, exvars[col])\n    regressor_lag = HistGradientBoostingRegressor(\n        random_state=0,\n    )\n    est_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n    est_lag.fit(exvars, data.air_quality_index)\n    meta[name] = {\"score\": est_lag.best_score_}\n    y_pred_lag = cross_val_predict(\n        est_lag.best_estimator_, exvars, data.air_quality_index, cv=5\n    )\n    pred_lag = pd.Series(y_pred_lag, index=data.index)\n    residuals_lag = data.air_quality_index - pred_lag\n    meta[name][\"mse\"] = mean_squared_error(data.air_quality_index, pred_lag)\n    meta[name][\"me\"] = residuals_lag.abs().mean()\n    meta[name][\"prediction\"] = pred_lag\n    meta[name][\"residuals\"] = residuals_lag\n    meta[name][\"model\"] = est_lag.best_estimator_\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nSave evaluation metrics as series.\n\nmse = pd.Series([vals[\"mse\"] for vals in meta.values()], index=meta.keys())\nme = pd.Series([vals[\"me\"] for vals in meta.values()], index=meta.keys())\nscore = pd.Series([vals[\"score\"] for vals in meta.values()], index=meta.keys())\n\nSort according to MSE. Lower is better.\n\nmse.sort_values()\n\nqueen2000m    0.505627\nqueen4        0.510048\n2000m         0.520661\nqueen5        0.523149\nqueen3        0.544779\n1000m         0.671224\nqueen1000m    0.677756\nqueen2        0.714267\nqueen500m     0.826434\nqueen         0.919178\n500m          0.947078\ndtype: float64\n\n\nSort according to ME. Lower is better.\n\nme.sort_values()\n\nqueen2000m    0.445121\n2000m         0.453738\nqueen4        0.481805\nqueen5        0.486240\nqueen3        0.514920\n1000m         0.551970\nqueen1000m    0.558655\nqueen2        0.601619\nqueen500m     0.658718\n500m          0.708802\nqueen         0.712700\ndtype: float64\n\n\nSort according to R2. Higher is better.\n\nscore.sort_values()\n\nqueen         0.553838\n500m          0.563087\nqueen500m     0.615798\nqueen2        0.690342\nqueen1000m    0.721429\n1000m         0.724661\nqueen3        0.762028\nqueen5        0.780816\nqueen4        0.782347\n2000m         0.803538\nqueen2000m    0.807554\ndtype: float64\n\n\nThe optimal model seems to use a combination of Queen weights and Distance Band 2000m. Let’s explore it.\nThe actual vs predicted values.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.air_quality_index, meta[\"queen2000m\"][\"prediction\"], s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot the values using the original cmap.\n\nfrom shapely.geometry import box\n\nbds = data.total_bounds\nextent = gpd.GeoSeries(\n    [box((bds[0] - 7000), bds[1], bds[2] + 7000, bds[3])], crs=data.crs\n).to_crs(3857)\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(data[\"air_quality_index\"].values, k=20).bins\n\ndata.assign(pred=meta[\"queen2000m\"][\"prediction\"]).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"magma_r\",\n)\nlegendgram(\n    f,\n    ax,\n    meta[\"queen2000m\"][\"prediction\"],\n    bins,\n    pal=palmpl.Magma_20_r,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(10, 20),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=meta[\"queen2000m\"][\"residuals\"]).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-4, vmax=4, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nIt seems that large OAs are less precise. Green belt tends to be overpredicted whuile some more central areas underpredicted. However, the error across the area is minimal.\nWe can save the meta dict with all the data. The final model is part of that.\n\nimport pickle\n\n\nwith open(f\"{data_folder}/models/air_quality_meta.pickle\", \"wb\") as f:\n    pickle.dump(meta, f)\n\nSave just the model for easy inference.\n\nwith open(f\"{data_folder}/models/air_quality_model.pickle\", \"wb\") as f:\n    pickle.dump(meta[\"queen2000m\"][\"model\"], f)"
  },
  {
    "objectID": "code/02_models/05a_house_price_model_exploration.html#using-spatial-lag",
    "href": "code/02_models/05a_house_price_model_exploration.html#using-spatial-lag",
    "title": "Appendix K — House Price prediction model",
    "section": "K.1 Using spatial lag",
    "text": "K.1 Using spatial lag\nWe can add a lag into the mix. Let’s start with the Queen weights.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nsdsAre_lag\nsisBpM_lag\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n30280.039796\n0.064310\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n17776.067815\n0.054231\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n54958.015580\n0.058214\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n78050.340373\n0.062603\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n27462.223773\n0.067705\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n\n\n\n\n5 rows × 118 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars[mask], data[mask].house_price_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 13min 29s, sys: 18min 57s, total: 32min 27s\nWall time: 18min 13s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.3852767425712931\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data[mask].index)\nresiduals_lag = data[mask].house_price_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_lag)\n\n201453.6577863544\n\n\nMSE got down from 248704 to 201453 but that still seems to be a bit high (mean error is 448 from 1500 mean value).\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(\n    data[\"house_price_index\"].fillna(method=\"pad\").values, k=20\n).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nOverlay of original and a lagged model. No clear indication that the lagged one would be vastly superior.\n\nax = residuals_lag.plot.hist(bins=50)\nresiduals.plot.hist(bins=50, ax=ax, color=\"r\", alpha=0.7)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThe average residuals:\n\nresiduals_lag.abs().mean(), residuals.abs().mean()\n\n(345.0198428517582, 381.3997073048114)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=128, max_iter=500,\n                              random_state=0)\n\n\n\nest.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)\n\n\n\nK.1.1 Wider weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen3 = libpysal.weights.higher_order(queen, k=3, lower_order=True)\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nqueen3.transform = \"R\"\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen3, exvars[col])\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars[mask], data.house_price_index[mask])\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 10min 36s, sys: 11min 30s, total: 22min 6s\nWall time: 13min 58s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.4753837295272674\n\n\n\n%%time\nest_lag.best_estimator_.predict(exvars[mask])\n\nCPU times: user 147 ms, sys: 32.8 ms, total: 180 ms\nWall time: 86 ms\n\n\narray([1731.20448464, 2830.44054758, 1613.82686231, ..., 1590.77525305,\n       2612.76409275, 1496.54246374])\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data[mask].index)\nresiduals_lag = data[mask].house_price_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_lag)\n\n168855.00058162943\n\n\n\nresiduals_lag.abs().mean()\n\n314.2761789390005\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(data[\"house_price_index\"].dropna().values, k=10).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_lag,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "code/02_models/05a_house_price_model_exploration.html#include-accessibilities",
    "href": "code/02_models/05a_house_price_model_exploration.html#include-accessibilities",
    "title": "Appendix K — House Price prediction model",
    "section": "K.2 Include accessibilities",
    "text": "K.2 Include accessibilities\nWe can try to include green space and jobs accessibility, as those are likely to affect the house price. We create a lagged model with these two variables on top of explanatory variables.\n\nexvars = data.drop(\n    columns=[\"geo_code\", \"geometry\", \"air_quality_index\", \"house_price_index\"]\n)\n\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nfor col in exvars.columns.copy():\n    exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(queen, exvars[col])\n\n\nexvars.head()\n\n\n\n\n\n\n\n\npopulation_estimate\nA, B, D, E. Agriculture, energy and water\nC. Manufacturing\nF. Construction\nG, I. Distribution, hotels and restaurants\nH, J. Transport and communication\nK, L, M, N. Financial, real estate, professional and administrative activities\nO,P,Q. Public administration, education and health\nR, S, T, U. Other\nLand cover [Discontinuous urban fabric]\n...\nmisCel_lag\nltcRea_lag\nldeAre_lag\nlseCCo_lag\nlseERI_lag\nlteOri_lag\nlteWNB_lag\nlieWCe_lag\njobs_accessibility_index_lag\ngreenspace_accessibility_index_lag\n\n\n\n\n0\n315\n0.547024\n3.146641\n5.436732\n4.839081\n2.065436\n4.283496\n6.922660\n1.606140\n1.000000\n...\n28.550796\n51.060993\n197839.799834\n0.409687\n0.881904\n28.787310\n0.014056\n0.013885\n1582.500000\n2.650345e+05\n\n\n1\n415\n0.028774\n4.017802\n3.870111\n33.515266\n8.821760\n26.432214\n83.050968\n15.597812\n0.152752\n...\n13.316627\n60.676470\n32812.973123\n0.398693\n0.937619\n20.853156\n0.029603\n0.001324\n10238.400000\n1.329411e+06\n\n\n2\n246\n0.110388\n1.325208\n4.297030\n11.481138\n2.315769\n4.408114\n8.930658\n3.636795\n1.000000\n...\n27.507441\n53.117042\n216595.359019\n0.323117\n0.784838\n16.607145\n0.009236\n0.002879\n1211.000000\n5.170833e+05\n\n\n3\n277\n0.478192\n2.732637\n4.713528\n4.167451\n1.776267\n3.689214\n5.194090\n1.366452\n0.931788\n...\n29.854903\n70.854692\n351081.933889\n0.398664\n0.862998\n23.747635\n0.009530\n0.013940\n2275.500000\n3.185030e+05\n\n\n4\n271\n0.680069\n3.917490\n6.768659\n6.029714\n2.573226\n5.335117\n8.758326\n2.003362\n0.696472\n...\n29.971743\n52.001773\n59309.608454\n0.379304\n0.890960\n18.537922\n0.016313\n0.004461\n1480.142857\n3.063273e+05\n\n\n\n\n5 rows × 122 columns\n\n\n\n\nregressor_lag = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_lag.fit(exvars[mask], data[mask].house_price_index)\n\nFitting 5 folds for each of 54 candidates, totalling 270 fits\nCPU times: user 19min 33s, sys: 33min 44s, total: 53min 17s\nWall time: 26min 23s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1, 0.2, 0.5, 1),\n                         'max_bins': (64, 128, 255),\n                         'max_iter': [100, 200, 500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_lag.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_iter=500, random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_lag.best_score_\n\n0.40067474727924707\n\n\n\ny_pred_lag = cross_val_predict(\n    est_lag.best_estimator_, exvars[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_lag = pd.Series(y_pred_lag, index=data[mask].index)\nresiduals_lag = data[mask].house_price_index - pred_lag\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_lag)\n\n196944.98165903118\n\n\nMSE is only marginally better than before.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_lag, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.EqualInterval(\n    data[\"house_price_index\"].fillna(method=\"pad\").values, k=20\n).bins\n\ndata.assign(pred=pred_lag).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_lag,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_lag).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_lag.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nNot great so far, it seems that our set of explanatory variables is not explanatory enough to predict house prices in this way…"
  },
  {
    "objectID": "code/02_models/05a_house_price_model_exploration.html#with-latent-representation-of-sentinel-2",
    "href": "code/02_models/05a_house_price_model_exploration.html#with-latent-representation-of-sentinel-2",
    "title": "Appendix K — House Price prediction model",
    "section": "K.3 With latent representation of Sentinel 2",
    "text": "K.3 With latent representation of Sentinel 2\nTry including the lagged latent representation from the postcode Sentinel paper.\n\nlatent_oa = pd.read_parquet(f\"{data_folder}/processed/sentinel/latent_oa.parquet\")\n\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nqueen.transform = \"R\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\n\nexvars_latent = pd.concat([exvars, latent_oa.drop(columns=\"geometry\")], axis=1)\n\n\nfor col in exvars_latent.columns.copy():\n    exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n        queen, exvars_latent[col]\n    )\n\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n/var/folders/2f/fhks6w_d0k556plcv3rfmshw0000gn/T/ipykernel_54818/186550642.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  exvars_latent[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(\n\n\n\nexvars_latent = exvars_latent.copy()\n\n\nparameters = {\n    \"learning_rate\": (0.01, 0.05, 0.1),\n    \"max_iter\": [500],\n    \"max_bins\": (64, 128),\n}\n\nInitiate GridSearchCV with Histogram-based Gradient Boosting Regression Tree.\n\nregressor = HistGradientBoostingRegressor(\n    random_state=0,\n)\nest_latent = GridSearchCV(regressor, parameters, verbose=1)\n\nFit the data.\n\n%%time\nest_latent.fit(exvars_latent[mask], data[mask].house_price_index)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\nCPU times: user 3min 12s, sys: 2min 41s, total: 5min 53s\nWall time: 3min 30s\n\n\nGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0),\n             param_grid={'learning_rate': (0.01, 0.05, 0.1),\n                         'max_bins': (64, 128), 'max_iter': [500]},\n             verbose=1)estimator: HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)HistGradientBoostingRegressorHistGradientBoostingRegressor(random_state=0)\n\n\n\nest_latent.best_estimator_\n\nHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor(learning_rate=0.05, max_bins=64, max_iter=500,\n                              random_state=0)\n\n\nTest HistGradientBoostingRegressor without any pre-processing as a baseline.\n\nest_latent.best_score_\n\n0.42214079367595386\n\n\n\ny_pred_latent = cross_val_predict(\n    est_latent.best_estimator_, exvars_latent[mask], data[mask].house_price_index, cv=5\n)\n\nGet the prediction and residuals.\n\npred_latent = pd.Series(y_pred_latent, index=data[mask].index)\nresiduals_latent = data[mask].house_price_index - pred_latent\n\nCheck the error\n\nmean_squared_error(data[mask].house_price_index, pred_latent)\n\n190508.98759834783\n\n\nMSE is only marginally better than before.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], pred_latent, s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot prediction using the same bins as real values.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(data[\"house_price_index\"].dropna().values, k=10).bins\n\ndata.assign(pred=pred_latent).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_latent,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=residuals_latent).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nresiduals_latent.plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(pred_latent.dropna().values, k=10).bins\n\ndata.assign(pred=pred_latent).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    pred_latent,\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(0, pred_latent.max()),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")"
  },
  {
    "objectID": "code/02_models/05b_house_price_model_search.html",
    "href": "code/02_models/05b_house_price_model_search.html",
    "title": "Appendix L — House price prediction model search",
    "section": "",
    "text": "Loop and grid search for the optimal house price model.\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\n\nimport contextily\nimport palettable.matplotlib as palmpl\nimport matplotlib.pyplot as plt\nimport mapclassify\nimport libpysal\n\nfrom utils import legendgram\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data\n\ndata = gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n\nFilter only explanatory variables.\n\nexvars = data.drop(\n    columns=[\n        \"geo_code\",\n        \"geometry\",\n        \"air_quality_index\",\n        \"house_price_index\",\n        \"jobs_accessibility_index\",\n        \"greenspace_accessibility_index\",\n    ]\n)\n\nSpecify grid search parameters. We can limit the options based on previous exploration.\n\nparameters = {\"learning_rate\": (0.05, 0.1), \"max_iter\": [500], \"max_bins\": (64, 128)}\n\nDefine the simple weights matrices.\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nweights = {\n    \"queen\": queen,\n    \"queen2\": libpysal.weights.higher_order(queen, k=2, lower_order=True),\n    \"queen3\": libpysal.weights.higher_order(queen, k=3, lower_order=True),\n    \"queen4\": libpysal.weights.higher_order(queen, k=4, lower_order=True),\n    \"queen5\": libpysal.weights.higher_order(queen, k=5, lower_order=True),\n    \"500m\": libpysal.weights.DistanceBand.from_dataframe(data, 500),\n    \"1000m\": libpysal.weights.DistanceBand.from_dataframe(data, 1000),\n    \"2000m\": libpysal.weights.DistanceBand.from_dataframe(data, 2000),\n}\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 110 disconnected components.\n There are 82 islands with ids: 47, 71, 72, 89, 263, 361, 364, 375, 376, 377, 378, 541, 642, 983, 993, 1092, 1220, 1295, 1339, 1343, 1345, 1383, 1406, 1640, 1756, 1772, 1809, 1851, 1944, 1958, 2124, 2148, 2181, 2182, 2188, 2195, 2214, 2222, 2223, 2237, 2265, 2277, 2281, 2283, 2307, 2361, 2485, 2493, 2594, 2686, 2766, 2809, 2825, 2868, 2940, 2980, 3091, 3094, 3112, 3146, 3191, 3197, 3207, 3223, 3235, 3276, 3397, 3400, 3415, 3419, 3423, 3427, 3451, 3475, 3488, 3528, 3555, 3577, 3707, 3723, 3743, 3778.\n  warnings.warn(message)\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 16 disconnected components.\n There are 12 islands with ids: 89, 377, 378, 1944, 2182, 2277, 2493, 2594, 2868, 3146, 3223, 3528.\n  warnings.warn(message)\n\n\nAdd combined weights on top.\n\nweights[\"queen500m\"] = libpysal.weights.w_union(weights[\"queen\"], weights[\"500m\"])\nweights[\"queen1000m\"] = libpysal.weights.w_union(weights[\"queen\"], weights[\"1000m\"])\nweights[\"queen2000m\"] = libpysal.weights.w_union(weights[\"queen\"], weights[\"2000m\"])\n\nGet a mask to ignore missing values.\n\nmask = data.house_price_index.notna()\n\nUse Grid Search CV to find the best model for each weights option.\n\nmeta = {}\nfor name, W in weights.items():\n    W.transform = \"r\"\n    exvars = data.drop(\n        columns=[\n            \"geo_code\",\n            \"geometry\",\n            \"air_quality_index\",\n            \"house_price_index\",\n            \"jobs_accessibility_index\",\n            \"greenspace_accessibility_index\",\n        ]\n    )\n    for col in exvars.columns.copy():\n        exvars[f\"{col}_lag\"] = libpysal.weights.spatial_lag.lag_spatial(W, exvars[col])\n    regressor_lag = HistGradientBoostingRegressor(\n        random_state=0,\n    )\n    est_lag = GridSearchCV(regressor_lag, parameters, verbose=1)\n    est_lag.fit(exvars[mask], data.house_price_index[mask])\n    meta[name] = {\"score\": est_lag.best_score_}\n    y_pred_lag = cross_val_predict(\n        est_lag.best_estimator_, exvars[mask], data.house_price_index[mask], cv=5\n    )\n    pred_lag = pd.Series(y_pred_lag, index=data.index[mask])\n    residuals_lag = data.house_price_index[mask] - pred_lag\n    meta[name][\"mse\"] = mean_squared_error(data.house_price_index[mask], pred_lag)\n    meta[name][\"me\"] = residuals_lag.abs().mean()\n    meta[name][\"prediction\"] = pred_lag\n    meta[name][\"residuals\"] = residuals_lag\n    meta[name][\"model\"] = est_lag.best_estimator_\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 47, ' is an island (no neighbors)')\n('WARNING: ', 71, ' is an island (no neighbors)')\n('WARNING: ', 72, ' is an island (no neighbors)')\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 263, ' is an island (no neighbors)')\n('WARNING: ', 361, ' is an island (no neighbors)')\n('WARNING: ', 364, ' is an island (no neighbors)')\n('WARNING: ', 375, ' is an island (no neighbors)')\n('WARNING: ', 376, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 541, ' is an island (no neighbors)')\n('WARNING: ', 642, ' is an island (no neighbors)')\n('WARNING: ', 983, ' is an island (no neighbors)')\n('WARNING: ', 993, ' is an island (no neighbors)')\n('WARNING: ', 1092, ' is an island (no neighbors)')\n('WARNING: ', 1220, ' is an island (no neighbors)')\n('WARNING: ', 1295, ' is an island (no neighbors)')\n('WARNING: ', 1339, ' is an island (no neighbors)')\n('WARNING: ', 1343, ' is an island (no neighbors)')\n('WARNING: ', 1345, ' is an island (no neighbors)')\n('WARNING: ', 1383, ' is an island (no neighbors)')\n('WARNING: ', 1406, ' is an island (no neighbors)')\n('WARNING: ', 1640, ' is an island (no neighbors)')\n('WARNING: ', 1756, ' is an island (no neighbors)')\n('WARNING: ', 1772, ' is an island (no neighbors)')\n('WARNING: ', 1809, ' is an island (no neighbors)')\n('WARNING: ', 1851, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 1958, ' is an island (no neighbors)')\n('WARNING: ', 2124, ' is an island (no neighbors)')\n('WARNING: ', 2148, ' is an island (no neighbors)')\n('WARNING: ', 2181, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2188, ' is an island (no neighbors)')\n('WARNING: ', 2195, ' is an island (no neighbors)')\n('WARNING: ', 2214, ' is an island (no neighbors)')\n('WARNING: ', 2222, ' is an island (no neighbors)')\n('WARNING: ', 2223, ' is an island (no neighbors)')\n('WARNING: ', 2237, ' is an island (no neighbors)')\n('WARNING: ', 2265, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2281, ' is an island (no neighbors)')\n('WARNING: ', 2283, ' is an island (no neighbors)')\n('WARNING: ', 2307, ' is an island (no neighbors)')\n('WARNING: ', 2361, ' is an island (no neighbors)')\n('WARNING: ', 2485, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2686, ' is an island (no neighbors)')\n('WARNING: ', 2766, ' is an island (no neighbors)')\n('WARNING: ', 2809, ' is an island (no neighbors)')\n('WARNING: ', 2825, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 2940, ' is an island (no neighbors)')\n('WARNING: ', 2980, ' is an island (no neighbors)')\n('WARNING: ', 3091, ' is an island (no neighbors)')\n('WARNING: ', 3094, ' is an island (no neighbors)')\n('WARNING: ', 3112, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3191, ' is an island (no neighbors)')\n('WARNING: ', 3197, ' is an island (no neighbors)')\n('WARNING: ', 3207, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3235, ' is an island (no neighbors)')\n('WARNING: ', 3276, ' is an island (no neighbors)')\n('WARNING: ', 3397, ' is an island (no neighbors)')\n('WARNING: ', 3400, ' is an island (no neighbors)')\n('WARNING: ', 3415, ' is an island (no neighbors)')\n('WARNING: ', 3419, ' is an island (no neighbors)')\n('WARNING: ', 3423, ' is an island (no neighbors)')\n('WARNING: ', 3427, ' is an island (no neighbors)')\n('WARNING: ', 3451, ' is an island (no neighbors)')\n('WARNING: ', 3475, ' is an island (no neighbors)')\n('WARNING: ', 3488, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\n('WARNING: ', 3555, ' is an island (no neighbors)')\n('WARNING: ', 3577, ' is an island (no neighbors)')\n('WARNING: ', 3707, ' is an island (no neighbors)')\n('WARNING: ', 3723, ' is an island (no neighbors)')\n('WARNING: ', 3743, ' is an island (no neighbors)')\n('WARNING: ', 3778, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n('WARNING: ', 89, ' is an island (no neighbors)')\n('WARNING: ', 377, ' is an island (no neighbors)')\n('WARNING: ', 378, ' is an island (no neighbors)')\n('WARNING: ', 1944, ' is an island (no neighbors)')\n('WARNING: ', 2182, ' is an island (no neighbors)')\n('WARNING: ', 2277, ' is an island (no neighbors)')\n('WARNING: ', 2493, ' is an island (no neighbors)')\n('WARNING: ', 2594, ' is an island (no neighbors)')\n('WARNING: ', 2868, ' is an island (no neighbors)')\n('WARNING: ', 3146, ' is an island (no neighbors)')\n('WARNING: ', 3223, ' is an island (no neighbors)')\n('WARNING: ', 3528, ' is an island (no neighbors)')\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nSave evaluation metrics as series.\n\nmse = pd.Series([vals[\"mse\"] for vals in meta.values()], index=meta.keys())\nme = pd.Series([vals[\"me\"] for vals in meta.values()], index=meta.keys())\nscore = pd.Series([vals[\"score\"] for vals in meta.values()], index=meta.keys())\n\nSort according to MSE. Lower is better.\n\nmse.sort_values()\n\nqueen5        160164.675606\n2000m         160245.606116\nqueen2000m    160766.019742\n1000m         164343.481698\nqueen1000m    165435.671928\nqueen4        167802.714113\nqueen3        170864.677687\nqueen2        177496.106149\nqueen500m     185800.082368\n500m          186592.783587\nqueen         201453.657786\ndtype: float64\n\n\nSort according to ME. Lower is better.\n\nme.sort_values()\n\n2000m         304.115638\nqueen2000m    305.246902\nqueen5        306.013012\n1000m         310.392479\nqueen4        311.826728\nqueen1000m    312.101787\nqueen3        317.151294\nqueen2        324.171353\nqueen500m     330.578553\n500m          332.239067\nqueen         345.019843\ndtype: float64\n\n\nSort according to R2. Higher is better.\n\nscore.sort_values()\n\nqueen         0.385277\n500m          0.431761\nqueen500m     0.433833\nqueen2        0.454928\nqueen3        0.468809\nqueen4        0.480325\n1000m         0.486258\nqueen1000m    0.489035\n2000m         0.497828\nqueen2000m    0.498994\nqueen5        0.504176\ndtype: float64\n\n\nThe optimal model seems to use either Queen 5, 2000m or a combination. As the original distribution of prices can be a bit bumpy, let’s stick to Q5. Let’s explore it.\nThe actual vs predicted values.\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplt.scatter(data.house_price_index[mask], meta[\"queen5\"][\"prediction\"], s=0.25)\nplt.xlabel(\"Y test\")\nplt.ylabel(\"Y pred\")\n\nText(0, 0.5, 'Y pred')\n\n\n\n\n\nPlot the values using the original cmap.\n\nfrom shapely.geometry import box\n\nbds = data.total_bounds\nextent = gpd.GeoSeries(\n    [box((bds[0] - 7000), bds[1], bds[2] + 7000, bds[3])], crs=data.crs\n).to_crs(3857)\n\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\nbins = mapclassify.NaturalBreaks(data[\"house_price_index\"].dropna().values, k=10).bins\n\ndata.assign(pred=meta[\"queen5\"][\"prediction\"]).to_crs(3857).plot(\n    \"pred\",\n    scheme=\"userdefined\",\n    classification_kwds={\"bins\": bins},\n    ax=ax,\n    alpha=0.9,\n    cmap=\"viridis\",\n)\nlegendgram(\n    f,\n    ax,\n    meta[\"queen5\"][\"prediction\"],\n    bins,\n    pal=palmpl.Viridis_10,\n    legend_size=(0.35, 0.15),  # legend size in fractions of the axis\n    loc=\"lower left\",  # matplotlib-style legend locations\n    clip=(\n        0,\n        data[\"house_price_index\"].max(),\n    ),  # clip the displayed range of the histogram\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\nPlot residuals.\n\nf, ax = plt.subplots(figsize=(18, 12))\nextent.plot(ax=ax, alpha=0)\ndata.assign(res=meta[\"queen5\"][\"residuals\"]).to_crs(3857).plot(\n    \"res\", ax=ax, alpha=0.9, cmap=\"RdBu\", vmin=-2000, vmax=2000, legend=True\n)\nax.set_axis_off()\ncontextily.add_basemap(\n    ax=ax, source=contextily.providers.CartoDB.PositronNoLabels, attribution=\"\"\n)\ncontextily.add_basemap(\n    ax=ax,\n    source=contextily.providers.Stamen.TonerLines,\n    alpha=0.4,\n    attribution=\"(C) CARTO, Map tiles by Stamen Design, CC BY 3.0 -- Map data (C) OpenStreetMap contributors\",\n)\n# plt.savefig(f\"{data_folder}/outputs/figures/air_quality_index.png\", dpi=150, bbox_inches=\"tight\")\n\n\n\n\n\nmeta[\"queen5\"][\"residuals\"].plot.hist(bins=25)\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nThere doesn’t seem to be any general pattern in where we over- and where underpredict. The general tendency seems to be captured and a comparison against this base model shall work in the final app and scenario building.\nWe can save the meta dict with all the data. The final model is part of that.\n\nimport pickle\n\n\nwith open(f\"{data_folder}/models/house_price_meta.pickle\", \"wb\") as f:\n    pickle.dump(meta, f)\n\nSave just the model for easy inference.\n\nwith open(f\"{data_folder}/models/house_price_model.pickle\", \"wb\") as f:\n    pickle.dump(meta[\"queen5\"][\"model\"], f)"
  },
  {
    "objectID": "code/03_prediction/01_summary.html#non-morphological-data",
    "href": "code/03_prediction/01_summary.html#non-morphological-data",
    "title": "Appendix M — Create summary data for signatures",
    "section": "M.1 Non-morphological data",
    "text": "M.1 Non-morphological data\nKey:\n\nfn_key = {\n    \"Workplace population [Agriculture, energy and water]\": \"A, B, D, E. Agriculture, energy and water\",\n    \"Workplace population [Manufacturing]\": \"C. Manufacturing\",\n    \"Workplace population [Construction]\": \"F. Construction\",\n    \"Workplace population [Distribution, hotels and restaurants]\": \"G, I. Distribution, hotels and restaurants\",\n    \"Workplace population [Transport and communication]\": \"H, J. Transport and communication\",\n    \"Workplace population [Financial, real estate, professional and administrative activities]\": \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n    \"Workplace population [Public administration, education and health]\": \"O,P,Q. Public administration, education and health\",\n    \"Workplace population [Other]\": \"R, S, T, U. Other\",\n}\n\nRead and merge labels (and areas).\n\nfunction = (\n    dd.read_parquet(\n        \"signatures_gb/function\",\n        columns=[\n            \"hindex\",\n            \"Population\",\n            \"Land cover [Non-irrigated arable land]\",\n            \"Land cover [Industrial or commercial units]\",\n            \"Land cover [Sport and leisure facilities]\",\n            \"Land cover [Green urban areas]\",\n            \"Land cover [Discontinuous urban fabric]\",\n            \"Land cover [Pastures]\",\n            \"Land cover [Continuous urban fabric]\",\n        ]\n        + list(fn_key),\n    )\n    .compute()\n    .set_index(\"hindex\")\n)\n\nfunction = function.merge(\n    signature_type[[\"type\"]], how=\"left\", left_index=True, right_index=True\n).merge(form[[\"sdcAre\"]], how=\"left\", left_index=True, right_index=True)\nfunction = function.rename(columns=fn_key)\n\nNormalize subset by area.\n\nsubset = [\n    \"Population\",\n    \"A, B, D, E. Agriculture, energy and water\",\n    \"C. Manufacturing\",\n    \"F. Construction\",\n    \"G, I. Distribution, hotels and restaurants\",\n    \"H, J. Transport and communication\",\n    \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n    \"O,P,Q. Public administration, education and health\",\n    \"R, S, T, U. Other\",\n]\nfunction[subset] = function[subset].divide(function.sdcAre, axis=0)\n\nGroup by type, get quartiles and IQR.\n\ngrouper = function.groupby(\"type\")\nmedian = grouper.quantile(0.5)\nq25 = grouper.quantile(0.25)\nq75 = grouper.quantile(0.75)\niqr = q75 - q25\nmedian = median.rename(index=renamer)\niqr = iqr.rename(index=renamer)\n\nSave.\n\nmedian.drop(columns=\"sdcAre\").to_parquet(\n    f\"{data_folder}/sampling/median_function.parquet\"\n)\niqr.drop(columns=\"sdcAre\").to_parquet(f\"{data_folder}/sampling/iqr_function.parquet\")"
  },
  {
    "objectID": "code/03_prediction/02_sample_exvars.html",
    "href": "code/03_prediction/02_sample_exvars.html",
    "title": "Appendix N — Sample explanatory variables based on empirical data and custom scenario definition",
    "section": "",
    "text": "This notebook contains code to generate explanatory variables for an output area based on 4 sliders:\n\nLevel of urbanity\nUse\nGreenspace\nJob types\n\nThe data are either adapted from the original observed value (if the level of urbanity does not change) or sampled from the signature types from across the GB.\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the data informing the distributions.\n\nmedian_form = pd.read_parquet(f\"{data_folder}/sampling/median_form.parquet\")\niqr_form = pd.read_parquet(f\"{data_folder}/sampling/iqr_form.parquet\")\nmedian_function = pd.read_parquet(f\"{data_folder}/sampling/median_function.parquet\")\niqr_function = pd.read_parquet(f\"{data_folder}/sampling/iqr_function.parquet\")\noa = (\n    gpd.read_parquet(f\"{data_folder}/processed/interpolated/all_oa.parquet\")\n    .set_index(\"geo_code\")\n    .rename(columns={\"population_estimate\": \"population\"})\n)\noa_key = pd.read_parquet(f\"{data_folder}/sampling/oa_key.parquet\")\n\nGet OA areas for area-weighted variables.\n\noa_area = oa.area\n\nDefine a sampling method.\n\ndef _form(signature_type, variable, random_seed):\n    \"\"\"Get values for form variables\n\n    Values are sampled from a normal distribution around\n    median of a variable per signature type. The spread is\n    defined as 1/5 of interquartile range.\n    \"\"\"\n    rng = np.random.default_rng(random_seed)\n    return rng.normal(\n        median_form.loc[signature_type, variable],\n        iqr_form.loc[signature_type, variable] / 5,\n    )\n\n\ndef _function(signature_type, variable, random_seed):\n    \"\"\"Get values for function variables\n\n    Values are sampled from a normal distribution around\n    median of a variable per signature type. The spread is\n    defined as 1/5 of interquartile range.\n    \"\"\"\n    rng = np.random.default_rng(random_seed)\n    return rng.normal(\n        median_function.loc[signature_type, variable],\n        iqr_function.loc[signature_type, variable] / 5,\n    )\n\n\ndef _populations(defaults, index):\n    \"\"\"Balance residential and workplace population\n\n    Workplace population and residential population are treated 1:1 and\n    are re-allocated based on the index. The proportion of workplace categories\n    is not changed.\n    \"\"\"\n    if not -1 &lt;= index &lt;= 1:\n        raise ValueError(f\"use index must be in a range -1...1. {index} given.\")\n    jobs = [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"G, I. Distribution, hotels and restaurants\",\n        \"H, J. Transport and communication\",\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n        \"R, S, T, U. Other\",\n    ]\n    n_jobs = defaults[jobs].sum()\n    if index &lt; 0:\n        difference = index * n_jobs\n    else:\n        difference = index * defaults.population\n    new_n_jobs = n_jobs + difference\n    defaults.population = defaults.population - difference\n    multiplier = new_n_jobs / n_jobs\n    defaults[jobs] = defaults[jobs] * multiplier\n    return defaults\n\n\ndef _greenspace(defaults, index):\n    \"\"\"Allocate greenspace to OA\n\n    Allocate publicly accessible formal greenspace to OA. Defines a portion\n    of OA that is covered by gren urban areas. Realistic values are be fairly\n    low. The value affects populations and other land cover classes.\n    \"\"\"\n    if not 0 &lt;= index &lt;= 1:\n        raise ValueError(f\"greenspace index must be in a range 0...1. {index} given.\")\n    greenspace_orig = defaults[\"Land cover [Green urban areas]\"]\n    newly_allocated_gs = index - greenspace_orig\n    defaults = defaults * (1 - newly_allocated_gs)\n    defaults[\"Land cover [Green urban areas]\"] = index\n    return defaults\n\n\ndef _job_types(defaults, index):\n    \"\"\"Balance job types\n\n    Balance job types between manual and white collar workplace categories.\n    Index represents the proportion of white collar jobs in an area. The\n    total sum of FTEs is not changed.\n\n    The service category is not affected under an assumption that both white\n    and blue collar workers need the same amount of services to provide food etc.\n    \"\"\"\n    if not 0 &lt;= index &lt;= 1:\n        raise ValueError(f\"job_types index must be in a range 0...1. {index} given.\")\n    blue = [\n        \"A, B, D, E. Agriculture, energy and water\",\n        \"C. Manufacturing\",\n        \"F. Construction\",\n        \"H, J. Transport and communication\",\n    ]\n    white = [\n        \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n        \"O,P,Q. Public administration, education and health\",\n    ]\n    blue_collar = defaults[blue].sum()\n    white_collar = defaults[white].sum()\n    total = blue_collar + white_collar\n    orig_proportion = white_collar / total\n\n    new_blue = total * (1 - index)\n    new_white = total * index\n\n    blue_diff = new_blue / blue_collar\n    white_diff = new_white / white_collar\n\n    defaults[blue] = defaults[blue] * blue_diff\n    defaults[white] = defaults[white] * white_diff\n\n    return defaults\n\n\ndef get_signature_values(\n    oa_code: str,\n    signature_type: str = None,\n    use: float = 0,\n    greenspace: float = None,\n    job_types: float = None,\n    random_seed: int = None,\n):\n    \"\"\"Generate explanatory variables based on a scenario\n\n    Generates values for explanatory variables based on empirical data derived\n    from the Urban Grammar project and a scenario definition based on a\n    Urban Grammar signature type, land use balance, greenspace allocation\n    and a job type balance.\n\n    If the target ``signature_type`` differs from the one already allocated\n    to OA, the data is sampled from the distribution from the whole GB. If\n    they are equal, the existing values measured in place are used. That allows\n    playing with other variables without changing the form.\n\n    Parameters\n    ----------\n    oa_code : string\n        String representing the OA code, e.g. ``\"E00042707\"``.\n\n    signature_type : string\n        String representing signature type. See below the possible options\n        and their relationship to the level of urbanity.\n\n            0: 'Wild countryside',\n            1: 'Countryside agriculture',\n            2: 'Urban buffer',\n            3: 'Warehouse/Park land',\n            4: 'Open sprawl',\n            5: 'Disconnected suburbia',\n            6: 'Accessible suburbia',\n            7: 'Connected residential neighbourhoods',\n            8: 'Dense residential neighbourhoods',\n            9: 'Gridded residential quarters',\n            10: 'Dense urban neighbourhoods',\n            11: 'Local urbanity',\n            12: 'Regional urbanity',\n            13: 'Metropolitan urbanity',\n            14: 'Concentrated urbanity',\n            15: 'Hyper concentrated urbanity',\n\n    use : float, optional\n        Float in a range -1...1 reflecting the land use balance between\n        fully residential (-1) and fully commercial (1). Defautls to 0,\n        a value derived from signatures. For values &lt; 0, we are allocating\n        workplace population to residential population. For values &gt; 0, we\n        are allocating residential population to workplace population.\n        Extremes are allowed but are not realistic, in most cases.\n    greenspace : float, optional\n        Float in a range 0...1 reflecting the amount of greenspace in the\n        area. 0 representes no accessible greenspace, 1 represents whole\n        area covered by a greenspace. This value will proportionally affect\n        the amounts of jobs and population.\n    job_types : float, optional\n        Float in a range 0...1 reflecting the balance of job types in the\n        area between entirely blue collar jobs (0) and entirely white collar\n        jobs (1).\n    random_seed : int, optional\n        Random seed\n\n    Returns\n    -------\n    Series\n    \"\"\"\n    orig_type = oa_key.primary_type[oa_code]\n    if signature_type is not None and orig_type != signature_type:\n        form = pd.Series(\n            [_form(signature_type, var, random_seed) for var in median_form.columns],\n            index=median_form.columns,\n            name=oa_code,\n        ).abs()\n\n        defaults = pd.Series(\n            [\n                _function(signature_type, var, random_seed)\n                for var in median_function.columns\n            ],\n            index=median_function.columns,\n            name=oa_code,\n        ).abs()\n\n        area_weighted = [\n            \"population\",\n            \"A, B, D, E. Agriculture, energy and water\",\n            \"C. Manufacturing\",\n            \"F. Construction\",\n            \"G, I. Distribution, hotels and restaurants\",\n            \"H, J. Transport and communication\",\n            \"K, L, M, N. Financial, real estate, professional and administrative activities\",\n            \"O,P,Q. Public administration, education and health\",\n            \"R, S, T, U. Other\",\n        ]\n        defaults[area_weighted] = defaults[area_weighted] * oa_area[oa_code]\n\n    else:\n        form = oa.loc[oa_code][median_form.columns]\n        defaults = oa.loc[oa_code][median_function.columns]\n\n    # population\n    if use != 0:\n        defaults = _populations(defaults, index=use)\n\n    # greenspace\n    if greenspace:\n        defaults = _greenspace(defaults, greenspace)\n\n    if job_types:\n        defaults = _job_types(defaults, job_types)\n    return pd.concat([form, defaults])\n\nExample:\nSet the OA we are interested in.\n\noa_code = \"E00042271\"\n\nCheck the signature type of the OA.\n\noa_key.primary_type[oa_code]\n\n'Dense urban neighbourhoods'\n\n\nThis is the actual value with no changes.\n\nget_signature_values(\n    oa_code,\n)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                                 391\nA, B, D, E. Agriculture, energy and water                                             0.685492\nC. Manufacturing                                                                      0.097322\nF. Construction                                                                       7.038725\nG, I. Distribution, hotels and restaurants                                           21.731618\nH, J. Transport and communication                                                    22.106873\nK, L, M, N. Financial, real estate, professional and administrative activities       10.598732\nO,P,Q. Public administration, education and health                                   74.328726\nR, S, T, U. Other                                                                     9.858182\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.509093\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.0\nLand cover [Discontinuous urban fabric]                                               0.490907\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\nStay within the same signature type and change only use.\n\nMore residential\n\n\nget_signature_values(oa_code, use=-0.5)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                          464.222835\nA, B, D, E. Agriculture, energy and water                                             0.342746\nC. Manufacturing                                                                      0.048661\nF. Construction                                                                       3.519362\nG, I. Distribution, hotels and restaurants                                           10.865809\nH, J. Transport and communication                                                    11.053436\nK, L, M, N. Financial, real estate, professional and administrative activities        5.299366\nO,P,Q. Public administration, education and health                                   37.164363\nR, S, T, U. Other                                                                     4.929091\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.509093\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.0\nLand cover [Discontinuous urban fabric]                                               0.490907\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\n\nLess residential, more jobs\n\n\nget_signature_values(oa_code, use=0.4)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                               234.6\nA, B, D, E. Agriculture, energy and water                                              1.41758\nC. Manufacturing                                                                      0.201259\nF. Construction                                                                      14.555892\nG, I. Distribution, hotels and restaurants                                           44.940396\nH, J. Transport and communication                                                    45.716413\nK, L, M, N. Financial, real estate, professional and administrative activities        21.91789\nO,P,Q. Public administration, education and health                                  153.709788\nR, S, T, U. Other                                                                    20.386452\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.509093\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.0\nLand cover [Discontinuous urban fabric]                                               0.490907\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\n\nMore residential and more greenspace.\n\nCheck current greenspace first.\n\nget_signature_values(\n    oa_code,\n)[\"Land cover [Green urban areas]\"]\n\n0.0\n\n\nNothing. Allocate 20% of area\n\nget_signature_values(oa_code, use=0.4, greenspace=0.2)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                              187.68\nA, B, D, E. Agriculture, energy and water                                             1.134064\nC. Manufacturing                                                                      0.161007\nF. Construction                                                                      11.644714\nG, I. Distribution, hotels and restaurants                                           35.952317\nH, J. Transport and communication                                                     36.57313\nK, L, M, N. Financial, real estate, professional and administrative activities       17.534312\nO,P,Q. Public administration, education and health                                   122.96783\nR, S, T, U. Other                                                                    16.309162\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.407275\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.2\nLand cover [Discontinuous urban fabric]                                               0.392725\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\nChange job type allocation towards more blue collar jobs.\n\nget_signature_values(\n    oa_code,\n    use=0.4,\n    greenspace=0.2,\n    job_types=0.2,\n)\n\nsdbAre                                                                               836.43386\nsdbCoA                                                                               10.849734\nssbCCo                                                                                0.342091\nssbCor                                                                                5.918075\nssbSqu                                                                                5.799881\nssbERI                                                                                  0.8782\nssbCCM                                                                               28.454278\nssbCCD                                                                                3.243832\nstbOri                                                                               14.042689\nsdcAre                                                                              3387.82744\nsscCCo                                                                                0.449487\nsscERI                                                                                0.960133\nsicCAR                                                                                0.245127\nstbCeA                                                                                5.194842\nmtbAli                                                                                  4.3889\nmtbNDi                                                                               20.755305\nmtcWNe                                                                                0.028451\nltbIBD                                                                               21.166978\nsdsSPW                                                                               28.213326\nsdsSWD                                                                                2.899166\nsdsSPO                                                                                0.447096\nsdsLen                                                                              109.429581\nsssLin                                                                                 0.92334\nldsMSL                                                                             1645.009527\nmtdDeg                                                                                2.850976\nlinP3W                                                                                0.810375\nlinP4W                                                                                0.108821\nlinPDE                                                                                0.080804\nlcnClo                                                                                0.000002\nldsCDL                                                                              162.855545\nxcnSCl                                                                                0.018812\nlinWID                                                                                0.035681\nstbSAl                                                                                3.947726\nsdsAre                                                                             17595.05196\nsisBpM                                                                                0.056843\nmisCel                                                                               12.861005\nltcRea                                                                                49.85095\nldeAre                                                                            59270.775826\nlseCCo                                                                                0.387218\nlseERI                                                                                0.814681\nlteOri                                                                               13.778403\nlteWNB                                                                                0.031542\nlieWCe                                                                                0.000269\npopulation                                                                              187.68\nA, B, D, E. Agriculture, energy and water                                             3.481745\nC. Manufacturing                                                                      0.494316\nF. Construction                                                                      35.751011\nG, I. Distribution, hotels and restaurants                                           35.952317\nH, J. Transport and communication                                                   112.284974\nK, L, M, N. Financial, real estate, professional and administrative activities         4.74268\nO,P,Q. Public administration, education and health                                   33.260332\nR, S, T, U. Other                                                                    16.309162\nLand cover [Non-irrigated arable land]                                                     0.0\nLand cover [Industrial or commercial units]                                           0.407275\nLand cover [Sport and leisure facilities]                                                  0.0\nLand cover [Green urban areas]                                                             0.2\nLand cover [Discontinuous urban fabric]                                               0.392725\nLand cover [Pastures]                                                                      0.0\nLand cover [Continuous urban fabric]                                                       0.0\nName: E00042271, dtype: object\n\n\nChange the signature type (a proxy for a level of urbanity).\n\nget_signature_values(\n    oa_code,\n    signature_type=\"Local urbanity\",\n    use=0.4,\n    greenspace=0.2,\n    job_types=0.2,\n)\n\nsdbAre                                                                              422.259877\nsdbCoA                                                                                0.000000\nssbCCo                                                                                0.408337\nssbCor                                                                                3.769432\nssbSqu                                                                                0.403360\nssbERI                                                                                0.982311\nssbCCM                                                                               18.595822\nssbCCD                                                                                0.135386\nstbOri                                                                                8.222212\nsdcAre                                                                             1663.577623\nsscCCo                                                                                0.438507\nsscERI                                                                                0.984578\nsicCAR                                                                                0.229900\nstbCeA                                                                                1.659903\nmtbAli                                                                                2.924640\nmtbNDi                                                                               15.906085\nmtcWNe                                                                                0.032347\nltbIBD                                                                               21.239290\nsdsSPW                                                                               26.420297\nsdsSWD                                                                                3.844587\nsdsSPO                                                                                0.384913\nsdsLen                                                                              114.906193\nsssLin                                                                                0.991167\nldsMSL                                                                             1938.858230\nmtdDeg                                                                                3.000000\nlinP3W                                                                                0.773400\nlinP4W                                                                                0.092012\nlinPDE                                                                                0.144101\nlcnClo                                                                                0.000002\nldsCDL                                                                              210.710475\nxcnSCl                                                                                0.001760\nlinWID                                                                                0.034503\nstbSAl                                                                                3.207152\nsdsAre                                                                            18131.928008\nsisBpM                                                                                0.048301\nmisCel                                                                               15.195866\nltcRea                                                                               49.395492\nldeAre                                                                            19916.510706\nlseCCo                                                                                0.347203\nlseERI                                                                                0.865735\nlteOri                                                                               19.688769\nlteWNB                                                                                0.004753\nlieWCe                                                                                0.000877\npopulation                                                                          432.770089\nA, B, D, E. Agriculture, energy and water                                            10.577324\nC. Manufacturing                                                                     18.857491\nF. Construction                                                                      96.518541\nG, I. Distribution, hotels and restaurants                                           92.453344\nH, J. Transport and communication                                                   128.179567\nK, L, M, N. Financial, real estate, professional and administrative activities       30.728056\nO,P,Q. Public administration, education and health                                   32.805174\nR, S, T, U. Other                                                                    24.494562\nLand cover [Non-irrigated arable land]                                                0.000000\nLand cover [Industrial or commercial units]                                           0.000000\nLand cover [Sport and leisure facilities]                                             0.000000\nLand cover [Green urban areas]                                                        0.200000\nLand cover [Discontinuous urban fabric]                                               0.614254\nLand cover [Pastures]                                                                 0.000000\nLand cover [Continuous urban fabric]                                                  0.042641\nName: E00042271, dtype: float64"
  },
  {
    "objectID": "code/03_prediction/03_inference.html#air-quality",
    "href": "code/03_prediction/03_inference.html#air-quality",
    "title": "Appendix O — Air quality and house price model inference",
    "section": "O.1 Air quality",
    "text": "O.1 Air quality\nLoad the sklearn model\n\nwith open(f\"{data_folder}/models/air_quality_model.pickle\", \"rb\") as f:\n    air_quality = pickle.load(f)\n\nCreate spatial weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\n_2k = libpysal.weights.DistanceBand.from_dataframe(data, 2000)\nW = libpysal.weights.w_union(queen, _2k)\nW.transform = \"r\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\nCreate object.\n\naqm = Model(W, air_quality)\n\nSave the custom predictor class to a pickle.\n\nwith open(f\"{data_folder}/models/air_quality_predictor.pickle\", \"wb\") as f:\n    pickle.dump(aqm, f)\n\n\nO.1.1 England-wide model\nLoad the sklearn model\n\nwith open(f\"{data_folder}/models/air_quality_model_nc_urbanities.pickle\", \"rb\") as f:\n    air_quality = pickle.load(f)\n\nCreate spatial weights\n\nqueen = libpysal.weights.Queen.from_dataframe(data)\nW = libpysal.weights.higher_order(queen, k=5, lower_order=True, silence_warnings=True)\nW.transform = \"r\"\n\n/Users/martin/mambaforge/envs/demoland/lib/python3.11/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n There are 3 disconnected components.\n  warnings.warn(message)\n\n\nCreate object.\n\naqm = Model(W, air_quality)\n\nSave the custom predictor class to a pickle.\n\nwith open(\n    f\"{data_folder}/models/air_quality_predictor_nc_urbanities.pickle\", \"wb\"\n) as f:\n    pickle.dump(aqm, f)"
  },
  {
    "objectID": "code/03_prediction/03_inference.html#house-price",
    "href": "code/03_prediction/03_inference.html#house-price",
    "title": "Appendix O — Air quality and house price model inference",
    "section": "O.2 House price",
    "text": "O.2 House price\nLoad the sklearn model\n\nwith open(f\"{data_folder}/models/house_price_model.pickle\", \"rb\") as f:\n    house_price = pickle.load(f)\n\nCreate spatial weights\n\nq5 = libpysal.weights.higher_order(queen, k=5, lower_order=True)\nq5.transform = \"r\"\n\nCreate a wrapper class computing the lag.\n\nhpm = Model(q5, house_price)\n\nSave the custom predictor class to a pickle.\n\nwith open(f\"{data_folder}/models/house_price_predictor.pickle\", \"wb\") as f:\n    pickle.dump(hpm, f)\n\n\nO.2.1 England-wide model\nLoad the sklearn model\n\nwith open(\n    f\"{data_folder}/models/house_price_model_england_no_london.pickle\", \"rb\"\n) as f:\n    house_price = pickle.load(f)\n\nCreate a wrapper class computing the lag.\n\nhpm = Model(W, house_price)\n\nSave the custom predictor class to a pickle.\n\nwith open(\n    f\"{data_folder}/models/house_price_predictor_england_no_london.pickle\", \"wb\"\n) as f:\n    pickle.dump(hpm, f)"
  },
  {
    "objectID": "code/03_prediction/03_inference.html#using-the-class-for-prediction",
    "href": "code/03_prediction/03_inference.html#using-the-class-for-prediction",
    "title": "Appendix O — Air quality and house price model inference",
    "section": "O.3 Using the class for prediction",
    "text": "O.3 Using the class for prediction\nTo use the class for prediction, load the pickle and call predict on a data frame with explanatory variables (either default or reflecting a scenario).\n\nwith open(f\"{data_folder}/models/air_quality_predictor.pickle\", \"rb\") as f:\n    aqm2 = pickle.load(f)\n\n\naqm2.predict(exvars)\n\narray([17.19278662, 16.43954378, 17.48423016, ..., 16.7559517 ,\n       12.60627689, 17.31309272])\n\n\nExactly the same would it be for the house price model."
  },
  {
    "objectID": "code/03_prediction/04_acc_class.html",
    "href": "code/03_prediction/04_acc_class.html",
    "title": "Appendix P — Accessibility API",
    "section": "",
    "text": "Prototyping of a wrap of accessibility computation into custom classes with simple API.\n\nimport geopandas as gpd\nimport pandas as pd\nimport xarray as xr\nimport numpy as np\nimport joblib\n\nfrom demoland_engine.indicators import Accessibility\n\n\ndata_folder = \"/Users/martin/Library/CloudStorage/OneDrive-SharedLibraries-TheAlanTuringInstitute/Daniel Arribas-Bel - demoland_data\"\n\nLoad the travel time matrix.\n\nttm = pd.read_parquet(f\"{data_folder}/processed/accessibility/ttm_complete.parquet\")\n\nConvert the matrix to boolean xarray.DataArray.\n\nttm = ttm.set_index([\"from_id\", \"to_id\"])\nttm.columns = [\"transit\", \"car\", \"bike\", \"walk\"]\nttm.columns.name = \"mode\"\nttm_arr = xr.DataArray.from_series(ttm.stack())\nttm_15 = ttm_arr &lt;= 15\nttm_15.name = \"ttm_15\"\n\nLoad Workplace zone population.\n\nwpz_population = (\n    pd.read_csv(\n        f\"{data_folder}/processed/accessibility/wpz_tynewear_occupation_edited.csv\"\n    )\n    .rename(columns={\"wpz11cd\": \"to_id\"})\n    .set_index(\"to_id\")[\"pop\"]\n)\n\nMerge with the traveltime matrix to create a baseline.\n\nda = xr.DataArray.from_series(wpz_population)\nda.name = \"wpz_population\"\nbaseline = xr.merge([ttm_15, da])\nbaseline[\"wpz_population\"] = baseline[\"wpz_population\"].fillna(0)\n\n\nbaseline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:         (from_id: 3795, to_id: 9254, mode: 4)\nCoordinates:\n  * from_id         (from_id) object 'E00041363' 'E00041364' ... 'E00175605'\n  * to_id           (to_id) object 'E00041363' ... 'idFFE0D1A6-2B10-40AE-8E6E...\n  * mode            (mode) object 'transit' 'car' 'bike' 'walk'\nData variables:\n    ttm_15          (from_id, to_id, mode) bool True True True ... False False\n    wpz_population  (to_id) float64 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0xarray.DatasetDimensions:from_id: 3795to_id: 9254mode: 4Coordinates: (3)from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)to_id(to_id)object'E00041363' ... 'idFFE0D1A6-2B10...array(['E00041363', 'E00041364', 'E00041366', ...,\n       'idFFCECD51-C8D3-4DEF-9258-88B0240A502B',\n       'idFFD6A162-EFB8-4A17-9B2E-8D500B56A33F',\n       'idFFE0D1A6-2B10-40AE-8E6E-0AEB92375117'], dtype=object)mode(mode)object'transit' 'car' 'bike' 'walk'array(['transit', 'car', 'bike', 'walk'], dtype=object)Data variables: (2)ttm_15(from_id, to_id, mode)boolTrue True True ... False Falsearray([[[ True,  True,  True,  True],\n        [ True,  True,  True,  True],\n        [ True,  True,  True,  True],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[ True,  True,  True,  True],\n        [ True,  True,  True,  True],\n        [False,  True,  True, False],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[ True,  True,  True,  True],\n        [False,  True,  True, False],\n        [ True,  True,  True,  True],\n        ...,\n...\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[False,  True, False, False],\n        [False,  True, False, False],\n        [False,  True, False, False],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]],\n\n       [[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False],\n        ...,\n        [False, False, False, False],\n        [False,  True, False, False],\n        [False, False, False, False]]])wpz_population(to_id)float640.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0array([0., 0., 0., ..., 0., 0., 0.])Indexes: (3)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))to_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'idFF5D69F1-E02B-46EB-B28A-436049D483B2',\n       'idFF6C80EE-CC17-45BD-89D1-E08AA8187F98',\n       'idFF6ED153-BC31-4E74-8302-600261B247FA',\n       'idFF81A662-7FFC-4989-A89E-D153951A918C',\n       'idFF9931E9-5ABB-40DF-A188-B16D9A91730A',\n       'idFFA410C6-1645-4197-A7CA-8D2426EB327B',\n       'idFFA88491-3DDA-4831-A84C-5E3C714C456A',\n       'idFFCECD51-C8D3-4DEF-9258-88B0240A502B',\n       'idFFD6A162-EFB8-4A17-9B2E-8D500B56A33F',\n       'idFFE0D1A6-2B10-40AE-8E6E-0AEB92375117'],\n      dtype='object', name='to_id', length=9254))modePandasIndexPandasIndex(Index(['transit', 'car', 'bike', 'walk'], dtype='object', name='mode'))Attributes: (0)\n\n\nLoad greenspace data.\n\nacc_greenspace = pd.read_csv(\n    f\"{data_folder}/processed/accessibility/acc_greenspace_allmodes_15min_tynewear.csv\",\n    index_col=0,\n)\n\n\nacc_greenspace.columns = [\"transit\", \"car\", \"bike\", \"walk\"]\n\n\ngreenspace = xr.DataArray.from_series(acc_greenspace.stack()).rename(\n    {\"level_1\": \"mode\"}\n)\ngreenspace.name = \"green_accessibility\"\n\n\nbaseline = xr.merge([baseline, greenspace])\nbaseline[\"green_accessibility\"] = baseline[\"green_accessibility\"].fillna(0)\n\nCreate a wrapper class.\n\nacc = Accessibility(baseline)\n\nTest the object on example input.\nCreate random OA data.\n\noa_data = pd.Series(\n    np.random.randint(-100, 100, len(baseline.from_id)),\n    index=baseline.from_id.values,\n    name=\"oa\",\n)\noa_data.index.name = \"to_id\"\n\n\noa_data\n\nto_id\nE00041363    54\nE00041364    91\nE00041366    25\nE00041367    12\nE00041368    90\n             ..\nE00175601   -92\nE00175602   -48\nE00175603     4\nE00175604   -84\nE00175605   -35\nName: oa, Length: 3795, dtype: int64\n\n\nCompute accessibility.\n\nacc.job_accessibility(oa_data, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'combined' (from_id: 3795)&gt;\narray([ 9495., 10409.,  2972., ...,   395.,  1370.,   502.])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArray'combined'from_id: 37959.495e+03 1.041e+04 2.972e+03 8.643e+03 ... 963.0 395.0 1.37e+03 502.0array([ 9495., 10409.,  2972., ...,   395.,  1370.,   502.])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)\n\n\n\nnew_green = pd.Series(\n    np.random.randint(-10000, 10000, len(baseline.from_id)),\n    index=baseline.from_id.values,\n    name=\"oa\",\n)\nnew_green.index.name = \"to_id\"\n\n\nacc.greenspace_accessibility(new_green, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (from_id: 3795)&gt;\narray([ 484420.89095001,  519545.30120002,  297545.45855   , ...,\n       1828516.73930004, 1742541.35770002,   54213.1107    ])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArrayfrom_id: 37954.844e+05 5.195e+05 2.975e+05 ... 1.829e+06 1.743e+06 5.421e+04array([ 484420.89095001,  519545.30120002,  297545.45855   , ...,\n       1828516.73930004, 1742541.35770002,   54213.1107    ])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)\n\n\nSave the custom class to a compressed joblib.\n\nwith open(f\"{data_folder}/models/accessibility.joblib\", \"wb\") as f:\n    joblib.dump(acc, f, compress=True)\n\nTest loaded class\n\nwith open(f\"{data_folder}/models/accessibility.joblib\", \"rb\") as f:\n    acc2 = joblib.load(f)\n\n\nacc2\n\n&lt;demoland_engine.indicators.Accessibility at 0x175c43290&gt;\n\n\n\noa_data = pd.Series(\n    np.random.randint(-100, 100, len(acc2.baseline.from_id)),\n    index=acc2.baseline.from_id.values,\n    name=\"oa\",\n)\noa_data.index.name = \"to_id\"\nacc2.job_accessibility(oa_data, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'combined' (from_id: 3795)&gt;\narray([ 9051., 10132.,  2795., ...,   973.,  1547.,  -303.])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArray'combined'from_id: 37959.051e+03 1.013e+04 2.795e+03 8.589e+03 ... 973.0 1.547e+03 -303.0array([ 9051., 10132.,  2795., ...,   973.,  1547.,  -303.])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)\n\n\n\nnew_green = pd.Series(\n    np.random.randint(-10000, 10000, len(acc2.baseline.from_id)),\n    index=acc2.baseline.from_id.values,\n    name=\"oa\",\n)\nnew_green.index.name = \"to_id\"\nacc2.greenspace_accessibility(new_green, \"walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (from_id: 3795)&gt;\narray([ 489257.89095001,  528378.30120002,  307546.45855   , ...,\n       1775474.73930004, 1768524.35770002,   48372.1107    ])\nCoordinates:\n    mode     &lt;U4 'walk'\n  * from_id  (from_id) object 'E00041363' 'E00041364' ... 'E00175605'xarray.DataArrayfrom_id: 37954.893e+05 5.284e+05 3.075e+05 ... 1.775e+06 1.769e+06 4.837e+04array([ 489257.89095001,  528378.30120002,  307546.45855   , ...,\n       1775474.73930004, 1768524.35770002,   48372.1107    ])Coordinates: (2)mode()&lt;U4'walk'array('walk', dtype='&lt;U4')from_id(from_id)object'E00041363' ... 'E00175605'array(['E00041363', 'E00041364', 'E00041366', ..., 'E00175603', 'E00175604',\n       'E00175605'], dtype=object)Indexes: (1)from_idPandasIndexPandasIndex(Index(['E00041363', 'E00041364', 'E00041366', 'E00041367', 'E00041368',\n       'E00041369', 'E00041370', 'E00041371', 'E00041372', 'E00041374',\n       ...\n       'E00175596', 'E00175597', 'E00175598', 'E00175599', 'E00175600',\n       'E00175601', 'E00175602', 'E00175603', 'E00175604', 'E00175605'],\n      dtype='object', name='from_id', length=3795))Attributes: (0)"
  }
]