{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype workflow generating input for deployment of Demoland for a custom area in England\n",
    "\n",
    "Requires:\n",
    "\n",
    "- Area of interest defined in a GDAL-readable file. All geometries present in the file are considered a part of the AOI.\n",
    "- GTFS data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the extent of AoI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "from itertools import product\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shapely\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tracc\n",
    "from libpysal import graph\n",
    "\n",
    "import demoland_engine\n",
    "from demoland_engine.indicators import Accessibility, Model\n",
    "\n",
    "from r5py import TransportNetwork, TravelTimeMatrixComputer, TransportMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will be part of the container\n",
    "data_folder = \"../../../demoland_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"tyne_and_wear_hex\" # used for files\n",
    "area_name = \"Tyne and Wear\" # used in the app\n",
    "aoi_file_path = \"https://github.com/Urban-Analytics-Technology-Platform/demoland-web/raw/main/web/src/data/geography.json\"\n",
    "gtfs_data_file_path = f\"{data_folder}/raw/accessibility/itm_north_east_gtfs.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"app\")\n",
    "os.mkdir(\"app/scenarios\")\n",
    "os.mkdir(\"engine\")\n",
    "os.mkdir(f\"engine/{name}\")\n",
    "os.mkdir(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set date to the nearest monday in past (or today if monday is today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = dt.datetime.now()\n",
    "days_to_monday = today.weekday() - 0  # Monday is 0\n",
    "if days_to_monday < 0:\n",
    "    monday = today\n",
    "else:\n",
    "    monday = today - dt.timedelta(days=days_to_monday)\n",
    "date_time = f\"{monday.year},{monday.month},{monday.day},9,30\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = gpd.read_file(aoi_file_path)\n",
    "aoi_poly = aoi.to_crs(27700).unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get H3 grid with the data for the AoI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the full grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = gpd.read_parquet(f\"{data_folder}/h3/grid_complete.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a portion of the grid covering AoI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_aoi = grid.iloc[grid.sindex.query(aoi_poly, predicate=\"intersects\")].copy()\n",
    "grid_aoi[[\"lat\", \"lon\"]] = pd.DataFrame(grid_aoi.index.to_series().apply(h3.h3_to_geo).tolist(), columns=[\"lat\", \"lon\"], index=grid_aoi.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_aoi = grid_aoi.dropna(subset=\"signature_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Make predictive models ready\n",
    "\n",
    "Read the full matrix and subset it for AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = graph.read_parquet(f\"{data_folder}/h3/grid_adjacency_binary.parquet\").transform(\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_aoi = matrix.subgraph(grid_aoi.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_aoi.to_parquet(f\"engine/{name}/matrix.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make accessibility ready\n",
    "    6. Get GTFS\n",
    "  \n",
    "Go to https://data.bus-data.dft.gov.uk/downloads/, register and download timetable data for your region in GTFS data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_data_file = gtfs_data_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Get network from OSM\n",
    "\n",
    "Download a fresh OSM snapshot for England."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://download.geofabrik.de/europe/united-kingdom/england-latest.osm.pbf')\n",
    "with open(\"temp/england-latest.osm.pbf\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the AoI. We need a GeoJSON of the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi.dissolve().to_file(\"temp/aoi.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then can use osmium to get an extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================================================] 100% \n"
     ]
    }
   ],
   "source": [
    "!osmium extract -p temp/aoi.geojson temp/england-latest.osm.pbf -o temp/aoi.osm.pbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Get OS Greenspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.os.uk/downloads/v1/products/OpenGreenspace/downloads?area=GB&format=GeoPackage&redirect')\n",
    "with open(\"temp/opgrsp_gpkg_gb.zip\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/mambaforge/envs/shap/lib/python3.11/site-packages/pyogrio/raw.py:194: RuntimeWarning: File /vsimem/899208aeef7347a08e83dbc2a6dd275c has GPKG application_id, but non conformant file extension\n",
      "  result = ogr_read(\n",
      "/home/martin/mambaforge/envs/shap/lib/python3.11/site-packages/pyogrio/raw.py:194: RuntimeWarning: File /vsimem/3a23de11e1c34ca7ad631a020112ecbe has GPKG application_id, but non conformant file extension\n",
      "  result = ogr_read(\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('temp/opgrsp_gpkg_gb.zip', 'r') as zip_ref:\n",
    "    with zip_ref.open(\"Data/opgrsp_gb.gpkg\") as gsp:\n",
    "        f = gsp.read()\n",
    "        greenspace_sites = gpd.read_file(f, engine=\"pyogrio\", layer=\"greenspace_site\")\n",
    "        greenspace_access = gpd.read_file(f, engine=\"pyogrio\", layer=\"access_point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the AoI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenspace_sites_aoi = greenspace_sites.iloc[greenspace_sites.sindex.query(aoi_poly, predicate=\"intersects\")]\n",
    "greenspace_access_aoi = greenspace_access.iloc[greenspace_access.sindex.query(aoi_poly, predicate=\"intersects\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Process OS Greenspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenspace_sites_select = greenspace_sites_aoi.query(\n",
    "    \"function!='Allotments Or Community Growing Spaces' & function!='Golf Course' & function!='Bowling Green'\"\n",
    ")\n",
    "publicpark = greenspace_sites_select.query(\"function=='Public Park Or Garden'\")\n",
    "playingfield = greenspace_sites_select.query(\"function=='Playing Field'\")\n",
    "othersport = greenspace_sites_select.query(\"function=='Other Sports Facility'\")\n",
    "therest = greenspace_sites_select.query(\n",
    "    \"function!='Playing Field' & function!='Public Park Or Garden' & function!='Other Sports Facility'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 'therest' not included in the upper categories\n",
    "# we use sjoin to performe a spatial filter of 'therest' polygons contained in upper categories\n",
    "join11 = gpd.sjoin(therest, othersport, predicate=\"within\", how=\"inner\")\n",
    "join12 = gpd.sjoin(therest, playingfield, predicate=\"within\", how=\"inner\")\n",
    "join13 = gpd.sjoin(therest, publicpark, predicate=\"within\", how=\"inner\")\n",
    "\n",
    "# generate list of the IDs of 'therest' contained in upper categories, in order to eliminate the corresponding polygons from the layer\n",
    "list_for_diff11 = join11[\"id_left\"].drop_duplicates().to_list()\n",
    "\n",
    "diff11 = therest[\n",
    "    ~therest.id.isin(list_for_diff11)\n",
    "]  # 1st difference layer # note the negation character ~ to take the polygons NOT included\n",
    "\n",
    "list_for_diff12 = join12[\"id_left\"].drop_duplicates().to_list()\n",
    "diff12 = diff11[~diff11.id.isin(list_for_diff12)]  # 2nd difference layer\n",
    "\n",
    "list_for_diff13 = join13[\"id_left\"].drop_duplicates().to_list()\n",
    "diff13 = diff12[\n",
    "    ~diff12.id.isin(list_for_diff13)\n",
    "]  # 3rd difference layer, this is for 'therest' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat the same operation for subsequent categories:\n",
    "# find 'othersport' not included in the upper categories\n",
    "join21 = gpd.sjoin(othersport, playingfield, predicate=\"within\", how=\"inner\")\n",
    "join22 = gpd.sjoin(othersport, publicpark, predicate=\"within\", how=\"inner\")\n",
    "\n",
    "list_for_diff21 = join21[\"id_left\"].drop_duplicates().to_list()\n",
    "diff21 = othersport[~othersport.id.isin(list_for_diff21)]\n",
    "\n",
    "list_for_diff22 = join22[\"id_left\"].drop_duplicates().to_list()\n",
    "diff22 = diff21[~diff21.id.isin(list_for_diff22)]  # 'othersport' difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 'playing fields' not included in the upper categories (and viceversa?)\n",
    "join31 = gpd.sjoin(playingfield, publicpark, predicate=\"within\", how=\"inner\")\n",
    "join32 = gpd.sjoin(\n",
    "    publicpark, playingfield, predicate=\"within\", how=\"inner\"\n",
    ")  ## check it is not empty ... it is empty, we do not use this join\n",
    "\n",
    "list_for_diff31 = join31[\"id_left\"].drop_duplicates().to_list()\n",
    "diff31 = playingfield[\n",
    "    ~playingfield.id.isin(list_for_diff31)\n",
    "]  # 'playingfield' difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together all the differences layers: (and should bring out to the desired output)\n",
    "together1 = pd.concat([diff13, diff22]).pipe(\n",
    "    gpd.GeoDataFrame\n",
    ")  # 'therest' + 'othersport' differences\n",
    "together1.head()\n",
    "together2 = pd.concat([together1, diff31]).pipe(\n",
    "    gpd.GeoDataFrame\n",
    ")  # last gdf + 'playingfield' difference\n",
    "together_again = gpd.GeoDataFrame(pd.concat([together2, publicpark]), crs=27700)  # last gdf + all the public parks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gs_id = together_again[\"id\"].to_list()\n",
    "accesspoints_edge = greenspace_access_aoi[greenspace_access_aoi.ref_to_greenspace_site.isin(list_gs_id)]\n",
    "accesspoints_edge = accesspoints_edge.to_crs(27700)\n",
    "\n",
    "together_again[\"area_m2\"] = together_again[\"geometry\"].area\n",
    "\n",
    "together_again.to_file(\"temp/greenspace.gpkg\", layer=\"sites\")\n",
    "accesspoints_edge.to_file(\"temp/greenspace.gpkg\", layer=\"access_points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create traveltime matrix (origins are cells, destinations are cells plus greenspace entrances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = grid_aoi.set_geometry(grid_aoi.centroid).to_crs(4326)\n",
    "origins[\"id\"] = origins.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = pd.concat(\n",
    "    [\n",
    "        origins[[\"id\", \"geometry\"]],\n",
    "        accesspoints_edge[[\"id\", \"geometry\", \"ref_to_greenspace_site\"]].to_crs(4326),\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function TransportNetwork.__del__ at 0x7fe0d8304680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/martin/mambaforge/envs/shap/lib/python3.11/site-packages/r5py/r5/transport_network.py\", line 101, in __del__\n",
      "    self.osm_file.close()\n",
      "    ^^^^^^^^^^^^^\n",
      "AttributeError: 'TransportNetwork' object has no attribute 'osm_file'\n"
     ]
    }
   ],
   "source": [
    "transport_network = TransportNetwork(\"temp/aoi.osm.pbf\", [gtfs_data_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate dataframe with all from_id and all to_id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = product(origins[\"id\"].unique(), destinations[\"id\"].unique())\n",
    "empty_ttm = pd.DataFrame(prod)\n",
    "empty_ttm.columns = [\"from_id\", \"to_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables\n",
    "max_time = dt.timedelta(seconds=900) # SET TO 15 MIN\n",
    "walking_speed = 4.8\n",
    "cycling_speed = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe to match legmode and transitmode objects (to be inputted in the ttm computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes_lut = pd.DataFrame(\n",
    "    [\n",
    "        [\"transit\", TransportMode.CAR, TransportMode.WALK],\n",
    "        [\"car\", \"\", TransportMode.CAR],\n",
    "        [\"bicycle\", \"\", TransportMode.BICYCLE],\n",
    "        [\"walk\", \"\", TransportMode.WALK],\n",
    "    ],\n",
    "    columns=(\"Mode\", \"Transit_mode\", \"Leg_mode\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate custom list of transit+transport mode for the parameter transport_modes in TravelTimeMatrixComputer\n",
    "def list_making(s, z):\n",
    "    if s:\n",
    "        return [s] + [z]\n",
    "    return [z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current mode is: transit , transit is: TransportMode.CAR , transport var is: [<TransportMode.CAR: 'CAR'>, <TransportMode.WALK: 'WALK'>]\n",
      "finished calculating ttm for mode transit\n",
      "Duration for transit : 0:12:22.141491\n",
      "The current mode is: car , transit is:  , transport var is: [<TransportMode.CAR: 'CAR'>]\n",
      "finished calculating ttm for mode bicycle\n",
      "Duration for bicycle : 0:12:21.021629\n",
      "The current mode is: walk , transit is:  , transport var is: [<TransportMode.WALK: 'WALK'>]\n",
      "finished calculating ttm for mode walk\n",
      "Duration for walk : 0:12:47.189146\n"
     ]
    }
   ],
   "source": [
    "ttm_complete = empty_ttm.copy()\n",
    "\n",
    "# loop to compute a ttm for all the modes and generate one single ttm table in output\n",
    "for row in modes_lut.itertuples():\n",
    "    start_time = dt.datetime.now()\n",
    "    mode = row.Mode\n",
    "    transit_mode = row.Transit_mode\n",
    "    leg_mode = row.Leg_mode\n",
    "    transport_mode = list_making(\n",
    "        transit_mode, leg_mode\n",
    "    )  # creating list of objects for transport_modes parameter\n",
    "\n",
    "    print(\n",
    "        \"The current mode is:\",\n",
    "        mode,\n",
    "        \", transit is:\",\n",
    "        transit_mode,\n",
    "        \", transport var is:\",\n",
    "        transport_mode,\n",
    "    )\n",
    "    ttm_computer = TravelTimeMatrixComputer(\n",
    "        transport_network,\n",
    "        origins=origins,\n",
    "        destinations=destinations,\n",
    "        departure=dt.datetime.strptime(date_time, \"%Y,%m,%d,%H,%M\"),\n",
    "        max_time = max_time,\n",
    "        speed_walking=walking_speed,\n",
    "        speed_cycling=cycling_speed,\n",
    "        transport_modes=transport_mode,\n",
    "    )\n",
    "\n",
    "    ttm = ttm_computer.compute_travel_times()\n",
    "    ttm = ttm.rename(\n",
    "        columns={\"travel_time\": f\"time_{mode}\"}\n",
    "    )  # renaming 'travel_time' column (automatically generated) to 'time_{mode of transport}'\n",
    "    #  merging the empty table generated before (with all possible origins and destinations) with the ttm, per each mode adding a travel time column\n",
    "    ttm_complete = ttm_complete.merge(\n",
    "        ttm, how=\"outer\", left_on=[\"from_id\", \"to_id\"], right_on=[\"from_id\", \"to_id\"]\n",
    "    )\n",
    "\n",
    "    print(\"finished calculating ttm for mode\", mode)\n",
    "    end_time = dt.datetime.now()\n",
    "    print(\"Duration for\", mode, \": {}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttm_complete.to_parquet(f\"temp/ttm_complete.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap to a demoland_engine accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttm = ttm_complete.set_index([\"from_id\", \"to_id\"])\n",
    "ttm.columns = [\"transit\", \"car\", \"bike\", \"walk\"]\n",
    "ttm.columns.name = \"mode\"\n",
    "ttm_arr = xr.DataArray.from_series(ttm.stack())\n",
    "ttm_15 = ttm_arr <= 15\n",
    "ttm_15.name = \"ttm_15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpz_population = grid_aoi[[\n",
    "    'A, B, D, E. Agriculture, energy and water', 'C. Manufacturing',\n",
    "    'F. Construction', 'G, I. Distribution, hotels and restaurants',\n",
    "    'H, J. Transport and communication',\n",
    "    'K, L, M, N. Financial, real estate, professional and administrative activities',\n",
    "    'O,P,Q. Public administration, education and health',\n",
    "    'R, S, T, U. Other'\n",
    "]].sum(axis=1)\n",
    "wpz_population.index.name = \"to_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.DataArray.from_series(wpz_population)\n",
    "da.name = \"wpz_population\"\n",
    "baseline = xr.merge([ttm_15, da])\n",
    "baseline[\"wpz_population\"] = baseline[\"wpz_population\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load greenspace data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_sites = gpd.read_file(\"temp/greenspace.gpkg\", layer=\"sites\").rename(columns={\"id\": \"id_site\"})\n",
    "gs_entrances = gpd.read_file(\"temp/greenspace.gpkg\", layer=\"access_points\").rename(columns={\"id\": \"id_entrance\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate park area to entrances\n",
    "gs_entrances_with_parkarea = pd.merge(\n",
    "    gs_entrances[[\"id_entrance\", \"ref_to_greenspace_site\"]],\n",
    "    gs_sites[[\"id_site\", \"function\", \"area_m2\"]],\n",
    "    left_on=\"ref_to_greenspace_site\",\n",
    "    right_on=\"id_site\",\n",
    "    how=\"right\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost output is cum_15_transit\n",
      "area column name is area_15_transit\n",
      "cost output is cum_15_car\n",
      "area column name is area_15_car\n",
      "cost output is cum_15_bicycle\n",
      "area column name is area_15_bicycle\n",
      "cost output is cum_15_walk\n",
      "area column name is area_15_walk\n"
     ]
    }
   ],
   "source": [
    "ttm_greenspace = (\n",
    "    ttm_complete.copy()\n",
    ")  # saving a copy of the matrix (the following operations will add columns to it, but we want to keep the original one also)\n",
    "\n",
    "ttm_gs_with_area = pd.merge(\n",
    "    ttm_greenspace,\n",
    "    gs_entrances_with_parkarea[[\"id_entrance\", \"ref_to_greenspace_site\", \"area_m2\"]],\n",
    "    left_on=\"to_id\",\n",
    "    right_on=\"id_entrance\",\n",
    "    how=\"left\",\n",
    ")\n",
    "# generate tracc cost object\n",
    "ttm_gs_tracc = tracc.costs(ttm_gs_with_area)\n",
    "\n",
    "modes_list = [\"transit\", \"car\", \"bicycle\", \"walk\"]\n",
    "\n",
    "# empty dataframes to be filled up in the next for loop\n",
    "acc_pot_gs = origins[[\"id\"]]\n",
    "gs_acc = []\n",
    "\n",
    "for m in modes_list:\n",
    "    # generate variable names to be used in the tracc function below\n",
    "    cost_name = \"time_\" + m\n",
    "    travel_costs_ids = [\"from_id\", \"to_id\"]\n",
    "    impedence_param = 15  # value for impedence function, to be changed as needed\n",
    "    impedence_param_string = str(impedence_param)\n",
    "    # name of the column\n",
    "    cost_output = (\n",
    "        \"cum_\" + impedence_param_string + \"_\" + m\n",
    "    )  # naming depends on impedence function threshold\n",
    "    area_column_name = \"area_\" + impedence_param_string + \"_\" + m\n",
    "    acc_column_name = (\n",
    "        \"pot_cum_acc_\" + impedence_param_string + \"_\" + m\n",
    "    )  # naming depends on impedence function threshold\n",
    "    opportunity = \"pop\"\n",
    "    # Computing impedence function based on a 15 minute travel time threshold.\n",
    "    ttm_gs_tracc.impedence_calc(\n",
    "        cost_column=cost_name,\n",
    "        impedence_func=\"cumulative\",\n",
    "        impedence_func_params=impedence_param,  # to calculate opportunities in X min threshold\n",
    "        output_col_name=cost_output,\n",
    "        prune_output=False,\n",
    "    )\n",
    "    ttm_gs_df = ttm_gs_tracc.data\n",
    "    # Setting up the accessibility object. This includes joining the destination data to the travel time data\n",
    "    # this needed to be done differently for greenspace, as opportunity is sites's area cumulative sum\n",
    "    # A. Filtering only rows with time travel within the threshold\n",
    "    print(\"cost output is\", cost_output)\n",
    "    print(\"area column name is\", area_column_name)\n",
    "    # tracc_15min = ttm_gs_tracc.data[ttm_gs_tracc.data.loc[:,cost_output]==1] # this doesn't work because of the different lenghts of the columns generated per mode\n",
    "    ttm_gs_tracc.data[area_column_name] = (\n",
    "        ttm_gs_tracc.data[\"area_m2\"] * ttm_gs_tracc.data[cost_output]\n",
    "    )\n",
    "    ttm_gs_df = ttm_gs_tracc.data\n",
    "\n",
    "    # B. Filter entrances (only one per park)\n",
    "    oneaccess_perpark = ttm_gs_df.sort_values(cost_name).drop_duplicates(\n",
    "        [\"from_id\", \"ref_to_greenspace_site\"]\n",
    "    )\n",
    "    # C. Assign metric as sum[parks' area]\n",
    "    # generate df with one row per OA centroid ('from_id') and sum of sites' areas - per each mode\n",
    "    gs_metric_per_mode = oneaccess_perpark.groupby([\"from_id\"])[\n",
    "        area_column_name\n",
    "    ].sum()  # .reset_index()\n",
    "    gs_acc.append(gs_metric_per_mode)\n",
    "gs_acc = pd.concat(gs_acc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_acc.to_parquet(\n",
    "    f\"temp/acc_greenspace_allmodes_15min.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_acc.columns = [\"transit\", \"car\", \"bike\", \"walk\"]\n",
    "greenspace = xr.DataArray.from_series(gs_acc.stack()).rename(\n",
    "    {\"level_1\": \"mode\"}\n",
    ")\n",
    "greenspace.name = \"green_accessibility\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = xr.merge([baseline, greenspace])\n",
    "baseline[\"green_accessibility\"] = baseline[\"green_accessibility\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create demoland class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accessibility(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"engine/{name}/accessibility.joblib\", \"wb\") as f:\n",
    "    joblib.dump(acc, f, compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Generate files for the app\n",
    "\n",
    "Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_aoi.geometry.to_crs(4326).to_frame().assign(id=range(len(grid_aoi))).to_file(\"app/geography.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geo config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bds = grid_aoi.to_crs(4326).total_bounds\n",
    "zoom_lon = np.log2(360 * 2.0 / (bds[2] - bds[0]))\n",
    "zoom_lat = np.log2(360 * 2.0 / (bds[3] - bds[1]))\n",
    "zoom = round(np.min([zoom_lon, zoom_lat]), 2)\n",
    "geo_config = {\n",
    "    \"featureIdentifier\": \"to_id\",\n",
    "    \"initialLatitude\": round(np.mean([bds[1], bds[3]]), 2),\n",
    "    \"initialLongitude\": round(np.mean([bds[0], bds[2]]), 2),\n",
    "    \"initialZoom\": zoom - .4,\n",
    "    \"areaName\": area_name,\n",
    "    \"modelIdentifier\": name,\n",
    "}\n",
    "with open(\"app/geo_config.json\", \"w\") as f:\n",
    "    json.dump(geo_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_folder}/h3/house_price_model.joblib\", \"rb\") as f:\n",
    "    hp_model = joblib.load(f)\n",
    "with open(f\"{data_folder}/h3/air_quality_model.joblib\", \"rb\") as f:\n",
    "    aq_model = joblib.load(f)\n",
    "\n",
    "hp = Model(matrix_aoi, hp_model)\n",
    "aq = Model(matrix_aoi, aq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_hp = hp.predict(grid_aoi.drop(columns=[\n",
    "    \"geometry\",\n",
    "    \"air_quality_index\",\n",
    "    \"house_price_index\",\n",
    "    \"signature_type\"\n",
    "]))\n",
    "baseline_aq = aq.predict(grid_aoi.drop(columns=[\n",
    "    \"geometry\",\n",
    "    \"air_quality_index\",\n",
    "    \"house_price_index\",\n",
    "    \"signature_type\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"engine/{name}/accessibility.joblib\", \"rb\") as f:\n",
    "    acc = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = pd.Series(0, index=grid_aoi.index, name=\"oa\")\n",
    "oa.index.name = \"to_id\"\n",
    "\n",
    "baseline_ja = acc.job_accessibility(oa, \"walk\")\n",
    "baseline_ga = acc.greenspace_accessibility(oa, \"walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "        \"Wild countryside\": 0,\n",
    "        \"Countryside agriculture\": 1,\n",
    "        \"Urban buffer\": 2,\n",
    "        \"Warehouse/Park land\": 3,\n",
    "        \"Open sprawl\": 4,\n",
    "        \"Disconnected suburbia\": 5,\n",
    "        \"Accessible suburbia\": 6,\n",
    "        \"Connected residential neighbourhoods\": 7,\n",
    "        \"Dense residential neighbourhoods\": 8,\n",
    "        \"Gridded residential quarters\": 9,\n",
    "        \"Dense urban neighbourhoods\": 10,\n",
    "        \"Local urbanity\": 11,\n",
    "        \"Regional urbanity\": 12,\n",
    "        \"Metropolitan urbanity\": 13,\n",
    "        \"Concentrated urbanity\": 14,\n",
    "        \"Hyper concentrated urbanity\": 15,\n",
    "    }\n",
    "baseline = pd.DataFrame(\n",
    "    np.array([baseline_aq, baseline_hp, baseline_ja, baseline_ga]).T,\n",
    "    index=grid_aoi.index,\n",
    "    columns=[\n",
    "        \"air_quality\",\n",
    "        \"house_price\",\n",
    "        \"job_accessibility\",\n",
    "        \"greenspace_accessibility\",\n",
    "    ])\n",
    "baseline['signature_type'] = grid_aoi.signature_type.map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"app/scenarios/baseline.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "    {\n",
    "        \"metadata\":{\n",
    "            \"name\":\"baseline\",\"short\":\"Baseline\",\"long\":\"Baseline: Situation now\",\n",
    "            \"description\":\"The baseline reflects the situation as our models see it today. It shows what the four indicators are predicted to be using existing land use data.\"},\n",
    "        \"changes\":{},\n",
    "        \"values\": baseline.to_dict(orient='index')\n",
    "    },\n",
    "    f\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Generate files to be uploaded to the data repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_aoi.drop(columns=[\n",
    "    \"geometry\",\n",
    "    \"air_quality_index\",\n",
    "    \"house_price_index\",\n",
    "    \"signature_type\"\n",
    "]).to_parquet(f\"engine/{name}/default_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=grid_aoi.index, columns=[\"signature_type\",\"use\", \"greenspace\", \"job_types\"]).replace(np.nan, None).to_parquet(f\"engine/{name}/empty.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"area\": grid_aoi.area}).to_parquet(f\"engine/{name}/oa_area.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_aoi[[\"signature_type\"]].rename(columns={\"signature_type\": \"primary_type\"}).to_parquet(f\"engine/{name}/oa_key.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sha256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = {}\n",
    "for fp in glob(f\"engine/{name}/\"):\n",
    "    with open(fp, \"rb\") as f:\n",
    "        bytes = f.read()\n",
    "        registry[fp] = hashlib.sha256(bytes).hexdigest()\n",
    "with open(\"engine/hashes.json\", \"w\") as f:\n",
    "    json.dump(registry, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps are manual.\n",
    "\n",
    "14. Take the folder with engine files and upload it to `Urban-Analytics-Technology-Platform/demoland-engine/data/`.\n",
    "15. Use the information in `sha256.py` to update `data.py` in the `demoland_engine` code.\n",
    "16. Take the folder with the app files and use it to generate the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/martin/demoland-project/code/04_generalisation/engine.zip'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(\"app\", 'zip', \"app\")\n",
    "shutil.make_archive(\"engine\", 'zip', \"engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
